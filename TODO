I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

==============================================================================

o investigate failure in erlang qc-tests

o refactoring of the whole transaction-mess
    goals:
      1. transactions can be created on the stack
      2. transactions are no longer a linked list
      3. transaction operations are not handled in a separate tree but
            are attached to the btree nodes
      4. remove that awful complexity that currently exists

    implementation steps:
      x create new structure DeltaUpdate - identical to TransactionOperation,
            but part of the Btree sources
      x SortedDeltaUpdates is a vector which is sorted by key and attached to
            a node (the node manages this vector)
      x UnsortedDeltaUpdates is a vector which is appended to only, no lookup
      x Each Transaction has a UnsortedDeltaUpdates
      x Each DeltaUpdate has a state (committed or aborted, txn-id)
      x As soon as a Transaction is committed or aborted then the state of
            its DeltaUpdates is updated
      x insert: check/update DeltaUpdates instead of TxnTree
        x perform a lookup on that key
        x check for conflict or if the key was erased before
        x if not: insert a new DeltaUpdate
      x erase: check/update DeltaUpdates instead of TxnTree
      x find: check DeltaUpdates instead of TxnTree
      x simple tests should work (no cursors, no splits/merges)
      x If a node is split or merged then the DeltaUpdates are re-distributed
            between the nodes
      x cursor: consolidate DeltaUpdates instead of TxnTree
        x insert
        x find
        x erase
        x move first
        x move last
        x move next
        x move previous
      o handle duplicate keys
        o consolidate DUs/btree duplicates
        o cursor: get duplicate count
        o cursor: move_first
        o cursor: move_last
        o cursor: move_next
        o cursor: move_previous
      o implement approx. matching
      o periodically merge committed deltas, free aborted deltas
        o make sure that attached cursors are coupled to the
            new btree item
        o if keys are deleted then 'nil' the cursors
            o this needs to be documented b/c it's a semantic change
      o temp transactions are created on the stack
        -> temp transactions are not stored in a linked list
        o (non-temp) transactions form a single linked list, not a double one
      o finally clean up the remaining code (rb.h, txn_index etc)

      o cursor: when deleting a key and the deleted key/record is fetched
            -> return HAM_KEY_NOT_FOUND
      o txn_cursor.cpp: either rewrite or delete
      o github issue tracker has more bugs regarding approx. matching
      o new tests:
            BEGIN T1; INSERT K1; ERASE K1; FIND K1; -> error
            BEGIN T1; INSERT K1; ERASE K1; FIND K1 in T2; -> conflict
            BEGIN T1; INSERT K1; ERASE K1; COMMIT T1; FIND K1; -> error
            BEGIN T1; INSERT K1; ERASE K1; COMMIT T1; FIND K1 in T2; -> error

      o create cursors on the stack as well
            o cursors should form a single-linked list






o Make Pages resizable with variable length
    Pages can have different sizes, and are always aligned to the "native"
    page size of a Database (with the exception of header pages).

    These schemes are used for disk-based AND in-memory based databases!
    For this to work, the in-memory device must use a synthetic page ID
    o differentiate between page id and address; the address is only
        used by the Device!
    o in-memory device uses a synthetic page ID (atomic discrete counter)
    o rewrite allocation code path for in-memory pages
    o checkpoint - all in-memory tests must run

    The page IDs are now compressible and have variable length. The first
    2 bits are specifying the compression. The default page size is 16kb.
        00 - Page ID embedded in flags and the following byte,
        (2b) Page Size is always 1
             max index size: 14 bits - 268 mib
        01 - Page ID embedded in flags and the following 2 bytes,
        (4b) Page Size: 1 byte
             max index size: 22 bits ~ 68 gib
        10 - Page ID embedded in flags and the following 3 bytes,
        (6b) Page Size: 2 byte
             max index size: 30 bits ~ 17 tib

    Blob IDs have a similar schema, but in also specify the offset
    of the blob in this page. The default page size for blobs is 32kb.
        00 - Page ID embedded in flags and the following byte,
        (4b) Page Size is always 1                     32 kib
             Blob Offset is 2 bytes (4 byte-aligned)  262 kib
             max index size: 14 bits                  536 mib
        01 - Page ID embedded in flags and the following 2 bytes,
        (5b) Page Size: 1 byte                          8 mib
             Blob Offset is 3 bytes (4 byte-aligned)   64 mib
             max index size: 22 bits                  128 gib
        01 - Page ID embedded in flags and the following 3 bytes,
        (6b) Page Size: 1 byte                          8 mib
             Blob Offset is 2 bytes (4 byte-aligned)   64 mib
             max index size: 30 bits                   34 tib
        11 - Blob is inline with variable length
                length is encoded in the 6 bits of the first byte (max 63)
        (1+xb)

    o deprecate the HAM_PARAM_PAGE_SIZE parameter for the Environment
    o use HAM_PARAM_PAGE_SIZE parameter for the Database
    o each page needs to be aware of its own size
        o don't forget the journal!
    o store the size persistently
    o print in ham_info
    o adjust ham_bench (the monster tests should not require any changes)

    o rewrite pickle.h
        o create code to read and write the page ID (a special type)
        o page IDs are addressed as uint8_t *, not uint64_t
        o create code to read and write the blob ID
        o blob IDs are addressed as uint8_t *, not uint64_t
    o make sure that PageManager still works (the only user of pickle.h)
    o use compressed page id for root page storage

    o if no page size is specified: come up with good default values for
        blob pages and indices and leafs
        o make blob pages larger than normal pages
        o make index pages larger if duplicates are used

    o create code to write *groups* of PageIds and BlobIds; these groups
        will be used for the record lists (also for the duplicates)

    o remove the get_record_id()/set_record_id() methods from the record list
        interface (can be implemented by the BtreeNodeProxy base class)
    o combine BaseRecordList and BaseKeyList in a common base class

    o The InternalRecordList and the DefaultRecordLists need to use blocks
        to efficiently manage variable-length IDs
    o also use this for storing duplicates!

    o run monster tests
    o run performance tests

    o when rebasing pro: zint32-encodings currently don't work with
        page sizes > 16 kb!!

o Refactoring: would it make sense if each Database has its own BlobManager?
    Then ham_env_erase_db would be much faster, and if each Database has its
    own lock then the blob pages would not block each other
    o are there other modules that should be "per database" and not
        "per environment"?
        - PageManager
        - Device
        - ...?

o migrate to libuv 1.0, it has a stable API
    http://docs.libuv.org/en/latest/migration_010_100.html
    o also for windows!

o improve the webpage documentation
    o document the various btree formats on the webpage, with images
        o variable length keys (w/ extended keys)
        o POD keys
        o default records
        o inline records
        o fixed-length records
        o duplicates (w/ overflow tables)
        o PRO: compressed keys
        o PRO: compressed records

. investigate "pointer swizzling": internal btree nodes should store "hints"
    to the actual Page, not the Page IDs, as soon as the Page was loaded for
    the first time. This could circumvent the buffer cache and increase
    performance.
    How to invalidate those hints or discover that a page was evicted from
    cache?
    - Eviction could only free the persistent part of a page, and not the
        stub.
    - Could also use reference counting for the page





o PRO: Group Varint-related improvements
    o will currently not work with pages > 16kb
    o vacuumize (internal == false): merge two blocks if they're underfilled?
        we have to reduce the number of indices
        o also for varbyte?

o PRO: allow compression of 32bit record numbers

o PRO: prefix compression for variable-length keys
    use an indirection for the prefixes and suffixes; store each
    part in a slot. the keys themselves have then fixed length (2 slot id's)
        ==> supports efficient binary search!
        ==> is efficient for random read/writes AND linear scans
        however, it's very complex to figure out how to optimally split the
        strings into prefixes and suffixes
    ==> prefixes and suffixes can be stored as extended keys if they become
        too large
    see indexcompression2009.pdf - Efficient index compression in DB2 LUW
    o look for more research papers








. hola - next steps
    o support java api
    o support .net api
    o support erlang api
    o lua-functions as callbacks - then remote marshalling will work
    o PRO: compile callbacks with clang remotely
    o add remote support where it makes sense (only for PRO?)

o architecture for a new webpage
    - the new page should
        - look better (i.e. like rethinkdb.com)
        - better targeted towards conversion (like sidekiq.com)
        - be more light-weight
        - allow better integration of the documentation
        - allow the documentation to reside in the repository, not in github
    o pick an awesome design
        i.e. similar to http://foundationdb.com, http://laravel.com,
        http://rethinkdb.com, http://www.orientechnologies.com
        - http://pixelarity.com/axiom
    o use laravel as a routing framework
    o use make/m4/markdown to generate static pages:
        https://github.com/datagrok/m4-bakery
        https://developer.github.com/v3/markdown/
    o come up with a new header page (similar to rethinkdb.com)
        o show code snippets for the various APIs (C, C++, python, etc)
        o "download" button
        o top sales items, each linking to additional information
        o small footer with links to github, mailing lists, twitter
        o big "Upgrade to Pro!" banner - see http://sidekiq.org
    o come up with the full site structure/contents
        o buy - see http://sidekiq.org/pro/ for a very simplified process
        o architecture: btree (w/ images), transactions,
            journalling, analytics...
        o full documentation
        o samples
        o faq
        o tutorial
        o keep the documentation in the source tree, not in -www?
    o documentation comments are hosted on disqus
    o the blog
        o comments are hosted on disqus, articles are also written in markup

    o Makefile can "scp -r" everything to the servers (staging or production)

    . client area with (low priority)
        o authentication
        o collection of files
        o analytics (who downloads what and when?)
    . admin area with (even lower priority)
        o authentication
        o customer database
        o implementing business processes
        o sending out release emails
        o importing new releases
        o etc


. hola: use sum-prefix-trees to precalculate partial sums/results?
    they could be stored in a btree, and used to dynamically recalculate
    requested values 
    https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o QuickCheck: create a new property for testing duplicates; the key is
    always the same. The number of duplicate keys is tracked and
    periodically checked with the API. A cursor can be used to remove a
    specific duplicate, or to fetch a specific duplicate.


. use cache-oblivious b-tree layout
    -> http://supertech.csail.mit.edu/cacheObliviousBTree.html
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
        (but what about the blobs??)
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    o after resize: mmap the whole file area. this is actually important because
        mmap is much faster than r/w; but when the database is created, the
        original mapping already exists. therefore we might have to handle
        more than one mapping in the file
    o PageManager: when allocating a new page then use the distribution
        function to fetch a page from the reserved storage
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)
        -> could create a second memory mapping for the next chunk

o PRO: hot backups (vacuumizes to a different file)
    really only for PRO?
    http://sqlite.org/c3ref/backup_finish.html
    - make sure that all transactions are closed
    - perform ham_env_flush
    - then copy the file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    --> think this through; how to deal with delta updates? -> merge them
        what if only a few databases should be backed up?
        what if i want to back up in a logical format (i.e. csv)?

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    -> see supersonic:
        https://code.google.com/p/supersonic/
    -> see fast bitmap indices
        http://code.google.com/p/lemurbitmapindex/
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?
    o use async operations or futures/promises
    o deprecate ham_db_get_key_count() (tutorial, documentation)

- bloom filter -> PRO
- concurrency -> PRO

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

. PRO: should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvise, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurÃ¼ckgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

