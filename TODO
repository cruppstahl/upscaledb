
x install new uqi header file, remove old stuff

x manage plugins (globally, not per Environment!)
x load plugins via uqi_register_plugin
x load plugins from external libraries
x add unittests
    x negative test: access a plugin which does not exist
    x negative test: import from a library which does not exist
    x negative test: import from a library which does not export the symbol
    x negative test: import from a library which exports, but not the
        requested plugin
    x negative test: set a bad plugin version (load must fail)
    x create a library with a plugin
    x load the plugin from the library
    x type must be UQI_PLUGIN_PREDICATE or UQI_PLUGIN_AGGREGATE
    x verify functions based on type
    x verify that the plugin was registered correctly
x write the parser; move it to a separate c++ file. it should fill a
    SelectStatement object
    x unittests
        x negative test: check syntax errors
        x negative test: use unknown external plugins
        x negative test: use unknown function
        x test various query strings and verify the SelectStatement

x implement the uqi_select function: calls select_range

x implement the uqi_select_range function
    x open the database, then close it again
        x implement
        x unittest
    x if already open: use existing handle
        x implement
        x unittest
    x fail if the database does not exist
        x implement
        x unittest
    x if specified then use the 'begin' cursor, otherwise start from scratch
        x implement
        x unittest
    x make sure that the cursor(s) are from the selected database!
        x implement
        x unittest

    x move implementation details to the LocalDatabase class
    x split implementation into different sections:
        1) txn enabled? process (optional) transactional data at the beginning
            (while cursor.coupled_to_txn()) { process key; move next }
        2) txn enabled? process mixed nodes
            if (node.contains(txnkey) ? use cursor : process btree
        3) process pure btree nodes
            node.scan(...)
        4) txn enabled? process (optional) transactional data at the end
            (while cursor.coupled_to_txn()) { process key; move next }

x implement SUM, DISTINCT SUM, SUM WHERE, DISTINCT SUM WHERE
x implement AVERAGE, DISTINCT AVERAGE, AVERAGE WHERE, DISTINCT AVERAGE WHERE
x implement COUNT, DISTINCT COUNT, COUNT WHERE, DISTINCT COUNT WHERE
x SUM and AVERAGE must use different accumulator members (uint64_t OR double!)
    depending on the key type!
x Need a ScanVisitor which acts as a plugin proxy
x LocalDatabase::select_range must use the new ScanVisitors!
x move uqi-related code to 4uqi

x add new tests which use plugins
x also test with binary plugins (fixed length and variable length)
x restore db6.cpp
x re-activate tests in uqi.cpp and zint32.cpp
x function/predicate names should ignore the case -> always convert to
    lower case when parsing and when importing a plugin
    x add unittest for function name
    x add unittest for predicate name

x implement COUNT for binary keys with fixed- and variable length
    -> force use of cursors for var-length keys?
    x already works, but check why (btree node uses an iterator?)
x store the temporary parser objects from boost spirit

x add tests spanning multiple pages
x what if there are transactional keys "between" nodes? not sure if they
    are correctly handled
x close the plugin handles when shutting down - otherwise valgrind will
    report leaks
x allow numerical queries (SUM, AVERAGE...) only on numerical data!
    x ... but others are always allowed
    x add unittest
x parser: allow hex-and octal-style integers ("... from database 0x1")
    x add unittests

x the 'end' cursor is currently ignored.
    x Implement it!
        x If the end-cursor is coupled to a page then use cursors
            on that page
        x If the end-cursor is coupled to a txn then check if it modifies
            the current page
    x add unittests
    x verify that the 'begin' cursor is correctly moved to 'end'

x queries must be able to return a "result set" with a range of keys
    AND records
    x this will be a transparent type, allocated internally and
        made available through API functions
    x the user has to release the memory
    x look at jdbc result object for inspiration

    x how to iterate over the result set?
        size_t num_items;
        uint32_t key_type;
        uint32_t record_type;
        ups_key_t *keys(); // point into allocated memory
        ups_key_t *key(size_t index); // point into allocated memory
        ups_record_t *records(); // point into allocated memory
        ups_record_t *record(size_t index); // point into allocated memory
        void *key_data();
        void *record_data();
    x fix the current implementation; existing queries should return
        key="FUNCTION($key)", key_type = STRING
        record=value, record_type = UINT64 | REAL64

x add to remote API
x add to python API (w/ sample)
x ups_result_get_{key|record}: swap parameters (OUT is the last one!)
x uqi_select_range: begin-pointer no need to be a pointer of pointer!! 
x add to java API (w/ sample)
    x add unittests
    x make sure that the "begin" cursor is advanced!
    x improve javadoc documentation
x add new API uqi_result_get_record_type
    x also for python API
    x also for java API
x add to erlang API (w/ sample)
    x add unittests
    x make sure that the "begin" cursor is advanced!

o add numeric record types
    x persist the type, set the record size
    x query with ups_db_get_parameters()
        x add unittest
    x clean up the BtreeIndex interface and the way the parameters are persisted
    x need a way to better scale the index factory! with macros?

    o create a new RecordList for POD types
        x without support for duplicates
            x implement
            x add unittests
        o with support for duplicates
            o implement
            o add unittests
        o really its own file? there's no performance difference
            compared to fixed-length binary records

    x add command line option for ups_bench
    o add monster with duplicates tests
    o add monster without duplicates tests

    o do not allow CUSTOM typed records
        o document this
        o add unittest
    o print type in ups_info
        o also review the zint32 compression - it's sometimes not printed

o zint32.cpp: enable uqiTestDuplicate()

o merge isset, notset, issetany from topic/txn 

o the python UQI tests still has failures

o plugin typedefs should also work with records, but some variables
    (size) are uint16_t!
o allow queries on records
o allow queries on keys AND records (FOO($key, $record))?
    o what about existing records like COUNT, SUM? They should fail

o aggregation functions which return key/value pairs always return *both*!
    i.e. min($record) returns a key AND a record (at least for now)
    o uqi_result_get_key and uqi_result_get_record currently
        always return the FIRST key/record - fix this!
    o also make sure that the remote implementation works...

o add a builtin min function
    o shortcut for keys: simply use a cursor and read the first key
    o also for varlen data!
    o also for custom data!
o add a builtin max function
    o shortcut for keys: simply use a cursor and read the last key
    o also for varlen data!
    o also for custom data!

o add top-k/bottom-k functionality
    o should this implement the "limit" for the "-k"? or how do we pass a
        parameter to specify it?
    o add a builtin top-k function
        o also for varlen keys!
        o also for custom keys!
    o add a builtin bottom-k function
        o also for varlen keys!
        o also for custom keys!

o add samples
    o also add to webpage
o add documentation
    o ... about numeric record types
    o function and predicate names are not case-sensitive
    o extend the tutorial

o re-enable the hola functions? they were just "documented" in the latex
    paper - it would not be nice to get rid of them again... or not?
    o remove Database::scan
    o what happens with ups_db_count?

o add to dotnet API (w/ sample)
    o started, but not yet tested
    o add unittests
    o make sure that the "begin" cursor is advanced!
    o add a sample
    o improve the documentation

o release 2.2.0
    o win32 port!
    o also test the .NET API...
    o advertise the new features on the front page!

o add mixed queries (i.e. SUM($record) where PREDICATE($key))
o add integer record compression (simdfor)
o the 'limit' setting is currently ignored. Implement it!
    BUT: this limits the actual result set, not the original input set!
    o Could the limit be implemented as a "wrapper" around the visitor,
        or even a wrapper around the predicate?
    o it counts the invocations of agg_single and agg_many (and limits the
        size of the agg_many vector)
o allow selection of multiple functions, i.e.
    "min($key), max($key) from ..."
    This way we run the main loop only once, reduce I/O, use "hot" data etc.
    Should be much faster than running the two functions separated from
    each other.
o distinguish between different query types:
    - aggregation: returns a single result
    - filter: returns rows, based on a predicate

    The following queries are therefore valid:
    $key from database 1
    $record from database 1
    $key, $record from database 1
    $key from database 1 where predicate
    $record from database 1 where predicate
    $key, $record from database 1 where predicate

. use SSE or AVX for summing up?

------------------- idea soup ---------------------------------------------

- compilation for ARM:
    sudo apt-get install g++-arm-linux-gnueabihf
    ./configure --host=arm-linux-gnueabihf --disable-simd --enable-debug

o remove dependency to libuv 1.0, use boost::async instead (makes the build
    process a lot smoother)

o clean up BtreeIndex class; it has too many responsibilities, i.e.
    managing configuration, persisting configuration, btree traversal,
    btree actions etc
    o the configuration of all btrees is moved to a separate class
    o the BtreeIndexFactory can create or open btree indices; it manages
        the page with the persistent configurations
    o the BtreeIndex has a persistent configuration, a runtime configuration
        (DatabaseConfiguration) and entry points for btree actions
    o create a common root class for BtreeActions with common functions,
        i.e. traversal

o Refactoring: rewrite the whole cursor layer
    o clean up the public interface
    o remove the Transaction cursor, merge BtreeCursor with LocalCursor
    o there should be 3 states:
        - nil
        - coupled to btree
        - coupled to txn
        o state() - returns the state
        o set_state() - changes the state
    o key() returns the current (coupled) key
    o record() returns the current (coupled) record
    o get rid of the DuplicateCache and the duplicate index in the cursor; if
        the DeltaUpdates are correctly sorted, then the cursor should not be
        necessary (unless the duplicate position is explicitly required via
        ham_cursor_get_duplicate_position())

o More things to refactor in the btree
    o EraseAction uses duplicate_index + 1, InsertAction uses duplicate_index
        -> use a common behaviour/indexing
    o EraseAction line 71: if the node is empty then it should be merged and
        moved to the freelist!

o when splitting and HAM_HINT_APPEND is set, the new page is appended.
    do the same for prepend!

o Refactoring: all unittest fixtures should derive from a BaseFixture,
    which creates an Environment, creates a list of databases (w/ parameters),
    and if required also a cursor, a transaction and a context
    o include additional management functions like lenv(), ldb(), ltxn(),
        page_manager(), cache(), context()...
    o what else?
    o then reorganize the tests
        - public API
        - internal modules

o The PageManager state is currently stored in a compressed encoding, but
    it is less efficient than the standard varbyte encoding because
    pages > 15 * page_size have to be split. Use a standard vbyte encoding
    instead (it will anyway be required later on).

o Implement record compression - a few notes
    ByteSlice: Pushing the Envelop of Main Memory Data
    Processing with a New Storage Layout
    http://delivery.acm.org/10.1145/2750000/2747642/p31-feng.pdf

    1) the user defines the record structure.
    2) an optimization stage reorders the record columns to optimize storage
        (i.e. with dynamic programming)
    3) SIMD code is generated on the fly to pack, unpack records and single
        elements (see http://stackoverflow.com/questions/4911993/how-to-generate-and-run-native-code-dynamically or ask Ben/Andi...)
    4) Pack like PAX (group all column values together) or each record
        standalone?

o look for a better compression for DefaultRecordList, i.e.
    - Each group is a GroupedVarInt w/ 4 bits per entry; a 64bit
        number can then hold flags for 16 numbers
        -> (but maybe increase this to hold at least 32 or 64 numbers, to
            reduce the overhead ratio)
    o create a GroupedVarInt<Max, T> class, where |Max| is the maximum number
        of elements that are grouped, and T is the type of these elements
        (i.e. uint64_t)
        -> memory is managed by the caller
        -> the state (i.e. used block size etc) is stored externally, and
            managed by the caller
        o append a key
        o prepend a key
        o insert a key in the middle
        o grow blocks
        o split blocks
        o can perform copies w/o re-compressing

    o try to move the Zint32 index to a base class
    o Use small index which stores offset + bits for each group
    o a separate bit is used to signal whether the (compressed) number is
        a record id
    o avoid ripple effects by growing/splitting the block

o use compression also for duplicate records
    i.e. use GroupedVarint for inline duplicates

o Concurrency: merge BtreeUpdates in the background

o should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvise, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurÃ¼ckgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

