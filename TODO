I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.4 ..................................................
x check for scalability issues with files > 1 GB
x improvements for ham_bench
- get rid of the freelist
- support pages of different sizes
- improved (faster) logging
- delta updates (maybe)
-----------------------------------------------------------------------------

x update copyright string: 2013 -> 2014

x check for scalability issues with files > 1 GB and check if we're on the
    right track regarding scalability
    -> remove the freelist
    -> add delta updates to avoid memmove in the index pages

x ham_bench: new test mode: first load with LOTS of data, then erase in the
    same order (or in random order or reverse order) till the database is empty
    - implementation idea: when switching to ERASE, perform a reset() on the
        NumberGenerator, and it will start delivering the same sequence as
        from the beginning
    x add a reset() method to the datasources
    x new command line options --bulk-erase
        x if set then reset() datasource, delete
        x NOT for parser-generator (print error)
        x only for stop-ops
    x add to monster tests

x get rid of the freelist for pages (not yet for blobs)
    The PageManager keeps a (persistent) list of all pages, including their
    id, their size, the type, the database. It keeps a separate list with
    all free pages.
    x the PageManager has a list of all pages. this effectively replaces
        the cache
    x the PageManager knows if a page is old or not, and whether it can
        be flushed to disk
    x get rid of the "totallist" linked list of all pages
    x get rid of the "bucket" linked list of all pages
    x remove HAM_CACHE_FULL and HAM_CACHE_STRICT - don't think it is used
    x the PageManager knows whether a page is free or not
        x no longer use the freelist to manage empty pages, only for blobs
    x when allocating new pages: check the internal list, not the freelist
    x store the modified list on disk, load it
        x implement store/load methods
        x store blob-id in the environment header 
        x load when env is opened
        x store when env is closed
        x PageManager must accept entries which are available but do not
            have a Page pointer
        x fix unittests
    x pax-layout with recsize == 0: must be much faster now
        -> yes, but purge_cache requires 30% of the time now
        x re-introduce totallist
    x move the unittests from cache.cpp to page_manager.cpp, update them
    x do not persist state if Environment is in-memory 
    x re-activate unittests in log.cpp
    x if logging is enabled: immediately write the modified state
        (later this will be rewritten as incremental updates for the log)
        x verify recovery tests

x support pages with different sizes
    x for now just keep the setting as is (HAM_PARAM_PAGE_SIZE in the Env)
        x but the page stores its size persistently (for debugging)
        x always use page->get_page_size(), if possible
        x when loading a page: assert that page_size is correct
            (if a header exists)

    x make sure that the log can work with different sizes

    x device: make independent of page size
    x Page::allocate must accept page size as parameter

    x if a free page is deleted then delete the Page pointer, but keep the
        entry in the map, otherwise we lose the information whether the page
        is free
    x PageManager must store page sizes (and persist them)
    x when allocating a page: check freelist for pages with this size
    x PageManager->is_cache_full() must take the sizes in account

x v2: fix compilation problems on MacOS

x remove the freelist
    x blob: remove direct I/O - it's barely used anyway
    x completely get rid of the freelist
    x blobs no longer need to be aligned

    x very large blob pages no longer span multiple blobs but create a
        single page which is large enough
        x PageManager::alloc_page needs page-size parameter
        x PageManager::fetch_page does not require parameter cache_only
        x PageManager::fetch_page needs to know the size
    x blobs no longer cross page boundaries
    x this allows lots of cleanups in DiskBlobManager::read_chunk/write_chunk,
        because there's always just ONE page, not more
        x still uses env->get_page_size()
    x every page now has a page header, even blob pages
    x every blob page now has a blob page header with a counter ("free bytes")
        and a small freelist (constant length)

    x update the "free bytes" counter after overwriting, allocating and erasing
        blobs to/in/from this page
    x keep the actual freelist VERY simple - i.e. do not merge empty
        blobs, i.e. only store the 32 largest free entries etc

    x PageManager state persists page sizes for all pages with non-default size
    x The PageManager caches the id of the last used blob page to
        check for free space
    x verify that m_needs_flush is always updated correctly

    x re-activate unittests
        x implement PageManager persistence (simply reserve a page)
            x needs unittest
    x implement/update reclaim_space()
    x update freelist counters for metrics
    x also check recovery tests

    x git rm src/freelist* unittests/freelist*

    x improve performance
        x all pages have fixed size (except blob pages)
            x remove page->get_size()
            x adjust page->allocate, page->fetch
        x remove PageManager::get_page_from_address()
        x blob pages spanning multiple pages do NOT have a header!
        x the page-map ONLY persists free pages

    x run performance tests (compare PAX against 2.1.3, release build)
        o with records, 1 mio
            --seed=1389874598 --key=uint64
                                2.1.4           head
            total time:         9.272876        7.887640
            file size:          1159331840      1118830592
        o with records, 5 mio
            --seed=1389874598 --key=uint64 --stop-ops=5000000
                                2.1.4           head
            total time:         64.212313       53.898093
            file size:          5832523776      5595086848
        o with records, 1 mio, 30% erase
            --seed=1389874598 --key=uint64 --erase-pct=30
                                2.1.4           head
            total time:         6.530221        6.320913
            file size:          807469056       781975552
        o with records, 5 mio, 30% erase
            --seed=1389874598 --key=uint64 --stop-ops=5000000 --erase-pct=30
                                2.1.4           head
            total time:         44.115751       39.040701
            file size:          4074618880      3898146816
        o without records, 1 mio
            --seed=1389874598 --key=uint64 --recsize-fixed=0
                                2.1.4           head
     !!     total time:         3.155387        3.203447
            file size:          10534912        10534912
        o without records, 5 mio
            --seed=1389874598 --key=uint64 --recsize-fixed=0 --stop-ops=5000000
                                2.1.4           head
     !!     total time:         27.179945       27.068112
            file size:          67125248        67125248
        o without records, 1 mio, 30% erase
            --seed=1389874598 --key=uint64 --recsize-fixed=0 --erase-pct=30
                                2.1.4           head
     !!     total time:         2.255874        2.275510
            file size:          8421376         8421376
        o without records, 5 mio, 30% erase
            --seed=1389874598 --key=uint64 --recsize-fixed=0 --erase-pct=30 --stop-ops=5000000
                                2.1.4           head
     !!     total time:         20.919253       21.453294
            file size:          33898496        33816576

x v2: Fix .NET Wrapper issue: VS2010, SampleDb1. See
    https://github.com/cruppstahl/hamsterdb/issues/29 for details

x improve the freelist
    The current implementation is limited because it only supports one page
    to persist the PageManager's state. More than one page has to be
    supported, and logging must be efficiently (i.e. only log those pages
    that are modified.)
    x only freelist operations are logged
    x Reenable Env/eraseMultipleDatabases
    x Reenable Env/eraseMultipleDatabasesReopenEnv
    x store freelist on multiple pages, if required
        x implemented, but needs a test
    x BlobManager: merge adjacent free blobs
    x if a blob page is completely empty: add it to the freelist
        x add test
    x if page is a multi-page: add all pages separately

    x when closing: perform reclaim_space() BEFORE flush_all_pages(),
        and flush_all_pages() should NOT flush the state_page;
        instead (afterwards), the state is flushed and the state_page
        is flushed. afterwards, no more pages are allowed to be modified.
        -> that's already the case...
    x check recovery tests

o improve PageManager performance
    x if logging is enabled and a freelist entry is *added* then this change
        must not be logged. However, if a page is *taken* from the freelist
        then it must be logged.
    x use the non-intrusive hash table from 2.1.3
        x manage free pages in separate freelist (std::map)
        x cached pages are stored in a non-intrusive hash table
    x improve multi-page handling
        x PM::alloc_multiple_blob_pages() needs to be implemented
            x add tests
====================
    o check performance again
performance (see matrix above) is slower than 2.1.3 - why??
according to callgrind, 2.1.3 is slower in EVERY aspect with ONE exception:
  DiskBlobManager::allocate has a higher "self" time when looking up
    m_last_alloc_blobid -> move it to the PageManager and cache the Page
    pointer instead of the Page id, then try again
====================
    o compress the freelist state
        o collapse single free pages to multi-page blobs
            o add tests
        - 1 byte header
            - 3 bits: number of following bytes for page-id
                (address % page_size)
            - 5 bits: the multi-page counter
        - n bytes: page-id, multiple of page-size
    o run monster tests

o perftest
    o add tests with more elements (i.e. 10 mio ops)
    o ... and with very big records (1mb)
    o also run those for 2.1.4

o v2: verify/improve performance of btree_impl_default: it has a very high
    number of page splits compared to 2.1.3, even if the key size is <= 21.
    Why? maybe the capacity is not calculated correctly. or do keys have
    more overhead than in 2.1.3?
    o variable length keys (no duplicates), recsize=0
    o variable length keys (with duplicates), recsize=0
    o fixed length keys (with duplicates), recsize=0
    -> what exactly happens if the page is split? is it really resized
            correctly?

o erlang API

o web-page requires updates
    x deployed html differs from git-repository
    o www1
        o merge git repository of www1 (host on github, keep remote branch)
        o clean up 'dl' directory
        o where to host static files?
        o backup and deploy to www2, use round-robin DNS

. improve integration of static code analysis in release process
    o try to become "oclint-clean"
    o try to become "coverity-clean"

high-level plan for 2.1.6 ...................................................

- come up with a new design which is NOT based on virtual Page IDs, resizable
    pages etc - they would be way too slow
    o review Changeset::flush
    o buffer the log and journal, only flush if there's an overflow or a commit
    o is it possible to merge log and journal?
    o the PageManager should write "deltas"

- reduce the linked lists - they're hard to be updated with concurrent
    operations
    o page
    o transaction and dependent objects
    o ...

- delta updates: very similar to the current TransactionOperations; however,
    the difference is that delta updates are part of a page, whereas
    TransactionOperations are part of a Transaction. The difference is more
    obvious when talking about recovery: delta updates are logged in the
    "stream" of a page, not of a Transaction. When loading a page, they can be
    reapplied immediately and the page will be valid.
    -> logging disabled: don't bother
    -> logging enabled: write delta updates to a (buffered) log; flush log
        when a Txn is committed
    -> if the log exceeds 100 mb: switch to the next log
    -> delete old log files when all their streams are checkpointed in a
        newer file
    -> if delta updates exceed <threshold>: request/write a checkpoint
    -> each stream knows which log file stores its latest checkpoint
    -> when the database is closed: write checkpoints of as many streams
        as possible, esp. if older logs contain delta updates (then the
        old log can be purged)
    -> each log has a trailer which lists the streams in the log; this
        can be used to purge the log efficiently

  - Delta updates (concurrent/atomic!)
    are like TransactionOperations but directly attached to a BtreeNode.
    the Transaction is separated from the updates; its only state is a name,
    the Tid and the flags (committed, aborted...)
    - Transactions are stored in a linear array, just like the pages
    - db_local.cc: local Transactions must be aborted if a Transaction occurs
        (create them on the stack, abort in destructor)
    - Transactions are not connected to their operations or anywhere else
    - the TransactionOperation becomes a DeltaUpdate
    - attach the DeltaUpdate directly to the BtreeNode
    - the TransactionIndex is no longer required
    - fix insert/erase/find/cursor operations and do not use
        the TransactionIndex (rename BtreeIndex -> Btree)
    - the cursor can be directly coupled to a DeltaUpdate
    - implement SMOs (split/shifts/merge) and make sure the DeltaUpdates are
        moved to the other page, if necessary
    - when checkpointing: delete all DeltaUpdates that belong to an
        aborted Transaction (the Checkpoint is performed by the PageManager,
        which has to traverse the Btree because an SMO is performed
        recursively)
    - non-transactional updates are also performed as DeltaUpdates, but
        they are not logged
o support efficient logging
    o a log IS-A journal: it has "streams" with separate "entries", each has
        a lsn, a header and a payload; both buffer (and compress/encrypt), flush
        -> try to merge both
    o journal (and log) could buffer the pages and only write them to disk
        when the buffer overflows or a transaction is committed
        (later: this buffer could be compressed -> PRO)
    o the log remains responsible for the btree, the journal is a logical
        backup for the delta updates/txn updates
    o instead of flushing the full state, the PageManager could write "deltas"
        to the log
    o ... and the delta updates would be identical to the txn-operations
    o play around with certain thresholds; don't switch the file too often
    o when merging delta-updates, the log should try to store all delta-updates
        into one Changeset, and therefore flush the log only once (with
        combined updates)

    - the btree/data file is INDEPENDENT from the log! it has only atomic
        updates and might become out-of-sync with the actual log. This is by
        design and must not cause problems! this will later allow us to run
        "compactions" in background.
    - the log contains logical "streams", i.e. for PageManager, for each Page
        etc
    - the log writes checkpoints of those streams if the number of delta
        updates exceeds a threshold
    - the log buffers the data and only flushes/fsyncs when requested
        (TODO this needs to be clearly defined!)
    - whenever a Btree page is flushed, the log will contain a "flush" marker
        of this page (and the lsn of the last update which modified this page).
        this marker is stored in the log's PageManager stream
        (can this marker be stored in the page itself?)
    ? what about SMOs?
        they're performed atomically in the Btree by updating/allocating all
        pages, modifying them, updating the PageManager stream in the log
        and then switching atomically in the PageManager map
    ? a insert with a duplicate table or extended key is not atomic; how
        can we solve this?
    ? how is recovery performed?
        - read the last buffer from the last logfile (the tail contains a
            header)
        - uncompress, if it was compressed
        - ...?
    ? can this asynchronousness cause problems?
    ? when/how to fsync?

  - StreamContainer
    - a collection of static buffers (i.e. pages, header page) and streams;
      all stored in one file. each stream contains segments with a small
      header (stream-id, subject, lsn, number of buffers), one or more
      buffers (size, checksum?) and a trailer (last checkpoint of metadata,
      last checkpoint of page-manager)
    - buffers data before writing to file, but flushes (and fsyncs) on
        demand
    - implementation for in-memory remains empty
    - requests checkpoints from streams if there are too many delta updates
    - uses log-file switching: if one file overflows then switch to a second
        file, write checkpoints and continue
    - can maintain its own Stream where it stores its own state: the buffers
        that were written to disk, which streams are in these buffers,
        are they obsolete through newer checkpoints etc
    - use this information to garbage-collect buffers and re-use them
    - add to LocalEnvironment
    - vacuum: ask all streams to perform checkpoints, and flush the buffer
        into a separate file; then rename the file 
    - move the Environment's configuration to a separate object
    - move the Database's configuration to a separate object

  - StreamContainer ("sc") #2
    - perform full transition by moving the remaining modules to the sc
    - pages: fetch/flush from sc file
    - journal: write to sc stream
    - log: write to sc stream
    - remove the device classes


- prefetch
- linear SIMD search -> PRO
- compress the whole LSS (and the log) with snappy -> PRO
- encrypt the whole LSS (and the log) with AES -> PRO
- hot backups (vacuumizes to a different file) -> PRO
- compression -> PRO
- bulk updates -> PRO
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort
    - add those delta updates to the txn-trees or to the stream
- first PRO release
    o webpage updates, PR, mailings
- cache-oblivious page distribution? (maybe write the LSS first?)
    http://supertech.csail.mit.edu/cacheObliviousBTree.html
- bloom filter -> PRO
- concurrency
- operations on compressed data (COUNT(), MIN(), MAX(), ...)?
- C-Store style projections? (requires complex schema types)
    - introduce a new library with complex schema types, projections
    - analytic functions
        count, min, max, sum, product, average, ln, sqrt, exp, round, trunc,
        date/time functions and interval functions
    - select(predicate_t, column_descriptor_t, select_state_t)
             \- an AST of functors
                          \- existing columns or generated columns (i.e. sum())
                                                \- keeps track of offset, count
    - erase(predicate_t)
    - explain(predicate_t, column_descriptor_t)

o introduce PRO version
    o start a closed repository
    o one source base with different licensing headers, different licensing
        string (also for tools), different version tag
    o API to get licensee information (is_pro_version())
    o new release process
    o prebuilt win32 files
    o get rid of serial.h - it's not really required and only creates efforts
    o how to share files with customers? need a login area,
        downloadable, customized files (win32, serial.h, tarballs...)
        -> send out mails if a new file is available
    o evaluation license: build binaries for the most common architectures
        o insert expiration date checks
        o special copyright strings
        o prebuilt for win32/win64
        o unix: obfuscated source code
        o need an automated flow for signups, for evaluation licenses etc
    o extra documentation
    o define file format interoperability?
    o what are the minimum features required for the first release?
        - (evaluation licenses)
        - prefix compression for strings
        - lightweight compression for binary keys
        - SIMD for searches
        - AES encryption
        - hot backups

o PRO: btree can compress keys
    x get rid of the whole minkey/maxkey handling because these numbers
        are irrelevant with compression
    o try to reduce the changes to a new KeyProxy object
    o prefix-compression for strings
        o each 2kb have a full string (indexed by skiplist)
    o delta-compression for numeric data types (>= 32bit)
        (can this be combined with a bitmap compression? the deltas are
        compressed in a bit-stream? but then we end up with variable
        length encodings...)
    o lightweight compression for keys
        http://oldhome.schmorp.de/marc/liblzf.html
        http://lzop.org
    o record compression for blobs (lzop.org? snappy?)
        better postpone this and compress all pages in the lss
    o do we need delta updates for efficient inserts? - i think not yet...

o PRO: use SIMD for fixed-length scans and __builtin_prefetch
    o use valgrind to track cache misses
    http://pcl.intel-research.net/publications/palm.pdf
    http://www.cs.toronto.edu/~ryanjohn/teaching/csc2531-f11/slides/Ioan-SIMD-DBMS.pdf
    http://gcc.gnu.org/onlinedocs/gcc-3.3.6/gcc/Other-Builtins.html
    http://stackoverflow.com/questions/8460563/builtin-prefetch-how-much-does-it-read
    http://tomoyo.sourceforge.jp/cgi-bin/lxr/source/include/linux/prefetch.h
    http://stackoverflow.com/questions/7327994/prefetching-examples
    o use linear search with fixed length keys (if max-keys-per-page
        or the subrange in the binary search is low enough, and if the
        performance makes sense) -> also for MIT
    o if both layouts use binary search then move it back to the proxy!

o release-v2.pl: add test without berkeleydb

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

o btree_impl_default::set_record: if the duplicate is LAST of the last key
    in the node then simply append the record and increase next_offset

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o flush transactions in background (when the btree is concurrent)

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks
    o be careful: if N operations are modifying the same changelog, and
        then #N+1 aborts then the aborting operation must NOT clear the
        changelog!

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurÃ¼ckgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    -> or: make this the default; call the new flag HAM_TXN_MAYBE_WILL_ABORT
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

=======
>>>>>>> Updated TODO
