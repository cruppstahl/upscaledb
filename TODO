I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.7 pro ..............................................
x hamsterdb pro: compression for journal
x hamsterdb pro: compression for records
x hamsterdb pro: aes encryption
x hamsterdb pro: compression for keys
x hamsterdb: linear searches
x hamsterdb: upgrade libuv
x hamsterdb pro: SIMD instructions
x hamsterdb pro: 30 day evaluation

high-level plan for 2.1.8 ..................................................
x hamsterdb: code cleanups
x hamsterdb: performance improvements for remote
x hamsterdb: improve page splits
o hamsterdb: start with the hola API
o hamsterdb: more quickcheck tests
o hamsterdb: delta updates



x web-page requires updates
    x deployed html differs from git-repository
    x download/sources: add erlang, remove 1.x
    x www1-repository and hamsterdb-www should be identical
        x updates for 2.1.6 are missing
        x samples fehlen
        x doku fehlt
        x download-dateien fehlen
    x www1
        x www1 and www2 are already combined in a single remote target
        x where to host static files? - hetzner server
        x clean up 'dl' directory
    x webpage copyright is still 2013

x PRO: enable zlib/snappy/others compression for the records
    x add documentation to header file
    x db: add record compression parameter
        x when creating a database
        x when opening a database
    x create compressor in database (w/ autoptr)
    x refactor BlobManager: should have common functionality, subclasses
            implement do_overwrite, do_allocate etc
        x move parameter checks and duplicate code to parent class
        x do not allow partial updates if compression is enabled
            x unittests
    x implement the actual compression/decompression
        x for in-memory AND disk
        x compression: store flag in BlobHeader
        x only compress if compressed size < original size
        x directly uncompress into output buffer,
            not in internal buffer of the compressor
            (or into user allocated buffer)
        x add unittests (disk and inmemory)
    x add parameters to ham_bench
    x add perftests, monster tests

x PRO: should journal and record compression settings really
    be non-persistent?
    -> should be persistent, makes things easier
    x journal compression: store in env-header
        x env-header has 2 reserved bytes, use them for compression
    x journal compression: disallow in ham_env_open
        x unittest
    x record compression: store in db-header
        x BtreeHeader has 2 bytes reserved for padding, use them for compression
    x record compression: disallow in ham_env_open_db
        x unittest
    x support ham_env_get_parameters
        x unittest
    x support ham_db_get_parameters
        x unittest
    x fix ham_bench
    x add ham_*_get_parameters to header documentation
    x fix memory leaks in unittests

x PRO: is able to open APL files, but not vice versa
    x use msb in file version for a marker
    x when loading a APL file in PRO: set the flag, write back
    x APL will automatically reject the file
    x ham_info: show whether db is pro
    x ham_info should print compression information
    x make ham_info a 100% user of the public API; do not access
        internal classes!

x do we really need the compression LEVEL parameters? if not then we can
    store the key compression flags very conveniently. otherwise we
    need extra storage! - no, remove it

x PRO: work over the license agreement; PRO will be similar to an NDA

x PRO: heavyweight compression for keys
    -> support compression with lzf and the other libraries
    -> keys are stored compressed IF the compressed keys are smaller than the
        uncompressed keys
    -> key->size in the node is the compressed size, and it might further
            be extended! the uncompressed size is part of the payload (16bit
            at the front)
    x need new parameters; store persistently
    x compress before inserting
        x only compress if the compressed size < uncompressed size
        x store the uncompressed size up-front
    x uncompress in get_key(), get_key_direct(), compare() etc
    x add unittests in combination with record encryption (w/ different
        algorithms)
    x what if a PAX layout is selected? -> return an error
        x add unittest
    x extend ham_info
    x add new option to ham_bench
    x add to dotnet wrapper
    x add to java wrapper
    x add to erlang wrapper
    x ham_bench: accept string parameters when enabling compression
    x metrics: track number of compressed and uncompressed keys
        x for journal compression
        x for record compression
        x for key compression
        x print in ham_bench
    x bugs raus

    x blob mgr: do not compress extended keys if the keys are already compressed
    x add header file documentation
    x add to monster and perftests
        x also test in combination with extended keys (very low threshold)
        x also test in combination with encryption and record compression
        x also test in-memory compression (keys + records)
    x unittest: key/record compression with user_alloc
    x fix TODOs in btree_impl_default.h
    x run the full monster suite

x PRO: if a compressor does not exist then ham_env_create_db should immediately
    return an error (this is currently not the case)
    x unittest

x use linear search for PAX (see topic/linear), but improve the
    performance!

x run benchmark with this:
    -mfpmath=sse -Ofast -flto -march=native -funroll-loops
    -> add recommendation to Performance documentation

x performance question: running ham_bench with 10mio keys (uint32, no records)
    and then performing --open --find-pct=100 -> 6.5 sec;
    with --cache=unlimited -> 6.5 sec
    why is there no difference? this is random access, and pages should be
    purged frequently. disabling mmap does not make a difference (SSD!).
    -> try with a spinning disk (same result)
    -> since this is a pure read-only workload, the pages are not modified
        and purging them does not have any I/O costs associated. Fetching them
        simply returns a pointer to the mmapped storage.
        disabling mmap becomes very costly, though.

x PRO: use SIMD instructions for the linear search
    -> SSE2 only supports comparisons for signed integers!
    x configure switch --enable-sse2: adds -msse2 to CFLAGS
        x enable by default if `grep /proc/cpuinfo sse2` != 0
    x need to figure out whether the CPU supports SSE/SSE2 (also on Windows)
        http://stackoverflow.com/questions/6121792/how-to-check-if-a-cpu-supports-the-sse3-instruction-set
    x metrics: add flag whether sse2 is supported
    x need unittest: if sse is enabled and supported then perform an
        sse comparison
    x rewrite configure settings
        x a generic enable/disable SIMD option
        x if enabled: use avx if available; otherwise fallback to sse2
    x metrics should return the LANE width, not bools (default is 0)

    x replace linear search with the SSE2 variant
        x move SSE-related code to a template function in simd.h
        x simd available? -> the find function calls the function in simd.h
            x if HAM_ENABLE_SIMD is true
            x if os_get_simd_lane_width() > 1
        x if threshold is reached: continue with linear search
            x the linear search implementation is not working identically to
                the SIMD functions; add unittest (i.e. for signed int)
                -> no, it's ok
            x threshold depends on supported instruction set (AVX, SSE2)
            x implement linear search with SIMD (SSE - 4x4)
                -> only for exact searches!
                x ham_u16_t
                x ham_u32_t
                x float
                x ham_u64_t
                x add unittests for sse
            x revert support for AVX (move to branch)
            x currently crashing or failing in debug build
            x then perform another round of profiling (with different
                    thresholds!)
    x ham_bench: add flag to disable simd
    x review the changes - some can maybe be ported back to hamsterdb (APL)?
        -> find_exact!
    x test compilation with clang
    x add monster tests
    x add performance tests for all types (no records, many finds, with
        and without SSE)
    x run monster tests
    x test compilation with msvc

x port everything to MacOS

x PRO: 30 day evaluation library
    x remove the ham_get_license() API - it doesn't make sense and people
        will not use it anyway
        x also from erlang
    x ham_time_t ham_is_pro_evaluation(); -> returns end time, or 0
    x insert 30 day trial check macros at various positions
        x insert code in
            x ham_env_create
            x ham_env_open
            x LocalDatabase::create
            x LocalDatabase::open
            x flushing transactions
            x accepting remote connections
        x implement with static functions and boost::posix_time::second_clock, 
            boost::posix_time::microsec_clock, time(0)
        x don't hardcode the timestamp but use a simple calculation, i.e.
                (1396600000+2536), 0x533e76a8, 012317473250 etc
            x return in ham_is_pro_evaluation()
    x needs different licensing output in the tools (evaluation - XX days left)
    x print message to stdout in ham_env_create, ham_env_open
        x make sure that all strings are obfuscated!
    x need a license agreement similar to an NDA
    x evaluate source obfuscators
    x prepare code for obfuscator, improve obfuscation with perl-scripts
    x create automagically in release-build.pl

x PRO: extend release-tool
    x new switch --product=apl|pro (default: apl)
    x for pro: use different version tag
    x for pro: use different packaging directory
    x for pro: use different file list
    x for pro: check dependencies (zlib, snappy, lzo, crypto)
    x for pro: add test for --disable-compression
    x for pro: add test for --disable-encryption
    x for pro: add test for --disable-simd

o port everything to Win32/Visual Studio

o PRO: additional stuff for the first release
    x update the ChangeLog
    x update the README

    x create new documentation page (wiki) for pro (tutorial + ham_bench)
        x journal compression
        x record compression
        x zlib compression
        x aes encryption
        x file compatibility
        x simd

    o user-area: customers.hamsterdb.com
        o create "download" directory with all files
        o create one directory per customer
            o enable directory listing
            o enable symlinks
        o create one directory for trials
            o disable directory listing
            o enable symlinks
        o use htpasswd to regulate the access
        o store passwords in customer database
        o the "download" directory keeps all the files; the customer
            directories just have symlinks into this directory

    o webpage
        x new marketing focus
        x feature matrix APL vs. closed source
        x new layout
        x request evaluation version
        o commit everything to git
        o daily backups of the mysql database
        o once more check all the links, make sure they are relative!
        o add google analytics token!
        o go live

    o send email to all customers (add this to the release process)
        o include license agreement
        o include link to their download area, username/password

------------- first release of hamsterdb-pro 2.1.7 --------------------------

x remote code: get rid of protocol buffers (but leave them in the server
    for other languages). they are too expensive.
    x write a code generator which creates serialize/deserialize routines
        for the messages 
    x perform a benchmark - looks good, nearly 50% faster
    x port all functions
    x investigate libuv memory leaks reported by valgrind

x clean up globals; currently, many metrics and test gates are stored
    in global variables. collect them and store them in a struct Globals
    (globals.h).
    x try to get rid of those that are not necessary

x split functions in os.h in two classes: File and Socket. the actual handles
    will become a class member (including the win32 mmap handle). No longer
    directly access the file handles.

x insert: if HAM_HINT_APPEND is set then page splits should only create a
    new page, and not shifting any keys to the new sibling. only the new
    key is inserted in the new sibling.

x do not purge pages if a cursor is attached

x Fix Cursor::compare, remove the TODO

x find a way how to execute user's code in the btree; similar to the hola
    core described below. we want to run analytical code in the lowest possible
    level, running on linear key data (thus being able to apply SIMD in the
    application)
    -> use a cursor; if a page does not have any pending transactional updates
        then iterate over the whole btree node (in the direction of the cursor),
        otherwise go step by step.
    => implement the following functions:
        typedef struct ham_hola_predicate_t {
            bool (*ham_hola_predicate_t)(void *key_data, ham_u16_t key_size,
                        void *context);
            void *context;
        } ham_hola_predicate_t;
        ham_hola_count(ham_db_t *db)
        ham_hola_count_if(ham_db_t *db, ham_hola_predicate_t *predicate)
        ham_hola_count_distinct(ham_db_t *db)
        ham_hola_count_distinct_if(ham_db_t *db, ham_hola_predicate_t *predicate)
        ham_hola_average(ham_db_t *db)
        ham_hola_average_if(ham_db_t *db, ham_hola_predicate_t *predicate)
        ham_hola_sum(ham_db_t *db)
        ham_hola_sum_if(ham_db_t *db, ham_hola_predicate_t *predicate)
    -> based on this design: come up with an experimental API which will
        become the hola core routine, similar to the "scan" API that was
        planned
    -> the user's callback/function object can then accumulate or collect
        records, whereas the actual record data is only fetched when requested
    -> users need to be aware that it's not allowed to call into hamsterdb
        during such a scan (would create a deadlock)
    -> scans "pick up" other processors and run them at once, on the same
        data
    x create new header file ham/hamster_hola.h
    x create function stubs in hola.cc
    x implement a low-level scan function
        void LocalDatabase::scan(Cursor *cursor,
                     FunctionObject functor, ham_u32_t flags);
        x traverses with the cursor
        x if a page does not have any keys which are modified in a transaction:
            directly run functor in the btree node
        x otherwise use the cursor itself and traverse the node
        x functions like sum() and average() would benefit from
            running directly on the PAX key array, instead of looping over
            the abstracted BtreeNodeProxy data; can we push those
            calculations down to the KeyList? The Visitor would then need
            several interfaces, and the engine decides which one is used:
            - "slow" traversal with cursor (if transactions are present)
            - fast traversal with BtreeNodeProxy iterator (variable-length keys)
            - ultrafast traversal with sequential PAX storage
        x implement the functions
    x improve the internal documentation

    x rewrite ham_db_get_key_count internals, unify with hola_count[_distinct]
        void LocalDatabase::count(Cursor *cursor,
                     FunctionObject functor, ham_u32_t flags);
        x ham_db_get_key_count: call hola function
        x should continue working remotely

    x hola_* must lock the Environment, perform a try/catch and allocate
        Visitor with auto_ptr

x Refactoring in the Btree
    x rename "find" to "find_child", "find_exact" to "find_leaf"
    x when traversing internal nodes: can we avoid calling
        BtreeNodeProxy::get_record_id(), and return the record-id when calling
        BtreeNodeProxy::find_child()? This would save one virtual function
        call and clean up lots of code
        x do we still need get_record_id() then? -> yes
    x do we really need iterators? the iterators usually just wrap the slot
        number. if the layout's interface is sufficiently clean then they
        should not be required and can be removed

x Refactorings for the PAX layout with the following goals:
    - responsible for ALL keys that are non-duplicate!
    - the key flags are no longer required (they store flags whether the record
        is inline or not)
    - the layout class no longer deals with inline/non-inline records; this
        is all handled by the RecordList
    - the RecordList is responsible for inserting, removing, accessing records 
    - the RecordList can then perform compression, i.e. grouped varints for
        record IDs
    - can we reduce the library size, i.e. by reducing template parameters?
    x move everything that's relevant for keys to the KeyList, and for records
        to the RecordList; the Layout will no longer directly access key
        data, record data, key/record flags etc
        x scan: move to KeyList
        x get_key: move to KeyList
        x get_record: move to RecordList
        x set_record: move to RecordList
        x get_record_size: move to RecordList
        x erase_key: move to KeyList
        x erase_record: move to RecordList
        x insert: split to both
        x split: split to both
        x merge: split to both
        x remove m_flags from the layout
        x clean up/remove/move everything else
        x remove set_key_flags, set_key_data, set_key_size
    x the DefaultRecordList is responsible for managing the flags and blobs
    x once and forall try to fix get_actual_key_size(); split to RecordList
        and KeyList, rename to get_initial_page_capacity() and fix the code
        flow in the caller
        x remove RecordList::is_always_fixed_size
        x remove RecordList::get_max_inline_record_size

o default layout: the lookup operations are relatively slow compared to PAX;
    can we somehow improve this?
    o run profiling on a read-only workload, w/o records

o split cursor.cc in two files because gcc can run out of memory when compiling

o Refactorings for the Default layout with the following goals:
    -> responsible for duplicate keys ONLY!!
    -> UpfrontIndex manages freelist, upfront index
    -> KeyList manages the keys, incl. extended keys
    -> RecordList manages the records, incl. duplicate tables
    x rename FixedLayoutImpl -> FixedKeyList
    x rename DefaultLayoutImpl -> DefaultKeyList
    x create UpfrontIndex member
        x capacity handling
        x freelist handling
            x get_freelist_count/set_freelist_count
            x freelist_add -> copy_to_freelist
            x freelist_find -> find_in_freelist
            x freelist_remove -> remove_from_freelist
        x offset handling
            x get_key_data_offset()
            x get_record_data_offset()
            x calc_next_offset()
    o create DuplicateTable class, derive from ByteArray, move logic to this
        class
        o insert
        o erase
        o ...
    o improve RecordList separation
        o including duplicate handling (and the DuplicateTable cache)
        o try to create interface similar to PAX RecordList
        o template-parameter von KeyList übertragen (Offset, HasDuplicates)
    o rewrite one high-level function after the other
    o introduce KeyList, move relevant functions
        o including extkeys (and their cache)
        o try to create interface similar to PAX KeyList

    o create UpfrontIndex for fixed length keys w/ duplicates: store keys
        sequentially, move RecordList to the "end" of the node
    o create UpfrontIndex for variable length keys w/o duplicates: store records
        sequentially, move RecordList to the "end" of the node
        -> maybe reuse PAX RecordList??

o the steps above will create an incompatible file format
    o increment file version
    o collect other file format updates
        o PageManager state: also store m_last_blob_page_id persistently
        o EnvHeader is completely full; add at least 8 bytes for
                flags + reserved
        o reserve CRC32 for each page
        o PAX Layout: can compress the flags for the DefaultRecordList

o delta updates managed in the BtreeNode
    the operations are attached to the node, but as soon as the node
    is accessed for a lookup or a scan, or immediately before the node
    is flushed, the deltas are merged into the node. So far this does not
    sound too efficient, but the bulk updates in combination with the
    key compression (i.e. for prefix compression) will benefit a lot.

    Also, they are really required for concurrency and to allow multiple
    writers in parallel.

    x perform a benchmark/profiling: random inserts vs. ascending inserts;
        the difference should be caused by memcopy/memmove (is it?)
        x PAX
            -> absolutely worth the effort, about 60% are spent in memmove
        x Default
            -> also worth the effort, about 15% are spent in memmove

    o need a flag to disable DeltaUpdates
        o add flag to ham_bench
    o rename TransactionOperation to DeltaUpdate, decouple code from txn
    o totally transparent to the caller, handled in the proxy
    o only add deltas to leaf nodes; internal nodes have too many read
        operations and would anyway require immediate flushes
    o DeltaUpdate objects from a txn-flush should directly go down to
        the node (detach from txn, attach to node)
    o should the the insert + set_record operations be combined into a
        single call? this has the additional advantage that the overhead
        of calling set_record will disappear
    o merge delta updates when reading and flushing
        o however, for simple lookup calls (no duplicates) the DUs can
            be traversed, too 
    o requires_split() must take delta updates into account
    o make the merge algorithm as efficient as possible
        o sort deltas by key
        o first execute all 'erase' either against other deltas or against
            the node
        o then merge the remaining inserts
        o this needs lots of tests
    o now run tests: should every update be stored as a DeltaUpdate? If not
        then disable them by default, unless Transactions are used (and unless
        bulk updates are used)

o collect/publish benchmark against leveldb (with and without SSD)
    - each with 100k and 100m keys
    o uint64, small records, random write
    o uint64, small records, linear write
    o uint64, small records, random read
    o uint64, small records, linear read
    o bin16, default records, random write (w/o compression)
    o bin16, default records, linear write (w/o compression)
    o bin16, default records, random read (w/o compression)
    o bin16, default records, linear read (w/o compression)
    o bin16, default records, random write (w/ compression)
    o bin16, default records, linear write (w/ compression)
    o bin16, default records, random read (w/ compression)
    o bin16, default records, linear read (w/ compression)
    o uint64, calculate sum()
    o bin16, use count_if() of all keys with checksum % 10 == 0

o documentation TODOs for 2.1.8
    o hola: add samples and documentation
        o add samples to webpage
        o tutorial
    o FAQ: Why is the library size so big?

. hola - next steps
    o support java api
    o support .net api
    o support erlang api
    o PRO: lua-functions as callbacks - then remote marshalling will work
    o add remote support where it makes sense (only for PRO?)

. QuickCheck: create a new property for testing duplicates; the key is always
    the same. The number of duplicate keys is tracked and periodically
    checked with the API. A cursor can be used to remove a specific
    duplicate, or to fetch a specific duplicate.

. look into hive-integration
    -> would be an interesting market with requirements for column store
        storage
    -> how does the integration look like?
    -> has TPC-benchmarks and other benchmarks ready
    o write down everything i find, collect information and estimate
        efforts/gains (will do the same for mysql, R and mongodb later); this
        task is not about writing code!

. architecture for a new webpage
    -> will first start with the admin page, and the client's page for
        downloading, creating support requests etc
    o pick an awesome design
        i.e. similar to http://foundationdb.com, http://laravel.com,
        http://rethinkdb.com
    o pick a framework for routing and infrastructure (phalcon? laravel?)
    o page contents are written in markdown, textile or
        reStructuredText (http://sphinx-doc.org/rest.html)
    o Makefile converts markdown to html
    o Makefile can "scp -r" everything to the servers (staging or production)
    o client area with
        o authentication
        o collection of files
        o analytics (who downloads what and when?)
    o admin area with
        o authentication
        o customer database
        o implementing business processes
        o sending out release emails
        o importing new releases
        o etc
    . come up with the full site structure/contents
    . documentation comments are hosted on disqus
        o try to seamlessly integrate the doxygen documentation,
            the javagen documentation, the erlang docs and the .NET docs 
            (and whatever comes in the future)
        o keep the documentation in the source tree, not in -www?
    . blog comments are hosted on disqus, articles are also written in rst

------------- hamsterdb 2.1.8 ---------------------------------------

. PRO: add avx support
o PRO: add sse support for real64

o PRO: CRC32-checksums (as soon as the file format is updated)
    o SSE2 has support for a CRC32 checksum calculation, or find a good
        library

o PRO: enable SIMD in the default layout if it uses the same KeyList as
    in PAX *and* if record IDs are stored separately from the keys

o PRO: bulk updates (really only for pro?)
    - require delta updates
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort, but
        fail hard if the sort order is violated
    - add those delta updates to the txn-trees or to the btree node,
        simply transfer ownership of the memory
    -> or are these "batched" updates, in combination with cache-oblivious
        btrees? the batched updates contain lists of structures with
        information about the update (i.e. insert, erase etc). internally,
        a cursor is used to perform the update. this would be fast and a
        relatively cheap way to perform multiple operations in one
        single transaction. and they would not require delta updates, but
        still provide real value.
    o also for remote, transfer in a single message
    o also for .NET
    o also for java
    o also for erlang

o PRO: prefix compression for variable-length keys
    ==> not efficient for random read/writes, but that's ok. linear scans
        and appends will nevertheless be fast, and the delta updates
        will make it efficient too 
    ==> when performing bulk updates/batched updates, make sure that the
        page is compressed only once
    ==> only for leaf keys!? internal keys have too much "distance", and
        the decompression would require too much time
    ==> implement this as an "aspect" for the default-layout with
        variable-sized keys

    o if key is appended at the end: just write the delta
    o every "n'th" (50th?) key is written uncompressed
    o otherwise append a delta-update to the page, and "merge" all deltas
        before the page is flushed. This will improve performance for batched
        updates, since they will cause only one page compression for
        multiple updates.
    o however, it's tricky to figure out whether a node requires a split or
        not, since it requires a good estimate of the size for the new key 
        -> if in doubt then just perform the merge
    
    o full keys can be further compressed with lzf or lzo
    o key search: jumps from full key to full key; in between, there's a
        linear search for the key

o PRO: think about a bitmap index for recnos; one bit per key, and grouped
    varints for the records
    -> can a bitmap be used for other indices as well? theoretically yes;
        but a split can be a bit tricky at a later stage. everything else
        should be very fast and compact!

o PRO: use grouped varints for compressing int32/int64, record IDs and
    duplicate tables
    https://github.com/stuhood/gvi/blob/master/src/main/java/net/hoodidge/gvi/GroupVarInt.java
    http://www.oschina.net/code/snippet_12_5083
    http://www.ir.uwaterloo.ca/book/addenda-06-index-compression.html
    -> the concept will be similar to prefix compression
    -> can we come up with a schema that allows compression AND
        fast random access?

------------- hamsterdb 2.1.9 ---------------------------------------

o use cache-oblivious b-tree layout
    -> http://supertech.csail.mit.edu/cacheObliviousBTree.html
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    o after resize: mmap the whole file area. this is actually important because
        mmap is much faster than r/w; but when the database is created, the
        original mapping already exists. therefore we might have to handle
        more than one mapping in the file
    o PageManager: when allocating a new page then use the distribution
        function to fetch a page from the reserved storage
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)
        -> could create a second memory mapping for the next chunk

. binary search in the nodes: keep track of the last result, use this as
    a starting point? or use a weighted search with "hot spots" for the first
    few lookups? this would require tests with different lookup characteristics,
    i.e. zipfian instead of purely random. bad for benchmarking.

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o PRO: hot backups (vacuumizes to a different file)
    - copies the database file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    - then applies all committed transactions to the other file
    --> think this through; how to deal with delta updates? -> merge them
        what if only a few databases should be backed up?
        what if i want to back up in a logical format (i.e. csv)?

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    -> see supersonic:
        https://code.google.com/p/supersonic/
    -> see fast bitmap indices
        http://code.google.com/p/lemurbitmapindex/
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?
    o use async operations or futures/promises
    o deprecate ham_db_get_key_count() (tutorial, documentation)

- bloom filter -> PRO
- concurrency -> PRO

o the bucket for concurrency TODOs
    o reduce the linked lists - they're hard to be updated with atomic
        operations
        o page
        o transaction and dependent objects
        o ...

    o come up with a list of all functions, define which locking operation
        is required; then review the code and make sure this will work
        o the environment configuration
        o the database configuration
        o the transaction tree handling
        o the page manager, the device and the cache
        o the btree
        o the btree nodes (i.e. extkeycache, compressor)

    o come up with a list of functions for which concurrency makes most sense
        - parallel lookups (using the same memory arena)
        - flushing transactions asynchronously
        - purging caches asynchronously
        - async. merging of delta updates
        - have concurrent lookups/find/inserts (with delta updates)
    o separate SMOs from the actual operation (#2)
        -> check the literature
        http://pdf.aminer.org/000/409/763/b_trees_with_relaxed_balance.pdf
        o move SMO operations to "the janitor" (btree_janitor.h)
    o the global environment-lock should go because it's expensive; rather
        increment an atomic latch, and refuse to close/erase the database as
        long as the latch is > 0 

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

o btree_impl_default::set_record: if the duplicate is LAST of the last key
    in the node then simply append the record and increase next_offset

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for Transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)
