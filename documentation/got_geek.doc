/*
 * Copyright (C) 2005-2009 Christoph Rupp (chris@crupp.de).
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2 of the License, or 
 * (at your option) any later version.
 *
 * See files COPYING.* for License information.
 */
/*! \page got_geek hamsterdb Internal Implementation Details - The Nitty Gritty

TODO:

\section stats_gathering_techno Statistics Gathering and Hinting Internals

the statistics gathering: how it's done exactly; the thoughts behind

\subsection The three Levels of Hinting

It's really 2.5 levels, but that depends on how you look at tit: find/erase/insert come with their own hinter; unfortunately it's rather crude and limited functionality for erase, insert is little better and the find is the one who saw the most effort up to now.

Then there's the inser-related freelist hinting: 2 levels: 1 global level which teels us which freelist pages (entries) we should search (the .5 level) and that's followed by a magic mix of a special iterator combined with a local hinter, which tells us which bitarray-section to inspect.

The iterator is an important piece of magick-k-k: it allows us to 'sample' the freelist entries (pages), depending on what the global hinter told us we should do (i.e. it does for us what the global hinter hinted we should be doing) while doing some additional extra work for both regular searches (tiny sizes and reasonably sized BLOBs which still fit within a single freelist entry) and ELBLOB searches which are allocation requests spanning multiple freelist entries, guaranteed. The latter is resolved in a somewhat harsh way, but it's fast, and another example of Boyer Moore speeding up our scan/search. This time, however, there's both forward and backward scanning, depending on the DAM settings, while each (forward and backward/reverse scan) employ Boyer Moore: the special iterator takes care of the Boyer Moore index stepping for us (and a few odds an' ends), so our code outside the iterator can keep on looking understandable.

\subsubsection tie-in of magickal iterator, global hinter and ELBLOB searches

A special mention is deserved for this one, because the run-time flow for this scenario will surely get all youse knickers in a twist, surely. After all, it's a up/down selectable Boyer-Moore scan, set up by the global hinter, and then executed through the iterator for the Boyer-Moore outer loop stepped scan, while the sequence scan (inner loop) of Boyer Moore is situated elsewhere in the code: the function which calls that magickky iterator on every round.




\section approx_match_techno Approximate Matching and the B+tree

the LT/GT/etc. 'approximate matching' find operations - how it works and what it does when it hits a B+tree edge




\section BLOBs and ELBLOBs (Extremely Large BLOBs)

Overflowing a freelist page, what's a freelist page, how we cope with that





\section freelist handling

what's a freelist page, how is the freelist organized: chunked array of bits, one bit per DB_CHUNKSIZE


\subsection speed improvements in v1.1.0

what was done to get it to work faster: Tuned Boyer Moore for binary alphabets, pardon, multiple alphabets really: 64-bit 3-valued alphabets, 8-bit 3-valued alphabets and 1-bit binary alphabets, leading to a piece of code spanning nearly 60% of all freelist-related code in the hamster. Woof!

Given the 'local hinter' which tells us which section of the current freelist entry MAY carry viable free slots, this info is used to set up all those Boyer-Moore-derived scans, while the requested size is used to pick the 'most appropriate' Boyer Moore scan of the lot: in reality we've 3 (ore more?) full-fledged Boyer-Moore space scan implementations rolled into one single, large, function.

Yes, that could be cut up into multiple functions, but why bother, eh? ;-)


\subsubsection statistics gathering for freelist results

The BM scans and everything that's freelist will feed into the statistics gathering code section, whether the search failed or succeeded. The statistics collector(s) gather this data and use it to improve the subsequent freelist global and local hints: global hints are improved when we fail and thus discover that a given freelist page X doesn't have enough free space for a request of size S (or larger), which means we don't want to visit this freelist page again, but instead have a look at the 'next' one: and it's up to the DAM mode to help decide what 'next' actually means here.

Then there's the UBER/FAST mode, which turns the whole sequential scanning business into a freelist page sampling show.

Caveat Emptor: the downside of this one is the fact that fails due to full freelist pages cannot 'bump up' the starting freelist page index any more as we do not know if some previous page might have sufficient free slots still: since we sample the freelist semi-randomly now, we cannot say that page X-1 is not adequate when all we've noticed so far is that page X is not adequate.

And we haven't gone all the way and keep bitarrays marking 'viable & non-viable' freelist entries/pages: *that* would have still worked while sampling in UBER/FAST mode as each failing page can then be tagged, but the overhead to do this was deemed too large. Since it is a significant performance drain (or so we expect it to be, no hard data for this lemma yet) for sustained UBER/FAST mode insert, we might consider implementing those marker bitarrays anyway. But not now. Maybe in a next life/revision of the hamster.









\subsection the 16-bit boundary: caveat emptor!

Known issue in v1.0.x where the tail end of large pages is lost to the freelist; why there still is a freelist in there after all (thanks to the header being subtracted and then the modulo thing happening: size no longer 'a multiple of' so we're in luck today




\section callbacks and filters

filters can be stacked; now they can add headers and footers and then there's this stuff about lead-in and lead-out or leading slack and trailing slack. A pciture says a thousand words for this one: show a cascade of filters and how they get their data positioned and what we expect from them.

Then there's the question whether the hamster is re-entrant, which is mandatory if we wish to call hamster API functions from inside a filter callback.

Have a look at a few (sophisticated?) scanerios, e.g. data encryption. Now including the header page!







\section Mail dump to be sifted

Klunk. The mail trail.


















\section hamsterdb Contact Form inquiry
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Christoph Rupp
 to ger
	
show details Jul 16
	
	
Reply
	
	Follow up message
Hi Ger,

sorry for the late reply - i just came back from two weeks of vacation.

wow - your changes sound amazing! I'm very curious about them and
looking forward to the diff!

svn failed because svnserve wasn't running - i switched the server
some weeks ago and forgot to start it (I tunnel my accesses through
svn, therefore
i never discovered these problems). Everything should work now.

Best regards
Christoph


regards
Christoph

2009/7/15 hamsterdb Contact Form <cruppstahl@gmail.com>:
> hamsterdb contact form
>
> name: Ger Hobbelt
> email: ger@hobbelt.com
> company: hobbelt b.v.
>
> Hi,
>
> I was wandering if I've screwed up lately or that your SVN is down/gone; this invocation:
>
> svn co svn://www.hamsterdb.org/home/chris/repos/hamsterdb/trunk
>
> produces
>
> svn: Can't connect to host 'www.hamsterdb.org': Connection refused
>
> since a few days.
>
> Reason:
>
> I wanted to fetch the very latest again for Hamster 1.0 as I've been using it for a while now and made a couple of fixes/tweaks plus introduced a new functionality which is VERY handy, nay, mandatory, if you want index time series: 'approximate matching' a la good ol' VMS:RMS $FIND (RAB$V_KGE et al): find() can now produce the nearest greater/lesser record for a given key [when an exact match is lacking] instead of just the exact match [or a record-not-found error].
>
> This is very useful when storing time series data (which is sampled at regular or irregular intervals [jitter]) while the code retrieving such info can now simply specify such a timestamp as a key without the need to magically 'know' _exactly_ _that_ timestamp exists in the collection.
>
> Anyway, long story short: wanted to see if I could produce some diffs for all this to send to you.
>
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 17
	
	
Reply
	
	Follow up message
I hope you enjoyed your holiday :-)

Will check the svn access now; diffs may take a few days as it's done
while taking a break from pressing matters (for which I use hamsterDB)
and I have to 'decode' again what was done what for in a few places,
so you can have a bit of leggible documentation with the various
patches.

Up front warning/note: the changes have been tested on 32- and 64-bit
Windows platform extensively; the same sources have not seen much
scrutiny on UNIX yet - at least not after I added the 'approximate
matching' stuff. Should be portable though. Also lacking up to now is
including this stuff in your tests; the major tests have been done
from inside my own custom software.

Will send stuff to check out when time allows.

Expect a series of small and a few (very?) large diffs. Will include
the source files as-is as well -- I find that diffs aren't all that
nice to have when you want to inspect code changes on your own
(Vertrauen ist gut, Kontrolle ist besser, that sort of thing): I use
diff viewer Beyond Compare on Windows to view and merge changes
manually; for such graphical diff/merge tools, it's handy to have two
full copies side by side.

Till later,

Ger


To get a taste of things, here's the hamster API header file as
attachment: check out the comments/doc blocks for _find(),
cursor_find() and _cursor_find_ex() in particular.
The find_ex() is new; so are a few other support API calls, which I
use to dimension the Btree with large pages without getting an error
due to the 64K items-per-page limit -- the app using hamster is
storing 40+M records in a few tables (time series storing stock trade
transactions and some other stuff).

I plan to adjust the _LT and _GT defines to declare them as _LE and
_GE (less or equal, greater or equal) as THAT is what they do today,
really. LT and GT operations can be derived from that by checking the
returned key flags (if the 'approximate match' flag bit is set) - see
also the new is_approximate API function (the flag bit is a
hamster-internal flag bit now).

Yes, the interface isn't 'solid' yet: it does very well what I need it
to do, but for general purpose use and complete API 'cleanliness' I
want to provide an easy way to have EQ, LE, GE and LT, GT find()
operations available at the hamster API interface level using flags a
la VMS $RMS; with the set as it exists today, one can construct each
of these ops easily, but it's 'nicer' to have it done by hamster
itself from an API ease-of-use point of view IMO.

Anyway, it's bloody late, I'm blabbing and I should check out my bed instead.
- Show quoted text -







On Thu, Jul 16, 2009 at 10:43 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> sorry for the late reply - i just came back from two weeks of vacation.
>
> wow - your changes sound amazing! I'm very curious about them and
> looking forward to the diff!
>
> svn failed because svnserve wasn't running - i switched the server
> some weeks ago and forgot to start it (I tunnel my accesses through
> svn, therefore
> i never discovered these problems). Everything should work now.
>
> Best regards
> Christoph
>
>
> regards
> Christoph
>
> 2009/7/15 hamsterdb Contact Form <cruppstahl@gmail.com>:
>> hamsterdb contact form
>>
>> name: Ger Hobbelt
>> email: ger@hobbelt.com
>> company: hobbelt b.v.
>>
>> Hi,
>>
>> I was wandering if I've screwed up lately or that your SVN is down/gone; this invocation:
>>
>> svn co svn://www.hamsterdb.org/home/chris/repos/hamsterdb/trunk
>>
>> produces
>>
>> svn: Can't connect to host 'www.hamsterdb.org': Connection refused
>>
>> since a few days.
>>
>> Reason:
>>
>> I wanted to fetch the very latest again for Hamster 1.0 as I've been using it for a while now and made a couple of fixes/tweaks plus introduced a new functionality which is VERY handy, nay, mandatory, if you want index time series: 'approximate matching' a la good ol' VMS:RMS $FIND (RAB$V_KGE et al): find() can now produce the nearest greater/lesser record for a given key [when an exact match is lacking] instead of just the exact match [or a record-not-found error].
>>
>> This is very useful when storing time series data (which is sampled at regular or irregular intervals [jitter]) while the code retrieving such info can now simply specify such a timestamp as a key without the need to magically 'know' _exactly_ _that_ timestamp exists in the collection.
>>
>> Anyway, long story short: wanted to see if I could produce some diffs for all this to send to you.
>>
>>
>
>
>



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb.h	hamsterdb.h
79K   Download   


































































\section hamsterdb Contact Form inquiry --> SVN checked
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 17
	
	
Reply
	
	Follow up message
SVN checked.

WORKS :-)

Only the dotnet and python svn commands fail (not really important to
me now as I use neither at the moment, but it might either be me
screwing up there or documentation on hamster website off?

Anyway, my fetch UNIX shell script is attached

dotnet svn:

svn://www.hamsterdb.org/home/chris/repos/hamsterdb-dotnet/trunk

+ python svn says here:

~/prj/1original/hamsterdb/dotnet ~/prj/1original/hamsterdb
./_update_from_cvs.sh: line 18:
svn://www.hamsterdb.org/home/chris/repos/hamsterdb-dotnet/trunk: No
such file or directory
~/prj/1original/hamsterdb
mkdir: cannot create directory `python': File exists
~/prj/1original/hamsterdb/python ~/prj/1original/hamsterdb
./_update_from_cvs.sh: line 25:
svn://www.hamsterdb.org/home/chris/repos/hamsterdb-python/trunk: No
such file or directory
~/prj/1original/hamsterdb


ignore the mkdir warnings; that's me being lazy and blunt in my svn
fetch script.

--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
_update_from_cvs.sh	_update_from_cvs.sh
1K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 17
	
	
Reply
	
	Follow up message
Hi Ger!

Thanks for the header file. I'm very excited about the changes - this
is the first big patch that i'm receiving!

I will have a close look at the diffs, and i already found some things
which i would like to change because i feel that they are a bit
inconsistent, i.e.

ham_calc_maxkeys_per_page -. this is a very specific function. I tend
to move such specific stuff to hamsterdb_int.h (where it's still part
of the public API but a bit separated from the "mainstream" stuff).

ham_cursor_find_ex: the only difference to ham_cursor_find is that
this one also retrieves the record. Currently, the usage pattern is:
   ham_cursor_find(cursor, key, ...) -> positions the cursor
   ham_cursor_move(cursor, NULL, record, NULL) -> retrieves the record
The ham_cursor_move does not move the cursor (no direction flags were
specified) and it is a very cheap operation. This usage pattern is
also the one used by BerkeleyDB. I don't know if your implementation
of ham_cursor_find_ex has additional code, but if it just saves one
function call then maybe it's not big enough for its own API function
(which would also require documentation and code for C++, Python, .NET
etc).

Regarding svn i knew that the python repository was completely and
utterly broken - the svn database was corrupt and i was not able to
restore it. I recreated the repository, it should work now. The dotnet
stuff seems to work, i used this command:

 svn co svn://www.crupp.de/home/chris/repos/hamsterdb-dotnet

(crupp.de or hamsterdb.com/.org have the same ip address).

Also, when i look at your shell script, it seems that the "svn co" is
missing for the dotnet and python lines?

Regarding your changes there's one (nasty) legal thing to discuss. You
may have seen that i intend to sell commercial licenses (non-open
source) on my webpage. In order to dual-license my code, i would need
a *copy* of the copyright of your changes. If you agree, then as a
compensation i would offer you one or two commercial licenses (i
cannot offer you money because i don't earn any with hamsterdb - so
far it's just a hobby). I will also take care of the unittests and
testing on different platforms. If you disagree and want to release
your code as GPL-only, then i'll create a contrib-directory with your
patches, and i will make sure that the patches continue to work with
future versions.

(This is the one for OpenOffice and other SUN projects:
http://www.sun.com/software/opensource/sca.pdf)

The commercial license is licensed per-developer. You could create as
many commercial projects as you want, without additional royalties or
whatsoever.

Best regards
Christoph

2009/7/17 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> SVN checked.
>
> WORKS :-)
>
> Only the dotnet and python svn commands fail (not really important to
> me now as I use neither at the moment, but it might either be me
> screwing up there or documentation on hamster website off?
>
> Anyway, my fetch UNIX shell script is attached
>
> dotnet svn:
>
> svn://www.hamsterdb.org/home/chris/repos/hamsterdb-dotnet/trunk
>
> + python svn says here:
>
> ~/prj/1original/hamsterdb/dotnet ~/prj/1original/hamsterdb
> ./_update_from_cvs.sh: line 18:
> svn://www.hamsterdb.org/home/chris/repos/hamsterdb-dotnet/trunk: No
> such file or directory
> ~/prj/1original/hamsterdb
> mkdir: cannot create directory `python': File exists
> ~/prj/1original/hamsterdb/python ~/prj/1original/hamsterdb
> ./_update_from_cvs.sh: line 25:
> svn://www.hamsterdb.org/home/chris/repos/hamsterdb-python/trunk: No
> such file or directory
> ~/prj/1original/hamsterdb
>
>
> ignore the mkdir warnings; that's me being lazy and blunt in my svn
> fetch script.
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 19
	
	
Reply
	
	Follow up message
Christoph,

Had a longer reply waiting in the pipeline, but your comments sparked
a few things and then I ran straight into a brick wall. Ah well. I've
met worse.
There's some wickedness going on in the freelist code: running the
unittests in Windows XP64 (native 64-bit) produces an assertion
failure.
And running the beast on Linux64 (latest Ubuntu) gives a segfault in
EnvTest::createCloseOpenCloseWithDatabasesTest

Don't bother about those yet; I've patched the code, so if might be
something due to that. :-S


Anyway, because I'm still in debug Hell I'd thought I could reply to
at least a few points already:

first off, the license issue.

From my point of view, it's simple. There's two requirements that I have:

1) my name gets listed somewhere in the credentials for introducing
the lt/gt 'approximate matching' find feature. Doesn't need to be big,
just part of the list of people who've done something. The standard
CREDITS file is a prime candidate for that. This is irrespective of
the software license attached to the work.

Reason: when googled, my name should pop up in some way in the list of
people who contributed. It's okay when hamster turns up on the radar
when people do a background check on me.
Second reason: I've had a few bad experiences with commercial credit
ripping, i.e. someone taking my stuff, selling it to customers but not
crediting me for the original work. Not much you can do about it, but
I'd like the old adage 'credit where credit is due'. Which leads to
#2...


2) Regarding GPL/OSS & commercial: you can do anything with the
sources, but on one condition: any unbilled work of mine ends up in
the GPL distribution as well. I.e. anything I do for free, shows up in
any related free channels. I do not have any qualms about you adding
the same sources to the commercial release and tacking another license
on it in there; that's fine.
In fact, in my opinion, it would rather complicate matters when my
stuff would make those two distributions divergent by you having to
exclude it from the commercial package. And besides: heck, it's okay
if you sell a couple of licenses extra - I'm not interested in the
license fees, I'm interested in the longevity of the hamster source
code and I believe commercial sales might help there.

Reason: my major 'gain' in this is that when you decide to include my
changes, I can 'track' your mainline source distro with much less
effort over time (= years): less differences between my 'tweaked
version' and mainline.


When curious, see the (shoddy) website http://hebbut.net/ : this is
where I put my 'open source' work on-line when I finally take the time
to write a page for each lib or tool. I assume you don't mind, but an
(edited) hamster will show up there later: all the projects there come
with custom MSVC project files and solutions which fit a rather strict
build environment (which, granted, I should document on-line as well -
it's in there, but in a rather obscure spot now); I have those on-line
so I (and others besides me) can grab those archives and use them
without having to do the conversion work again. Yup, most of my work
is done on Windows platforms...


That should clear the license/copyright issue: credit where credit is
due, pass free stuff on, and for the rest: enjoy. My free work can be
included in commercial products, as long as the credits condition is
met anywhere.



Regarding the headerfile:

I have come to thing of hamsterdb_int.h as an 'internal only' header
file (probably due to the /brief up the saying it's hamster internals)
and hence 'off limits' for upper levels which access the hamsterDB
API.

Thanks to your comments I've done a bit of a shuffle in the code so as
to truly support LT/GT/LE/GE searches next to EQ (EXACT). The LT/GT
defines as you have them are a white lie; they really mean 'Less or
Equal' and 'Greater or Equal', i.e. LE/GE really. That's how I use(d)
them anyway.

The calc_keys API is a bit of a odd one, really, because I had a need
for 'maximum number of nodes fitting in a page' info as I'm working
with huge tables (or better put: in the process of working with them;
speed is acceptable, but I've been scratching my head at some results
I see, where large keys and records result in a surprisingly slow
insert operation; preliminary finding is that a lot of time is spent
in the freelist bitarray scanning then, but I've tabled that one for
later; conditions when this happens are currently limited to some
unittest code sections, but it worries me a bit anyway).
Anyway, I was considering moving that calc_keys API into the new
get_env/db_params() API which was added as well to report on
database/environment settings.



Today, I'm working on the unittests; your mention of them triggered
this and I found a few quirks while writing the test code. Oops. More
on this later, but I'll see if I can somehow wrap this up in a
satisfactory way. A side effect has been that those two int_to_pub and
pub_to_int compare routines have been cut down to one only, as I went
completely nuts when debugging the internals and having to think about
what a cmp == -1 would mean exactly right _here_, contrary to over
_there_.
I can see how those came to be, but one consistent
direction-of-comparison helps me a _lot_ in analyzing the edge
conditions for LT/LE/GT/GE checks; they're not that trivial, after all
;-)



Hm, allow me to send you a new hamsterdb.h at least - doc in there has
been adapted and one of the new doc blocks is utter copy&paste
erroneous cruft, be forewarned, that's how I code up and you're
getting something that's seen a bit of an edit this weekend. I hope
you don't mind; if you can't stand midwork snapshots, leave it be,
that's alright with me.



I have done a visual comparison of my current tree and your SVN and I
am pondering how to approach this; the plan now is to extract the bug
fixes and minor tweaks first, write up the when and why of them, and
then descend on to the major work of find/find_ex.



ah! Almost forgot about that. Had written down my arguments why, but
the short end of it all is: I am interested in speed. LOTS of it. (My
target is 10+M keys per table; a proof of concept munches through 1M
records already. And before hamster, I had a custom memory-mapped file
+ custom-tailored hash-based index solution (given my time-series
data, one can construct quite fast indexes using semi-hash mechanisms,
but a B+tree+binary search based LE/GE index is fine as well: I need
to pick record N, then N+1 and interpolate between the two. Or, when N
is spot on time-wise, NO interpolation needed.)
And to come back to this need for speed: two api calls may cost
little, but one, which's already got the lot, costs less. Because when
you dig down into hamster, you quickly find that find internally
already delivers everything the find+move usage pattern does; it's
just pointers that NULLed on the way up. That's nice for a EQ find,
but now that LE/GE find is here, the key is already edited at find
time, so why don't we export record out as well: it's much cheaper to
do that in terms of clock cycles (yeah, I now, minor stuff, but still,
at a couple of million calls, it adds up). And that's where find_ex()
comes in: it's there as I didn't want to change the find() interface
itself for reasons of backwards compatibility: after all, this was
done with half a mind on feeding this back into the mainline (you) and
given that others are using this as well, changing APIs in major ways
might be frowned upon.

And in case you don't want find+find_ex, my proposition then is to
make find into find_ex, adding one extra argument in the next
release's API then. After all, find() already reads like this:

-----
ham_status_t HAM_CALLCONV
ham_cursor_find(ham_cursor_t *cursor, ham_key_t *key, ham_u32_t flags)
{
       return ham_cursor_find_ex(cursor, key, NULL, flags);
}
-----


Given the current internals, there's zero extra costs in run-time in
supporting both patterns: find[_ex]() coughing up key+record, or
find[_ex]+ move(). I mention this to show that the existing usage
pattern can remain as is at no extra charge, while I get my way with a
faster find() giving me all the goods in one go. ;-)
Note though that the latter pattern (find+move) will loose a few bits
of info on the key in the end ("Is the key an exact match, or is it
larger or smaller than the value searched for?" is a question answered
by the key flags upon find[_ex]() return). More on this later.



As I write this, I think I might benefit from your help on Linux with
the unit tests when you have time later. There's a nasty segfault in
there that doesn't happen in the Windows setup.

Signing off, back to work on this. Got to stresstest the new
LT/GT/GE/LE code. (which spares out another key compare or two so it's
ever so slightly faster)
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 19
	
	
Reply
	
	Follow up message
Hi Ger,

thanks for the (long!) mail!

Regarding licensing issue: you will absolutely get all the credit you
asked for. So far contributors (which were bug reports) were listed in
the CHANGELOG file, but your patches are reason enough to create a new
CREDITS file. Also, i can offer you to add a link to your webpage(s)
on the frontpage of hamsterdb.com listening your contribution and
linking to your hamsterdb library/application.

Of course it's the best for me if i don't have to maintain two
branches of hamsterdb so i'm quite happy that you're ok with me using
your code for commercial and foss licenses. My offer of one (or two,
if you want) commercial licenses still stands and i'm happy if you
decide to accept it.

Regarding the header file your points make sense and i don't have
reasons not to follow them. I cannot change the interface of
ham_cursor_find (and ham_find) for backwards compatibility reasons,
but i don't mind adding the new _ex functions.

Regarding your patches and the problems you currently have: i know
that some parts of hamsterdb (esp. the comparison function) are so
tricky that they require cleanup.

I'm now planning to release the very first version of hamsterdb2 today
or tomorrow, and then i'm ready to work on hamsterdb. So feel free to
send me stuff that's not perfectly working, then i will help you
debugging/profiling. My main platform is anyway linux. I only have
limited time (family, daily work etc) but i'll happily invest it in
your changes. You can also send me your application that you use for
benchmarking, and then we can try to squeeze the best performance out
of it (maybe we can even disable the freelist, if it's not really
necessary).

Have a nice weekend (at least what's left of it),
Christoph

2009/7/19 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Christoph,
>
> Had a longer reply waiting in the pipeline, but your comments sparked
> a few things and then I ran straight into a brick wall. Ah well. I've
> met worse.
> There's some wickedness going on in the freelist code: running the
> unittests in Windows XP64 (native 64-bit) produces an assertion
> failure.
> And running the beast on Linux64 (latest Ubuntu) gives a segfault in
> EnvTest::createCloseOpenCloseWithDatabasesTest
>
> Don't bother about those yet; I've patched the code, so if might be
> something due to that. :-S
>
>
> Anyway, because I'm still in debug Hell I'd thought I could reply to
> at least a few points already:
>
> first off, the license issue.
>
> From my point of view, it's simple. There's two requirements that I have:
>
> 1) my name gets listed somewhere in the credentials for introducing
> the lt/gt 'approximate matching' find feature. Doesn't need to be big,
> just part of the list of people who've done something. The standard
> CREDITS file is a prime candidate for that. This is irrespective of
> the software license attached to the work.
>
> Reason: when googled, my name should pop up in some way in the list of
> people who contributed. It's okay when hamster turns up on the radar
> when people do a background check on me.
> Second reason: I've had a few bad experiences with commercial credit
> ripping, i.e. someone taking my stuff, selling it to customers but not
> crediting me for the original work. Not much you can do about it, but
> I'd like the old adage 'credit where credit is due'. Which leads to
> #2...
>
>
> 2) Regarding GPL/OSS & commercial: you can do anything with the
> sources, but on one condition: any unbilled work of mine ends up in
> the GPL distribution as well. I.e. anything I do for free, shows up in
> any related free channels. I do not have any qualms about you adding
> the same sources to the commercial release and tacking another license
> on it in there; that's fine.
> In fact, in my opinion, it would rather complicate matters when my
> stuff would make those two distributions divergent by you having to
> exclude it from the commercial package. And besides: heck, it's okay
> if you sell a couple of licenses extra - I'm not interested in the
> license fees, I'm interested in the longevity of the hamster source
> code and I believe commercial sales might help there.
>
> Reason: my major 'gain' in this is that when you decide to include my
> changes, I can 'track' your mainline source distro with much less
> effort over time (= years): less differences between my 'tweaked
> version' and mainline.
>
>
> When curious, see the (shoddy) website http://hebbut.net/ : this is
> where I put my 'open source' work on-line when I finally take the time
> to write a page for each lib or tool. I assume you don't mind, but an
> (edited) hamster will show up there later: all the projects there come
> with custom MSVC project files and solutions which fit a rather strict
> build environment (which, granted, I should document on-line as well -
> it's in there, but in a rather obscure spot now); I have those on-line
> so I (and others besides me) can grab those archives and use them
> without having to do the conversion work again. Yup, most of my work
> is done on Windows platforms...
>
>
> That should clear the license/copyright issue: credit where credit is
> due, pass free stuff on, and for the rest: enjoy. My free work can be
> included in commercial products, as long as the credits condition is
> met anywhere.
>
>
>
> Regarding the headerfile:
>
> I have come to thing of hamsterdb_int.h as an 'internal only' header
> file (probably due to the /brief up the saying it's hamster internals)
> and hence 'off limits' for upper levels which access the hamsterDB
> API.
>
> Thanks to your comments I've done a bit of a shuffle in the code so as
> to truly support LT/GT/LE/GE searches next to EQ (EXACT). The LT/GT
> defines as you have them are a white lie; they really mean 'Less or
> Equal' and 'Greater or Equal', i.e. LE/GE really. That's how I use(d)
> them anyway.
>
> The calc_keys API is a bit of a odd one, really, because I had a need
> for 'maximum number of nodes fitting in a page' info as I'm working
> with huge tables (or better put: in the process of working with them;
> speed is acceptable, but I've been scratching my head at some results
> I see, where large keys and records result in a surprisingly slow
> insert operation; preliminary finding is that a lot of time is spent
> in the freelist bitarray scanning then, but I've tabled that one for
> later; conditions when this happens are currently limited to some
> unittest code sections, but it worries me a bit anyway).
> Anyway, I was considering moving that calc_keys API into the new
> get_env/db_params() API which was added as well to report on
> database/environment settings.
>
>
>
> Today, I'm working on the unittests; your mention of them triggered
> this and I found a few quirks while writing the test code. Oops. More
> on this later, but I'll see if I can somehow wrap this up in a
> satisfactory way. A side effect has been that those two int_to_pub and
> pub_to_int compare routines have been cut down to one only, as I went
> completely nuts when debugging the internals and having to think about
> what a cmp == -1 would mean exactly right _here_, contrary to over
> _there_.
> I can see how those came to be, but one consistent
> direction-of-comparison helps me a _lot_ in analyzing the edge
> conditions for LT/LE/GT/GE checks; they're not that trivial, after all
> ;-)
>
>
>
> Hm, allow me to send you a new hamsterdb.h at least - doc in there has
> been adapted and one of the new doc blocks is utter copy&paste
> erroneous cruft, be forewarned, that's how I code up and you're
> getting something that's seen a bit of an edit this weekend. I hope
> you don't mind; if you can't stand midwork snapshots, leave it be,
> that's alright with me.
>
>
>
> I have done a visual comparison of my current tree and your SVN and I
> am pondering how to approach this; the plan now is to extract the bug
> fixes and minor tweaks first, write up the when and why of them, and
> then descend on to the major work of find/find_ex.
>
>
>
> ah! Almost forgot about that. Had written down my arguments why, but
> the short end of it all is: I am interested in speed. LOTS of it. (My
> target is 10+M keys per table; a proof of concept munches through 1M
> records already. And before hamster, I had a custom memory-mapped file
> + custom-tailored hash-based index solution (given my time-series
> data, one can construct quite fast indexes using semi-hash mechanisms,
> but a B+tree+binary search based LE/GE index is fine as well: I need
> to pick record N, then N+1 and interpolate between the two. Or, when N
> is spot on time-wise, NO interpolation needed.)
> And to come back to this need for speed: two api calls may cost
> little, but one, which's already got the lot, costs less. Because when
> you dig down into hamster, you quickly find that find internally
> already delivers everything the find+move usage pattern does; it's
> just pointers that NULLed on the way up. That's nice for a EQ find,
> but now that LE/GE find is here, the key is already edited at find
> time, so why don't we export record out as well: it's much cheaper to
> do that in terms of clock cycles (yeah, I now, minor stuff, but still,
> at a couple of million calls, it adds up). And that's where find_ex()
> comes in: it's there as I didn't want to change the find() interface
> itself for reasons of backwards compatibility: after all, this was
> done with half a mind on feeding this back into the mainline (you) and
> given that others are using this as well, changing APIs in major ways
> might be frowned upon.
>
> And in case you don't want find+find_ex, my proposition then is to
> make find into find_ex, adding one extra argument in the next
> release's API then. After all, find() already reads like this:
>
> -----
> ham_status_t HAM_CALLCONV
> ham_cursor_find(ham_cursor_t *cursor, ham_key_t *key, ham_u32_t flags)
> {
>        return ham_cursor_find_ex(cursor, key, NULL, flags);
> }
> -----
>
>
> Given the current internals, there's zero extra costs in run-time in
> supporting both patterns: find[_ex]() coughing up key+record, or
> find[_ex]+ move(). I mention this to show that the existing usage
> pattern can remain as is at no extra charge, while I get my way with a
> faster find() giving me all the goods in one go. ;-)
> Note though that the latter pattern (find+move) will loose a few bits
> of info on the key in the end ("Is the key an exact match, or is it
> larger or smaller than the value searched for?" is a question answered
> by the key flags upon find[_ex]() return). More on this later.
>
>
>
> As I write this, I think I might benefit from your help on Linux with
> the unit tests when you have time later. There's a nasty segfault in
> there that doesn't happen in the Windows setup.
>
> Signing off, back to work on this. Got to stresstest the new
> LT/GT/GE/LE code. (which spares out another key compare or two so it's
> ever so slightly faster)
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 19
	
	
Reply
	
	Follow up message
On Sun, Jul 19, 2009 at 9:26 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> thanks for the (long!) mail!

I sometimes get the verbiage flu ;-)
Just in case you get a headache from receiving such texts; just send a
TL;DR request, no offense taken (TL;DR: too long; didn't read).

On with the show...

> Regarding licensing issue: you will absolutely get all the credit you
> asked for. So far contributors (which were bug reports) were listed in
> the CHANGELOG file, but your patches are reason enough to create a new
> CREDITS file. Also, i can offer you to add a link to your webpage(s)
> on the frontpage of hamsterdb.com listening your contribution and
> linking to your hamsterdb library/application.
>
> Of course it's the best for me if i don't have to maintain two
> branches of hamsterdb so i'm quite happy that you're ok with me using
> your code for commercial and foss licenses. My offer of one (or two,
> if you want) commercial licenses still stands and i'm happy if you
> decide to accept it.

I'd like to take you up on that offer; 1 license would be much appreciated.
And once I get to the point of putting something online myself, a link
to that is appreciated as well, but definitely not mandatory.


> Regarding the header file your points make sense and i don't have
> reasons not to follow them. I cannot change the interface of
> ham_cursor_find (and ham_find) for backwards compatibility reasons,
> but i don't mind adding the new _ex functions.

Okay, do I understand you correctly in that we then keep both
cursor_find around as-is plus new cursor_find_ex() ?



> Regarding your patches and the problems you currently have: i know
> that some parts of hamsterdb (esp. the comparison function) are so
> tricky that they require cleanup.

Oh, ah, don't get me started on that one. That's been a major struggle
this weekend; I'm quite angry at myself for being so slow on the
uptake with 'grokking' int_to_pub and pub_to_int: those buggers play a
nasty game as their cmp outputs are mirrored.
Anyway, no worries there: I cut the code down to using only one of
them and the get_slot etc. code now invokes those key compare calls
less often too, resulting in a minor speed up: less key comparisons
for the same results.


> I'm now planning to release the very first version of hamsterdb2 today
> or tomorrow, and then i'm ready to work on hamsterdb. So feel free to
> send me stuff that's not perfectly working, then i will help you
> debugging/profiling. My main platform is anyway linux. I only have
> limited time (family, daily work etc) but i'll happily invest it in
> your changes. You can also send me your application that you use for
> benchmarking, and then we can try to squeeze the best performance out
> of it (maybe we can even disable the freelist, if it's not really
> necessary).

You ain't gonna like this, but I'll take you up on your help offer
there as I've run into a kinda showstopper: can't get my brain around
the detailed why&how of the freelist, so I can't for the life of me
see where I should fix this bugger for real.
I've gone back to basics after fighting it this day, but I've been
augmenting the unittests and included a stresstest for find which is
going bonkers in insert() when I insert a large number of records. The
crazy thing is that I haven't seen this with my own app but now I
suspect that this, ah, 'explains' why I was getting some weird results
in that one; why the asserts didn't fire in my own app+hamster is big
question mark, but I've already seen that my MSVC2008 has been
skipping compiling one assert in the code that I added, while all
other asserts work as expected: after a few years, I seem to have run
into a compiler bug there.

Anyway, long story short: I'm currently prepping a minimal patched
source tree for just this assert crap-up and test it on Linux; if you
can have a look at that one and help me out by pointing where the
bugger's gotta be fixed, much obliged.

My next email will contain the diff for that showstopper; hm, will
probably package the whole tree as a tar.bz2 for ease as well.

(To make it perfectly clear:
issue =
when inserting many thousands of records into a DB, hamster at a
certain point barfs inside the free list code; it happens when the
free list is exhausted and things are expanded by allocating a page
for the next record; there's code in hamster which then adds the
remainder of that new page to the free list and while doing so it
<clunk> because the new freelist chunk (pointed at by fp in most
routines) has a zeroed _address and doesn't look like it's been
initialized as a freelist page at all anyway; I'm pulling hair to find
out where I fix this. What I'm going to send will exhibit this
behaviour; I'll probably have assertions forcibly turned on to show
what's is failing where.

Don't bother with this until you've got my package; no use losing time
over this until you can see precisely what's going on.)



> Have a nice weekend (at least what's left of it),

Thank you and same to you ;-)



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 19
	
	
Reply
	
	Follow up message
Hi Ger!

2009/7/19 Ger Hobbelt <ger@hobbelt.com>:
> On Sun, Jul 19, 2009 at 9:26 PM, Christoph Rupp<chris@crupp.de> wrote:
> I'd like to take you up on that offer; 1 license would be much appreciated.
> And once I get to the point of putting something online myself, a link
> to that is appreciated as well, but definitely not mandatory.

No problem :) I'm happy to get another commercial "customer" (you're
number 2 - my employer also uses hamsterdb)

> Okay, do I understand you correctly in that we then keep both
> cursor_find around as-is plus new cursor_find_ex() ?

yes, let's keep them.

> Anyway, no worries there: I cut the code down to using only one of
> them and the get_slot etc. code now invokes those key compare calls
> less often too, resulting in a minor speed up: less key comparisons
> for the same results.

That's good news :) Actually i copied the comparison stuff for
hamsterdb2 but then really failed to understand it. Since then it's on
my refactoring-list...

> Anyway, long story short: I'm currently prepping a minimal patched
> source tree for just this assert crap-up and test it on Linux; if you
> can have a look at that one and help me out by pointing where the
> bugger's gotta be fixed, much obliged.

Yes, please send it asap. I have my last vacation day tomorrow and at
least 4 hours time for coding :)

My main advantage on linux is valgrind, I don't know anything
comparable on Windows, and it has saved me LOTS of time when searching
for bugs. Maybe it can help us. I suggest that i start working on the
freelist issue and you can concentrate on the other stuff - that way
we're more effective  and i can take some of the nasty bugfixing
headache away from you :)

I'll just pack up my hamsterdb2 release - the win32 port is not yet
stable, i'll postpone it to the next release.

Bye
Christoph

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 19
	
	
Reply
	
	Follow up message
> No problem :) I'm happy to get another commercial "customer" (you're
> number 2 - my employer also uses hamsterdb)

:-) thanks!

> That's good news :) Actually i copied the comparison stuff for
> hamsterdb2 but then really failed to understand it. Since then it's on
> my refactoring-list...

I don't know how much v2 differs from v1 of course, but let's just say
the whole slot/etc. comparison stuff has seen significant change.
And, yes, it took a while to 'grok' the cmp value and make the correct
moves when it's not 0; I still need to recall to myself that the 'cmp'
is -1 when the value we're looking for is smaller than the key for the
inspected record; similar for cmp == +1.

Now the 'fun' starts when you wish to pass keys back to the user and
tell him/her if the /key/ is smaller or bigger than what they've been
looking for: that the mirror image right there.
And then there's that beauty of slot==-1, but I've tackled it. Big
comment chunk comes with it.


>> Anyway, long story short: I'm currently prepping a minimal patched
>> source tree for just this assert crap-up and test it on Linux; if you
>> can have a look at that one and help me out by pointing where the
>> bugger's gotta be fixed, much obliged.
>
> Yes, please send it asap. I have my last vacation day tomorrow and at
> least 4 hours time for coding :)

WORST news possible: I've a minimal patched version of the source tree
running on both Ubuntu 64 and XP64 and guess what: the assertion error
only happens on the Windows box. And I ran
./configure --enable-debug
and made sure the assertions fire by adding a bogus one which fires
all the time.

:-((((((((((((((((((((((((((((((((((((((((


Okay, so this is again one of those.. sigh... well I'll see what I can
do to reproduce this on a Linux box. My current suspicion is it is
triggered by Windows spitting up multiple mmap pages which can be
'fragmented' throughout address space, while Linux seems a little
nicer about it, feeding hamster progressing addresses for each mmap
chunk.

In short: *shit*.

You'll be hearing from me, when I get something going here.
For now, here's the tar.bz2 of said minimal edit;

Notes:

- BFC has been augmented to allow running only specific fixtures
and/or tests, by specifying each fixture and/or test on the
commandline like this:

fixture     -- test all tests in this fixture
fixture:test   -- run only this specific test
:test       -- run specific test in any fixture which has it

instead of single colon ':' you can use two or more, thus allowing
copy&paste straight off the screen output of BFC of the test titles
(names).



Attached: edited 1.0.9 'release': my current minimal test to reproduce
the freelist issue -- which does NOT happen on my linux boxes (SuSe /
Ubuntu) :-( -- but does occur on Windows (MSVC2008).

For now, the value of this is limited to a 'first look-see' for the
BFC stuff (the MSVC-specific __try/__except stuff is left out, as that
isn't stable yet).
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.tar.gz	hamsterdb-1.0.9.tar.gz
1516K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 19
	
	
Reply
	
	Follow up message
> Attached: edited 1.0.9 'release': my current minimal test to reproduce
> the freelist issue -- which does NOT happen on my linux boxes (SuSe /
> Ubuntu) :-( -- but does occur on Windows (MSVC2008).

On Windows BTW: assertion fails in freelist.c @ line 478 (the
'BANG!!!' line in there).
NO such failure on Linux.

Now testing with NO_MMAP to test a hypothesys...
- Show quoted text -


--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 19
	
	
Reply
	
	Follow up message
Hi Ger,

thanks for the code - i'll have a look tomorrow (going to bed now, the
day was long... our kid wakes up at 6 every day...)

just a note - the main reason for win32 vs linux bugs is that the page
size is different (64k vs 16k). Which means that more keys fit in the
page -> different splitting behaviour etc. To reproduce it on linux,
maybe it's enough to change the pagesize (ham_env_create_ex with
HAM_PARAM_PAGESIZE).

I'll have a closer look tomorrow!

Christoph

2009/7/19 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
>> No problem :) I'm happy to get another commercial "customer" (you're
>> number 2 - my employer also uses hamsterdb)
>
> :-) thanks!
>
>> That's good news :) Actually i copied the comparison stuff for
>> hamsterdb2 but then really failed to understand it. Since then it's on
>> my refactoring-list...
>
> I don't know how much v2 differs from v1 of course, but let's just say
> the whole slot/etc. comparison stuff has seen significant change.
> And, yes, it took a while to 'grok' the cmp value and make the correct
> moves when it's not 0; I still need to recall to myself that the 'cmp'
> is -1 when the value we're looking for is smaller than the key for the
> inspected record; similar for cmp == +1.
>
> Now the 'fun' starts when you wish to pass keys back to the user and
> tell him/her if the /key/ is smaller or bigger than what they've been
> looking for: that the mirror image right there.
> And then there's that beauty of slot==-1, but I've tackled it. Big
> comment chunk comes with it.
>
>
>>> Anyway, long story short: I'm currently prepping a minimal patched
>>> source tree for just this assert crap-up and test it on Linux; if you
>>> can have a look at that one and help me out by pointing where the
>>> bugger's gotta be fixed, much obliged.
>>
>> Yes, please send it asap. I have my last vacation day tomorrow and at
>> least 4 hours time for coding :)
>
> WORST news possible: I've a minimal patched version of the source tree
> running on both Ubuntu 64 and XP64 and guess what: the assertion error
> only happens on the Windows box. And I ran
> ./configure --enable-debug
> and made sure the assertions fire by adding a bogus one which fires
> all the time.
>
> :-((((((((((((((((((((((((((((((((((((((((
>
>
> Okay, so this is again one of those.. sigh... well I'll see what I can
> do to reproduce this on a Linux box. My current suspicion is it is
> triggered by Windows spitting up multiple mmap pages which can be
> 'fragmented' throughout address space, while Linux seems a little
> nicer about it, feeding hamster progressing addresses for each mmap
> chunk.
>
> In short: *shit*.
>
> You'll be hearing from me, when I get something going here.
> For now, here's the tar.bz2 of said minimal edit;
>
> Notes:
>
> - BFC has been augmented to allow running only specific fixtures
> and/or tests, by specifying each fixture and/or test on the
> commandline like this:
>
> fixture     -- test all tests in this fixture
> fixture:test   -- run only this specific test
> :test       -- run specific test in any fixture which has it
>
> instead of single colon ':' you can use two or more, thus allowing
> copy&paste straight off the screen output of BFC of the test titles
> (names).
>
>
>
> Attached: edited 1.0.9 'release': my current minimal test to reproduce
> the freelist issue -- which does NOT happen on my linux boxes (SuSe /
> Ubuntu) :-( -- but does occur on Windows (MSVC2008).
>
> For now, the value of this is limited to a 'first look-see' for the
> BFC stuff (the MSVC-specific __try/__except stuff is left out, as that
> isn't stable yet).
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 20
	
	
Reply
	
	Follow up message
Good night ... or this should read: Good Morning! ;-)


On Sun, Jul 19, 2009 at 11:03 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> thanks for the code - i'll have a look tomorrow (going to bed now, the
> day was long... our kid wakes up at 6 every day...)
>
> just a note - the main reason for win32 vs linux bugs is that the page
> size is different (64k vs 16k). Which means that more keys fit in the
> page -> different splitting behaviour etc. To reproduce it on linux,
> maybe it's enough to change the pagesize (ham_env_create_ex with
> HAM_PARAM_PAGESIZE).

Yup, gathered as much, but didn't expect this kind of fatalities;
hadn't run the code on my Linuxes, which was kinda dumb as then I'd
seen quicker that it's a platform- or should I say paging-specific
issue. Besides, it's more subtle than that; increase the cache size
(default means 4 pages on Windows) and the issue is GONE on Windows as
well: 100K records insert and no b0rk. Hm....

Still looking into it...


--
- Show quoted text -


































































\section hamsterdb Contact Form inquiry --> SVN checked --> RESOLVED (90%)
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 20
	
	
Reply
	
	Follow up message
Burning the candle from both ends is going to cost me dear today, but
screw it. I just can't live with a machine that's outsmarting me.
Especially when it's software that _should_ be running today.
Up to a few seconds ago, though, the machine has me stumped. But
finally I saw the light.

It's not in the code, though it is in the code. ;-)

Hm, should keep the suspense till the end, but don't feel like it.

Cause of many of the ill effects:

1% sizeof() operations in packed and non-packed structs that weren't
exactly right due to alignment settings in the MSVC projects. This has
been resolved by #2 below.
99% mmap over network: this should work without a hitch, I'm using it
in several other tools: windows mmapping into files on a Samba share
(my development filesystem is on a samba server; so any builds and
test runs use that filesystem by default, even when building on any of
the Windows boxes over here). HOWEVER, you're particular flavor of
copy-on-write mmapping and parallel FileWriting the 'dirty' contents
to platter is blowing me out of the water when traffic is heavy
enough: the asserts and failures kick up as the mmap operation fetches
an all-zero-bytes page, while a FileWrite from shortly before is still
traveling towards platter -- at least that's my diagnosis, after
digging deep and just now thinking: "heck. It can't be this, can it?"
and subsequently hacking the unittest (nearFindStressTest) to point
the DB file to local storage on a fast disc intended for video
editing. VOILA! No more errors in Win.
100K records with 4 pages @ 64 K each: no problem.

(Though I'll have to look into those freelists etc. some more, because
1M record insert is taking forever and this animal was selected to be
doing that in a while (target: over 40M record per table, so I might
need to improve the insertion process a wee bit... my current hunch,
without profiling, is the freelists are slowing this brother down.)


Whew.

Funny though that it's only the nearFindTest and nearFindStressTest
tests that hit this raw nerve.

On the list to look at by me: using READWRITE mmap paging (which has
proven to work in different tools of mine) and discarding the
FileWrite flushing 'around the corner': after all, pages can be
flushed to disc using FlushViews (or how was that Win32 API called
again; ah well, never mind. I know the one. Just not now any more.
Need sleep.)




To make it worth your while anyway, I need to switch gears now, so I'm
not going to prep the diffs with nice explanations and then feed them
to you piecemeal; what you'll get from me is the latest 'minimal'
(less minimal now) patch which I've been using to doublecheck on the
Ubuntu and SuSE boxes.
That's a tar.gz with a couple of tiny linguistic fixes incorporated,
the #2-below sizeof() mishmash replaced by OFFSETOF macros that work
with the current no-pragma-pack-no-more hacked code tree too, and not
much else. The find/compare stuff is DEFINITELY NOT in there, so you
still have the benefit of a two-stage gradual process: I suggest you
check this minimal-patch first, see what I've been messing with and
<facepalm> at appropriate spots ;-)

Then next, I'm going to feed you the whole shebang in one go, because
I'll be crashing (self-employed and no appointments today, except the
deadline for monday night software ready, which is now _met_, thanks
to this discovery. We'll have to run on local storage with this one
for now, that's the only drawback that needs fixing asap for the
project to be alpha-stage ready&done. Anyway, not your problem.
What I am saying is: since you still have a couple of hours, while
I'll be checking the insides of my eyelids, you might want to spend
that time on looking at the 'minimal' archive and then on the big fat
coming after that next, so that you can see what I've done to
pub_to_int and others and the whole LT/GT/LE/GE thing.

The nearFindStressTest is still under development, as it's still
lacking the bulk mixed multi-style find + interleaved delete I am
planning for that one: 100+K records being checked in such a way that
I can be 99.9% sure I'll be hitting all the fringe cases, e.g.
comparisons landing at an edge, but on the wrong or right page -->
testing the correctness of the move-to-adjacent page solutions in the
code for said fringes. This has been tested in my own app, but I've
proven beyond a doubt that that has been shoddy testing or this
wouldn't have happened this weekend. Ah well, never mind.

I'm terribly sorry the explanations will come in delayed by about a
day (I'll try to write some stuff before I crash now), but you'll have
the sources at least.



With this email there's a new .tar.gz attached with new 'minimal'
source tree. At least you'll have something to dig your teeth in now.



In chronological midnight order:

1) I was thinking compiler bug. Nyet. Turned out that HAM_DEBUG was
not #define'd in each and every header & sourcefile at compile time,
so some sources compiled without the macros. !@#$%^ The latest tree
(attached) will only compile when you do a

./configure --enable-debug --enable-internal

as without the --enable-debug you'll get #errors - deliberately.
This was not an issue on Linux but due to my [tweaked] MSVC project
files; since those depended on HAM_DEBUG coming in from src/config.h I
was screwed, as not everyone got that header; when you diff the tree,
you'll see that I made damn sure every source file starts with that
one, so no more HAM_DEBUG yes/no surprises.



2) Since the MSVC debugger had some very peculiar ideas about
displaying certain ham_offset_t values in some structs, I had to go
down to view disassembly and registers to discover the debugger
(compiler too?) got a bit confused with all the #pragma pack()s
pushing an popping in and out, so I blew those away 100% -- see the
#if 0 conditionals in packstart/packstop.h

To make sure the structures are properly calculated after all (with
the packing in place, this was not entirely correct in 64-bit Win
builds; by using the OFFSETOF macros rigorously everywhere, this has
been solved.

So the assertions started to move after that one, but not disappear.

(had notable trouble with: freelist struct: _page_id: debugger shows
wrong values, shifted by 4 bytes, compared to actual reality; moving
this field to the front resolved that for test)






**** EDIT: wrote the next bit BEFORE I saw the LIGHT *** read only
when you like to know about this sewer ****

3) I've checked out various things, but the new augmented
nearFindStressTest unittest code (unittest/hamsterdb.cpp) shows
hamster going down now on Win32/Win64 AND Lunix64 (Ubuntu's latest) so
there's something to check out there.

I couldn't get my mind around to patching up the makefiles to get me a
static build instead, because the debugging tools (xgdb) would not
accept the dyn lib compiled version as it is; some confusion over the
hamsterdb.so, but I'll VERY gladly leave that rat for you to solve --
I'm a bit fed up after this stretch, but the mood will pass once I've
taken a quick nap.





How/what to do?

Want to see things go bad?

Take the unittest/hamsterdb.cpp and check out the params at the top of
nearFindStressTest (and be reminded that BFC is hacked to run just
this one if you do 'make test'):

               const int RECORD_COUNT_PER_DB = 10000;
               ham_env_t *env;
               ham_db_t *db;
       ham_parameter_t ps[]={
                       {HAM_PARAM_PAGESIZE,   64*1024}, /* UNIX == WIN now */
               {HAM_PARAM_CACHESIZE,  /*32*16*/ 64*64*1024},
                       {0, 0}
               };

run as is and you SHOULD see a BFC error report about the DB being corrupt:




Adjust the CACHESIZE to 4*64*1024 to get Windows-similar behaviour
(not exactly the same; my Linux build segfaults very quickly, while
the Win binary takes a long time (about 6 dots) before hitting a
ham_assert() in the lib internals.




Enlarge RECORD_COUNT_... by another decade (100 000) and things will
always go pearshaped before the inserts are done, no matter how large
I set the CACHESIZE size.




What I've seen when debugging is that in Windows it went haywire in
freelist.c; mostly the fp-pointed-at structs are zeroed at the
inappropriate moment. It turns out while debugging that the

           else {
               page=__freel_alloc_page(db, cache, entry);

branch in freelist.c (snippet below; near line 470) only happens
seldom (= after about 14K records being inserted already) and when it
happens, you can be sure the next insert is going to b0rk by failing
the :
       if (!freel_entry_get_page_id(entry)) {
check entirely, landing in the third alt
           page=db_fetch_page(db, freel_entry_get_page_id(entry), 0);
which will produce a 'page' with way too many zero bytes, so looks
like an uninitialized page.

When run wih non-mmap-ed page sizes, this does not happen,
interestingly. Only the speed then is no fun; snails are faster.


       if (!freel_entry_get_page_id(entry)) {
           if (freel_entry_get_start_address(entry)==db_get_cooked_pagesize(db)) {
               fp=db_get_freelist(db);
                               ham_assert(freel_get_start_address(fp) != 0, (0));
           }
           else {
               page=__freel_alloc_page(db, cache, entry);
               if (!page)
                   return (db_get_error(db));
               fp=page_get_freelist(page);
                               ham_assert(freel_get_start_address(fp) != 0, (0));
           }
       }
       /*
        * otherwise just fetch the page from the cache or the disk
        */
       else {
           page=db_fetch_page(db, freel_entry_get_page_id(entry), 0);
           if (!page)
               return (db_get_error(db));
           fp=page_get_freelist(page);
                       // this next IF is bogus crapology and does not belong here: it's
just trace hacking during late night debug sessions:
                       // the ELSE is what matters, but ASSERT b0rks once we've gone
through the pattern of hitting
                       //   page=__freel_alloc_page(db, cache, entry);
                       // at line 475 above; the very next insert is going to crash into us here...
                       if (freel_get_start_address(fp) == 0)
                       {
                               ham_assert(freel_get_max_bits(fp) == 0, (0)); // completely wrong,
but this is what I get...
                               //freel_set_start_address(fp, db_get_cooked_pagesize(db));
                               //freel_set_max_bits(fp, size);
                       }
                       else
                               ham_assert(freel_get_start_address(fp) != 0, (0)); // [i_a] BANG!!!!!!!
       }



An alternative spot for hamster to b0rk is at line 101: the
ham_assert() in here:

static ham_size_t
__freel_set_bits(freelist_payload_t *fp, ham_bool_t overwrite,
       ham_size_t start_bit, ham_size_t size_bits, ham_bool_t set)
{
   ham_size_t i;
   ham_u8_t *p=freel_get_bitmap(fp);

   ham_assert(start_bit<freel_get_max_bits(fp), (""));


as freel_get_max_bits(fp) will cough up zero(0) and party's over.


**** EDIT: wrote the above bit BEFORE I saw the LIGHT *** read only
when you like to know about this sewer ****


--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9-semi-minimal-tree-on-monday-morning-WORKS-OK-ON-WIN-WITH-THIS.tar.gz	hamsterdb-1.0.9-semi-minimal-tree-on-monday-morning-WORKS-OK-ON-WIN-WITH-THIS.tar.gz
1414K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 20
	
	
Reply
	
	Follow up message
Hi Ger,

good news first - tonight (around 1 o'clock, when the baby woke up for
the 3rd time...) i got a great idea how to speed up your specific use
case (inserting incrementing keys, i.e. a timestamp or a RECORD_NUMBER
database):

i modify ham_cursor_insert. If a special flag (HAM_CURSOR_HINT_APPEND)
is set, the function will try to append the key to the database,
instead of inserting it.
if (cursor.page is the "rightmost" page in the b+tree
   AND this page is not yet empty
   AND the highest key in the page is < the new key):
       -> append the new key in this page
otherwise: just do a normal insert

This will completely remove walking through the B+Tree and never call
the compare functions or get_slot() when inserting the new key.

Also, another idea would be: currently, records <= 8 byte are stored
directly in the B+Tree, not in a blob. Maybe this could be made
configurable, so that records of other sizes also could be stored
directly in the tree. This would make insert and lookup faster.

OK, now to the other stuff:

1) i already merged the config.h changes (at least for those files
where this was the only change)

2) sizeof/OFFSETOF - i remember that MSVC sometimes displayed crap
when setting breakpoints. and maybe i found the solution: in
packstop.h there are include guards (#ifndef HAM_PACKSTOP_H) which
shouldn't be there. Basically packstop.h was always evaluated once and
never twice, and maybe that's the reason? Can you try if removing them
improves the situation on MSVC? I discovered this yesterday...

The problem with disabling packing is that the database files are no
longer compatible.

3) mmap over the network - actually my plan was (right from the
beginning) to offer a service/daemon for remote access. i know that
file locking and mmap on network devices has issues, also with NFS. So
i'm anyway surprised that it worked well for you. Would such a service
be interesting for you, or would you rather avoid it because of the
additional dependency?

The reason why i decided to use mmap for reading and WriteFile for
writing was basically to make it more foolproof. I wanted to make sure
that the page is only modified when it's explicitely written. And the
transaction routines make use of this - when the Transaction is
aborted, the modified pages are discarded (without being written to
disk, of course). If they would be written when they're unmapped, this
wouldn't work.

4) Bad news about the crash - it doesn't happen. I tried with 100000
keys and a cachesize of 4*64*1024 - it works fine, only the assert
("fire all the time") asserts. Tomorrow, at work, i'll have access to
a win64 bit machine. But i tried this test on linux 64 and 32 AND with
valgrind and it doesn't fail me :(

BTW - did the crash appear after you removed the packing or before?

I'll continue to merge your other changes and i hope you got some
hours of good sleep :)


Best regards
Christoph
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 20
	
	
Reply
	
	Follow up message
Hi Ger,

i merged all changes (all but packstart.h/packstop.h and the use of
OFFSETOF in keys.h, which is related to the packing).

i also merged the unittest in hamsterdb.cpp.

The unittests are running fine, and valgrind finds no issues (well, a
memory leak of 200 bytes, but i'll search for that later because
valgrind needs about 30 mins for one run, and with
memory-leak-detection it's even longer).

right now i just started my acceptance tests - they need about 4-6
hours. if they succeed, I'm pretty sure that the current state of
affairs is bug-free.

regarding packstart.h/packstop.h: i really do not want to sacrifice
backwards-compatibility in the file format. If my changes in
packstop.h (removing the include guards) don't have effect in MSVC, i
would suggest adding another compile time directive (i.e.
HAM_DONT_PACK) which disables packing. You can then use this for your
code, and everything else will continue to use the packed format. But
i really hope my changes have some effect...

I'll tell you when the tests are through, and then i'll search for the
memory leak.

Have a nice day,
Christoph

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 20
	
	
Reply
	
	Follow up message
Excellent thoughts at 0100 hours there ;-)))

Hm, I'll check out SVN tomorrow and see how I can merge the bundle;
need to filter my crap from my good work anyway. (So tread carefully
with the copying this into SVN (or not); I'll be reviewing the whole
thing later anyway as such a round is needed badly after a heated
session like this, any which way.)


On Mon, Jul 20, 2009 at 9:39 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> good news first - tonight (around 1 o'clock, when the baby woke up for
> the 3rd time...) i got a great idea how to speed up your specific use
> case (inserting incrementing keys, i.e. a timestamp or a RECORD_NUMBER
> database):
>
> i modify ham_cursor_insert. If a special flag (HAM_CURSOR_HINT_APPEND)
> is set, the function will try to append the key to the database,
> instead of inserting it.
> if (cursor.page is the "rightmost" page in the b+tree
>    AND this page is not yet empty
>    AND the highest key in the page is < the new key):
>        -> append the new key in this page
> otherwise: just do a normal insert
>
> This will completely remove walking through the B+Tree and never call
> the compare functions or get_slot() when inserting the new key.
>
> Also, another idea would be: currently, records <= 8 byte are stored
> directly in the B+Tree, not in a blob. Maybe this could be made
> configurable, so that records of other sizes also could be stored
> directly in the tree. This would make insert and lookup faster.

Well, let's see... my REAL records are about 4 to 20 floating point
values long, while the keys vary from 8 to 16 bytes (100nsec
timestamps plus a few extra IDs combined). All tables of my real app
are fixed sized key and fixed size record - I don't like variable
width that much anyway as I come from a custom solution which was
hash-table + single mmapped space based: ultrafast, but restricted on
the flexibility in such matters as record size variation.


>
> OK, now to the other stuff:
>
> 1) i already merged the config.h changes (at least for those files
> where this was the only change)

Happened to all in src/ and unittests/ , by the way.


> 2) sizeof/OFFSETOF - i remember that MSVC sometimes displayed crap
> when setting breakpoints. and maybe i found the solution: in
> packstop.h there are include guards (#ifndef HAM_PACKSTOP_H) which
> shouldn't be there. Basically packstop.h was always evaluated once and
> never twice, and maybe that's the reason? Can you try if removing them
> improves the situation on MSVC? I discovered this yesterday...

Yeah, saw the (to me) 'weird' double push/pop under particular
conditions in there; as you can see in my code, I just blew it all
away. I've shelved getting this back for later this week; I know it's
important to you, but see also my comment in my previous email: pack
push/pop will/should/must return, but I cut it out as there was too
much wierdness going on all at the same time in my debugger.
With the latest runs, there's no more wickedness there, so tomorrow or
the day after might be a nice time to see about re-introducing that
bit over here.
I'll keep you posted when I do this.


>
> The problem with disabling packing is that the database files are no
> longer compatible.

Yup, I know. See above and previous comment: bang, shotgun.


> 3) mmap over the network - actually my plan was (right from the
> beginning) to offer a service/daemon for remote access. i know that
> file locking and mmap on network devices has issues, also with NFS. So
> i'm anyway surprised that it worked well for you. Would such a service
> be interesting for you, or would you rather avoid it because of the
> additional dependency?

Go lightly on the service stuff, please. hamster is interesting to me
because it's giving me the chance to get close to raw local I/O
performance and I can assure you socketizing the whole shebang into
modern 'services' and all isn't going to be a winner with respect to
raw speed.
I am a nut in using it with network file I/O anyway - that is *me*
doing things wrong as that little action is the result of other
corners that have been cut over here.

Maybe others like it when hamster becomes a similar-to-Oracle
client&server networked DB solution, but I think hamster is the wrong
layer to do this; I should be kicked in the butt and told to do local
file I/O only; the socket/server/service-etc. stuff should be a couple
of layers higher in the architecture -- at least that's my taste in
these things:
For 10G LAN it may all sound very nice, but I still have Stone Age
100M around here and it shows in the numbers. Imagine what happens
when some fruit basket takes such a DB service and puts its on the Net
(WAN) 'because it's got encryption anyway so we're safe' (I've seen
'em do it). Ergo: hamster as a service may be the dream of others, but
not mine.


>
> The reason why i decided to use mmap for reading and WriteFile for
> writing was basically to make it more foolproof. I wanted to make sure
> that the page is only modified when it's explicitely written. And the
> transaction routines make use of this - when the Transaction is
> aborted, the modified pages are discarded (without being written to
> disk, of course). If they would be written when they're unmapped, this
> wouldn't work.

I gathered as much, yes. Good solution for local I/O and that's what I
should stick with anyway.

>
> 4) Bad news about the crash - it doesn't happen. I tried with 100000
> keys and a cachesize of 4*64*1024 - it works fine, only the assert
> ("fire all the time") asserts. Tomorrow, at work, i'll have access to
> a win64 bit machine. But i tried this test on linux 64 and 32 AND with
> valgrind and it doesn't fail me :(
>
> BTW - did the crash appear after you removed the packing or before?

Crash was before, but there's been happening so much code-wise in here
that the crash (or more correctly: the asserts in freelist.c being
triggered) do not happen any more.
Though I still reproduce those fine when I run that nearFindStressTest
with the record count dialed up to 20K or above (100K does it) as then
there sufficient SMB network file I/O (cut down the cachesize to 4*64K
and pagesize=64K and hamster goes bonkers with the asserts)

So, in short, it's largely a network file I/O issue.

There was a bit about the alignment of a few fields in the code
before, but that's fuzzy ATM, sorry. Anyway, /that/ does not occur any
longer; not in any way I've seen it happen throughout the weekend, so
consider that part closed. Better to focus on what we've got today
than to chase a wild goose of mine.


>
> I'll continue to merge your other changes and i hope you got some
> hours of good sleep :)

Still working towards them. As I know I'll be down for the duration, I
stuck around and checked my toolchain that needs to run tonight - that
one is working now, so I won't be getting complaints from that. Phew.


>
>
> Best regards
> Christoph
>


Cheers,

Ger



Sorry to blast you off your feet like this; it's gone a bit faster and
less organized than I originally intended, loading you with a lot of
stuff all in one go.
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 20
	
	
Reply
	
	Follow up message
On Mon, Jul 20, 2009 at 11:10 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> i merged all changes (all but packstart.h/packstop.h and the use of
> OFFSETOF in keys.h, which is related to the packing).
>
> i also merged the unittest in hamsterdb.cpp.
>
> The unittests are running fine, and valgrind finds no issues (well, a
> memory leak of 200 bytes, but i'll search for that later because
> valgrind needs about 30 mins for one run, and with
> memory-leak-detection it's even longer).

that new CreateCloseOpenClose... test is going to crash on linux if my
systems are any judge.
Next to that, be aware that my 'DB in memory' hacking in unittests is
based on guesswork, so take with grain of salt.


> right now i just started my acceptance tests - they need about 4-6
> hours. if they succeed, I'm pretty sure that the current state of
> affairs is bug-free.
>
> regarding packstart.h/packstop.h: i really do not want to sacrifice
> backwards-compatibility in the file format. If my changes in
> packstop.h (removing the include guards) don't have effect in MSVC, i
> would suggest adding another compile time directive (i.e.
> HAM_DONT_PACK) which disables packing. You can then use this for your
> code, and everything else will continue to use the packed format. But
> i really hope my changes have some effect...

Like I said: never mind the don't pack - that was a 1 second make it
work now hack, to be rolled back once the issues are gone. So they can
be rolled back now, is my guess.
I'll check your SVN when I am living and awake again.


>
> I'll tell you when the tests are through, and then i'll search for the
> memory leak.

Great!


Take care,

till tomorrow and I'll stay off the mail a bit more then. ;-)
- Show quoted text -
































































\section [mem dump] thoughts on speed improvements for the freelists
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
(As promised; I'll take up your time for reading this, so you ain't
got time to fix stuff ;-) )

For filing only. A reminder to self, when the time is ripe for this.

This is a memory dump so I get my head cleared of this as I need to do
other stuff now.

----

Since Christoph had this lovely idea for linear record insertion (when
keys are incrementally inserted into the database), which jumps to the
last freelist page for inserting the blob data, I've been pondering a
few things.

1) This idea holds only when there are no deletions, as otherwise you
'forget' the gaps introduced in the freelist by record deletion. This
may be acceptable, in an ever-growing database it's fine. In actual
practice, one might need to 'vacuum' the tables now and then during
maintenance time - this will be costly record reshuffling operations.
'vacuum' is a term borrowed from PostgreSQL as the purpose is rather
similar here: crunching the gaps.


2) given the above specialist behaviour and what comes next (see #3),
I think a 'heuristics' bit/flag field ~~ DB param may be nice to add
to the create_ex() param lists, in a fashion similar to Win32 API and
other platforms (VMS) where you can inform the system what your intent
is, so subsystems can be configured to act optimally when doing that
(Win32: sequential versus random-access file I/O, ...)
I can see flags like
 HAM_HEUR_ONLY_ADD_RECORDS
 // announce the intent to never delete records
and
 HAM_HEUR_OPTIMIZE_FOR_INSERT
 // announce that insert should be very fast and search/insert ratio
will probably be close to 1.0 (a very bad use case for SQL servers,
BTW)
in there, which can trigger different behaviour in hamster (maybe by
switching methods ( ~ function pointers) in some sections, e.g.
freelist management through a few function pointers, so we can switch
algos based on these heuristics


2) Since I saw already earlier you optimized the freelist search by
scanning qwords, then going down to bits, another thought hit me: what
if you regard those individual bits as /elements/, i.e. characters in
an alphabet in the mathematical sense; that alphabet would be size=2,
but this can lead to surprising things on it own when applying
algorithms developed for character-based scanning. I.e. instead of a
linear search, we might consider using Boyer-Moore; given that we look
for an 'empty slot' of size P buckets, we can restate this as looking
for a pattern 11111111....1 of length P in a large 'string' - which is
the freelist bitarray itself.

The fact that you check whole bytes at first is already a speedup, but
considering that store slots are often longer than 8, Boyer-Moore can
help both on the qword and bit-level, so a two-level Boyer-Moore
approach might be an improvement. Read as: apply Boyer-Moore to bits
== string searches in a realm where the alphabet is {0,1} for both
pattern and search string.

A further optimization then is to 'upchunk' this (or a similar
sub-linear algorithm) to employ byte/qword-wide scanning for larger
patterns: for instance, searches for 15-bit or larger blobs need at
least one 0xFF byte, so we can convert larger sized pattern searches
to a mix of bit and byte searching/skipping.

After looking at Sunday's and other work (Moore has a very interesting
website of his own, and then there's a few papers available on-line
about fast string searching; most of them do not help when you know
your search pattern is always all '1's, so Nebel, for instance, is not
useful, but Sunday's optimization is. Unfortunately, his papers are
behind the ACM pay server, but here's something to grok if we go this
route (STBM & Sunday attachments)


3) Yesterday I didn't think of this, but 'skip lists' were at the
front of my mind, or a derivation thereof: the freelist page header
tracking:

a) in simplest form: where the first free bit (1) can be found within
this freelist page bitarray, so we don't have to start at offset 0.
This can be used to skip large sections of a freelist fast, ending up
an approximation of your 'go to last freelist page and append' idea,
but without the drawback of getting ever-unused gaps after delete. The
drawback compared to 'go to last immediately' is the O(n) cost, though
n now is pages instead of freelist bytes (or bits), so it's a speed
improvement by a large magnitude any which way.

b) skip list-like per page: do not remember one offset to a free slot,
but have a set of those, one entry for each power-of-2 size (up to a
certain limit, e.g. powers 1-16 only.

The way to use these would be to jump to those offsets while searching
for free slots, when a free slot is found, adjust the offset table
accordingly (adjust offset of corresponding power-of-2 slot entry and
max() with all entries for larger sizes. 'adjust accordingly' here is
NOT searching for more free space, but just adjusting the offset to
point beyond the current 'space-to-occupied'; the next search for free
space will start there and adjust the offset once its found another
group of 1/free bits further up in the bitarray.

Delete can adjust the offsets to decrease where appropriate; one
optimization for speed would be NOT to scan for adjacent free space,
i.e. no 'merging' as one can assume similar sized keys & records will
be inserted in the future, occupying such space again.

Thus no extra cost in insert/delete apart from updating the offset
table of size 1..16 in the freelist page header; benefit is averaging
on halving search time within the page, as the offset(s) range from 0
to maxlen-of-page.



4) Taking the Boyer-Moore and the 'heuristics' idea a bit further
using an adaptive/inverted binary search scheme, we can search for
slots in the free list as follows (I thought of after reading a
paragraph about another tiny-alphabet problem area: DNA searching
(alphabet size=3..5) where BM is used)

The search algorithm would start by testing bit P (1-based indexing
here as that's easier to write down now) where we're looking for a
slot of length P, i.e. a 'string' of length P: Boyer-Moore checks the
last 'character' so that's 'character' P == bit P.
When no hit, do not jump according to Boyer-Moore, but assume a
pattern of doubled size, i.e. jump +2P bits and test again. double the
jump size like that until we hit a free bit. (Inverted binary search:
x2 instead of /2)
Then, when we've got a hit, apply a P-interval-based binary search on
the zone between the previous check offset and the last one (meaning:
check only bits at positions i MOD P = 0: after all, for a suitably
large free area to exist, we'll hit a 'free' bit at those positions
anyway. When a free bit is found during this search, scan left & right
to see if the area is large enough. If not, continue with the
inverted-binary search jump pattern.
Scan can be done by binary search

The initial p, 2P, 4P, ... jump increments are meant to statistically
lean the distribution to the left, i.e. to assure we're minimizing
fragmentation by prefering allocation at the left side.

Of course, this goes very well with the skip-listy tracking of the
'first free bit offset' in the freelist page header as suggested in
#3: start the jump increment pattern from there and you're chances of
neatly appending when possible increase manyfold, while a fragmented
freelist following deletes is still reused as free slots within
allocated areas are still found.

(One caveat: the rule to binary search between probe t-1 and t (at
offsets iP and (i+2i)P) will introduce lingering holes as we won't
find the very first free slot sitting within the previous skipped zone
when i>2, so when this jumping scheme hits the end of the page [and
assuming we know the page has free bits available] the system should
'wind back' to test those previously jumped zones. Since we know how
we jumped, i.e. which spaces we've tested already, we can solve this
algorithmically (no need to keep lists of spots we inspected during a
run like this --> no obvious costs in terms of RAM access due to large
tables using in the algo).






--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
2 attachments  Download all attachments  
stbm.c.html	stbm.c.html
32K   Open as a Google document   View   Download  
String Searching over Small Alphabets - sustik-moore.pdf	String Searching over Small Alphabets - sustik-moore.pdf
64K   View   Download  
Reply
	
Reply to all
	
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 22
	
	
Reply
	
	Follow up message
Hi Ger,

thanks for your mails - just a very quick reply to distribute another
idea before it gets lost:

- store the freelist entries not sorted by position, but sorted by
size: allocate a few pages, split them into logical chunks (i.e.
initially of 2 kb) and reserve one chunk for blobs > 1024 byte, one
for blobs > 786 bytes, one for blobs > 512 bytes etc. If a chunk is
too full, allocate an overflow chunk.

to store data in a chunk we could even use a compression algorithm
(i.e. RLE) in order to reduce I/O time.

later an optimization algorithm could even decide how the blobs/sizes
would be chosen optimally.

this could be a really simple algorithm because it doesn't require
sorting or searching. And it could really be fast, at the cost of a
few allocated pages.

Thanks for your other mail - i'll reply later, they're already coming
to drag me to another meeting...

Regards
Christoph

2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> (As promised; I'll take up your time for reading this, so you ain't
> got time to fix stuff ;-) )
>
> For filing only. A reminder to self, when the time is ripe for this.
>
> This is a memory dump so I get my head cleared of this as I need to do
> other stuff now.
>
> ----
>
> Since Christoph had this lovely idea for linear record insertion (when
> keys are incrementally inserted into the database), which jumps to the
> last freelist page for inserting the blob data, I've been pondering a
> few things.
>
> 1) This idea holds only when there are no deletions, as otherwise you
> 'forget' the gaps introduced in the freelist by record deletion. This
> may be acceptable, in an ever-growing database it's fine. In actual
> practice, one might need to 'vacuum' the tables now and then during
> maintenance time - this will be costly record reshuffling operations.
> 'vacuum' is a term borrowed from PostgreSQL as the purpose is rather
> similar here: crunching the gaps.
>
>
> 2) given the above specialist behaviour and what comes next (see #3),
> I think a 'heuristics' bit/flag field ~~ DB param may be nice to add
> to the create_ex() param lists, in a fashion similar to Win32 API and
> other platforms (VMS) where you can inform the system what your intent
> is, so subsystems can be configured to act optimally when doing that
> (Win32: sequential versus random-access file I/O, ...)
> I can see flags like
>  HAM_HEUR_ONLY_ADD_RECORDS
>  // announce the intent to never delete records
> and
>  HAM_HEUR_OPTIMIZE_FOR_INSERT
>  // announce that insert should be very fast and search/insert ratio
> will probably be close to 1.0 (a very bad use case for SQL servers,
> BTW)
> in there, which can trigger different behaviour in hamster (maybe by
> switching methods ( ~ function pointers) in some sections, e.g.
> freelist management through a few function pointers, so we can switch
> algos based on these heuristics
>
>
> 2) Since I saw already earlier you optimized the freelist search by
> scanning qwords, then going down to bits, another thought hit me: what
> if you regard those individual bits as /elements/, i.e. characters in
> an alphabet in the mathematical sense; that alphabet would be size=2,
> but this can lead to surprising things on it own when applying
> algorithms developed for character-based scanning. I.e. instead of a
> linear search, we might consider using Boyer-Moore; given that we look
> for an 'empty slot' of size P buckets, we can restate this as looking
> for a pattern 11111111....1 of length P in a large 'string' - which is
> the freelist bitarray itself.
>
> The fact that you check whole bytes at first is already a speedup, but
> considering that store slots are often longer than 8, Boyer-Moore can
> help both on the qword and bit-level, so a two-level Boyer-Moore
> approach might be an improvement. Read as: apply Boyer-Moore to bits
> == string searches in a realm where the alphabet is {0,1} for both
> pattern and search string.
>
> A further optimization then is to 'upchunk' this (or a similar
> sub-linear algorithm) to employ byte/qword-wide scanning for larger
> patterns: for instance, searches for 15-bit or larger blobs need at
> least one 0xFF byte, so we can convert larger sized pattern searches
> to a mix of bit and byte searching/skipping.
>
> After looking at Sunday's and other work (Moore has a very interesting
> website of his own, and then there's a few papers available on-line
> about fast string searching; most of them do not help when you know
> your search pattern is always all '1's, so Nebel, for instance, is not
> useful, but Sunday's optimization is. Unfortunately, his papers are
> behind the ACM pay server, but here's something to grok if we go this
> route (STBM & Sunday attachments)
>
>
> 3) Yesterday I didn't think of this, but 'skip lists' were at the
> front of my mind, or a derivation thereof: the freelist page header
> tracking:
>
> a) in simplest form: where the first free bit (1) can be found within
> this freelist page bitarray, so we don't have to start at offset 0.
> This can be used to skip large sections of a freelist fast, ending up
> an approximation of your 'go to last freelist page and append' idea,
> but without the drawback of getting ever-unused gaps after delete. The
> drawback compared to 'go to last immediately' is the O(n) cost, though
> n now is pages instead of freelist bytes (or bits), so it's a speed
> improvement by a large magnitude any which way.
>
> b) skip list-like per page: do not remember one offset to a free slot,
> but have a set of those, one entry for each power-of-2 size (up to a
> certain limit, e.g. powers 1-16 only.
>
> The way to use these would be to jump to those offsets while searching
> for free slots, when a free slot is found, adjust the offset table
> accordingly (adjust offset of corresponding power-of-2 slot entry and
> max() with all entries for larger sizes. 'adjust accordingly' here is
> NOT searching for more free space, but just adjusting the offset to
> point beyond the current 'space-to-occupied'; the next search for free
> space will start there and adjust the offset once its found another
> group of 1/free bits further up in the bitarray.
>
> Delete can adjust the offsets to decrease where appropriate; one
> optimization for speed would be NOT to scan for adjacent free space,
> i.e. no 'merging' as one can assume similar sized keys & records will
> be inserted in the future, occupying such space again.
>
> Thus no extra cost in insert/delete apart from updating the offset
> table of size 1..16 in the freelist page header; benefit is averaging
> on halving search time within the page, as the offset(s) range from 0
> to maxlen-of-page.
>
>
>
> 4) Taking the Boyer-Moore and the 'heuristics' idea a bit further
> using an adaptive/inverted binary search scheme, we can search for
> slots in the free list as follows (I thought of after reading a
> paragraph about another tiny-alphabet problem area: DNA searching
> (alphabet size=3..5) where BM is used)
>
> The search algorithm would start by testing bit P (1-based indexing
> here as that's easier to write down now) where we're looking for a
> slot of length P, i.e. a 'string' of length P: Boyer-Moore checks the
> last 'character' so that's 'character' P == bit P.
> When no hit, do not jump according to Boyer-Moore, but assume a
> pattern of doubled size, i.e. jump +2P bits and test again. double the
> jump size like that until we hit a free bit. (Inverted binary search:
> x2 instead of /2)
> Then, when we've got a hit, apply a P-interval-based binary search on
> the zone between the previous check offset and the last one (meaning:
> check only bits at positions i MOD P = 0: after all, for a suitably
> large free area to exist, we'll hit a 'free' bit at those positions
> anyway. When a free bit is found during this search, scan left & right
> to see if the area is large enough. If not, continue with the
> inverted-binary search jump pattern.
> Scan can be done by binary search
>
> The initial p, 2P, 4P, ... jump increments are meant to statistically
> lean the distribution to the left, i.e. to assure we're minimizing
> fragmentation by prefering allocation at the left side.
>
> Of course, this goes very well with the skip-listy tracking of the
> 'first free bit offset' in the freelist page header as suggested in
> #3: start the jump increment pattern from there and you're chances of
> neatly appending when possible increase manyfold, while a fragmented
> freelist following deletes is still reused as free slots within
> allocated areas are still found.
>
> (One caveat: the rule to binary search between probe t-1 and t (at
> offsets iP and (i+2i)P) will introduce lingering holes as we won't
> find the very first free slot sitting within the previous skipped zone
> when i>2, so when this jumping scheme hits the end of the page [and
> assuming we know the page has free bits available] the system should
> 'wind back' to test those previously jumped zones. Since we know how
> we jumped, i.e. which spaces we've tested already, we can solve this
> algorithmically (no need to keep lists of spots we inspected during a
> run like this --> no obvious costs in terms of RAM access due to large
> tables using in the algo).
>
>
>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>



































































\section the full monty: alles in einem (HamsterDB)
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 20
	
	
Reply
	
	Follow up message
Here's the whole bundle.

Couldn't leave it alone, so the unittest has been further extended
(nearFindStressTest) and that turned up a couple of lingering bugs in
the latest LT/GT matching internal logic.


FYI:

there's a couple of coding patterns that are my habbit:

1)

#if 0
...
#endif

to comment out chunks of code. You'll find those all over the place.
Where I've been busy, you'll run into those. Some of them are even
nested, due to pattern #2:




2)

when I find a bug or weirdness, I tend to grep the code and check out
all the similar spots; this also happens when I find fixes, which are
often 'distributed' that way too; a couple of #if 0...#endif sections
show where I first copy&pasted, only then starting to think about the
local differences and in a few spots that led to the conclusion a
certain chunk wasn't needed after all.




3)

All transaction begin/end logic now sits at the inner level; see
hamsterdb.c et al to see where those calls ahve gone and have moved
inside. This is an 'old' edit of mine, in fact this is from before the
LT/GT/LE/GE function introduction.

Same applies to the thorough use of the objectification, i.e. using
the DB method function pointers in the DB API interface layer where
such didn't happen already.

Incidentally, I forgot already, but I had added that calc_max_keys as
an extra method in there.



4)

As you mention the packing: those #if 0's in packstart/stop.h are ONLY
there for me to have something that works /yesterday/ in the MSVC
debugger; I'm fully aware this blows DB binary portability out of the
water; it's not intended as permanent changes - I got so crazy over
the asserts and weird issues, anything that was in my way got blasted
Hiroshima style. I hope I put a few comments in there saying as much,
or it gets hard to find what is 'good stuff' and what is 'smash this,
gotta pass through here now pronto' in some spots.




5)

The MAJOR thing is the changed get_slot and all related find code;
anything that's twiddling with KEY_IS_LT/GT is very important.
The pub_to_int and int_to_pub have been 'unified' into one being used
only; since this would flip the sign on the cmp output, a few
conditionals have changed in insert and delete: be aware there!



6)

The hamsterDB.h has been 'documented' a wee bit better, but as you'll
qucikly see I only did one function (the <ul><li>....</ul. stuff for
EQ/GE/GT/etc. in there) and I still need to adjust that text and
spread it across to ham_find and cursor_find_ex().




7)

BFC: had a few weird happenings in the debug sessions where the error
object carried corrupted strings, which was solved by copying them
intern by changing the const char * member vars to string types; not
nice, not well thought through, but a dirty job done cheap. ;-)



8)

BFC: now has extra ****_I(cond, scenario_integer) macros to help me
discover which round in a loop, i.e. which 'scenario' run caused the
b0rk. This is combined with a fast hack of error() constructor to
accept printf-style varargs; you can see todays work by diffing with
the tar.gz from tomorrow.



9)

the nearFindStressTest has been tested and works on XP64.

A quick run on Ubuntu (64-bit) seems okay, but there's another new
one, which does NOT crash on Win64, but DOES on my Ubuntu box and
that's the createCloseopenClose...whatever test method which, IIRC,
does env_create_ex() with a large number of DBs -- this is what I
needed and previously, hamster didn't allow that.
See also the various augments in the hamster API layer code
(hamsterdb.c) regarding param checking and ditto use.


10)

Makefiles et al are changed; I run the close-to-bleeding edge automake
1.10.3 cum cuis overhere, and bootstrap b0rked _hard_ on the libtool
stuff and the rest without those changes. Unfortunately, those work
for later automake/autoconf/libtool/aclocal but not for older; it's an
either/or situation there. Do whatever you like.



11)

Since I have multiple Win platforms/build systems, it's win32/msvc2005
and win32/msvc2008 where all the vcproj's etc. ended up.
The MSVC2008 ones are okay; the MSVC2005 ones are older ones which
have been kept somewhat in sync by hack&slash in the XML; those will
probably NOT work out of the box with MSVC2005.
These projects exhibit my build system indirectly: the directory setup
and the copying of final binaries to the solution/bin/platform
directory.



12)

So far, the issues have been solved, regarding the asserts firing all
over in freelist, etc.: the way to 'reproduce' that issue is by
placing the sources on a network share, then build and run from a
remote station, so that the mmapping + FileWrite activity travels the
network. When you really go for it, you /may/ get a few 'access
denied' errors even from the MappingView Win32 api's inside hamster -
when the planets are aligned.

(I've found that mmapping to samba is fine, as long as you don't do
anything 'besides' it, so no Write/Read to the same files/handles. I
read your email from today and saw you mentioning issues regarding
network file I/O: yes, there's a lot of that, especially on NFS, but
SMB is a bugger too. Unfortunately, my tool should work with file I/O,
but I'm thinking about patching up the os_win32 port code in there so
that write-through gets me what I need: READWRITE through the mapping
and no FileWrite no more. Yes, I'm aware that blows up my transaction
rollback / integrity, but I'm stubborn: got to try that one anyhow --
I'll probably fold in a couple of days, when I keep having trouble. I
know for a fact that mmap performance over network is subpar when done
from a Win32/64 box anyway.



So far this email; will reply to your emails that I saw waiting for
me, then it's finally shut-eye for good today. Stuff's installed,
runs, so I got my deadline done.


Sorry, but I don't think I'll make any more noise today.... ;-)







--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.20090720-001-full-monty.7z	hamsterdb-1.0.9.20090720-001-full-monty.7z
1060K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 20
	
	
Reply
	
	Follow up message
Hi Ger!

I merged all your changes, some with modifications.

In general i try to avoid tabs, but i don't mind if some of them "slip
through". But i use vi as text editor in a console window (yes,
really! :) ) and therefore i put line-breaks after 80 characters.

in hamsterdb.h i made some changes - i.e. renamed ham_get_env_params
to ham_env_get_parameters, for consistency reasons (same for
ham_get_db_params). also, i moved the HAM_FIND_* flags closer to the
ham_cursor_find function. so basically my modifications were
cosmetically.

in hamsterdb.c i found something critical, i think. ham_open_ex sets
the env_set_max_databases value. But this value can only be set in
ham_create. in hamsterdb, the freelist starts in the header page after
the database descriptors. if the max_databases value is incremented,
this array must be resized and the freelist start must be moved, but
that would be too complicated. Therefore i didn't merge this change.

The unittest segfaults at the new test, because a pointer value was
bogus. then i had to change some parameters which were not allowed in
create_db/open_db. i fixed these things and now it works fine. I saw
that valgrind found some memory leaks, but i'll get them tomorrow or
so (they're so easy to find with valgrind).

The CppApi tests now fail with a strange message, i'll have a look
tomorrow evening. I'll call it a day now. Today's my last day of
vacation :( I'll have a closer look tomorrow, and i think i'll also
find the memory leak till then. And then i want to add more tests for
the approximate matching. If i have enough time then i really hope i
can make a release till next weekend, but i can't promise anything.

I'm really glad that the weird freelist bug disappeared. i'll add a
new entry to the FAQ describing this issue.

Looking at the code i think i understood what you're doing (thanks to
your clean coding style and the extensive comments), but i don't yet
say that i grok it (for that i will need more time that i'll invest
happily).

The tests which i started this morning are still running. Everything's fine.

Good night! :)
Christoph
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 21
	
	
Reply
	
	Follow up message
Just woke up after the crash; will return to horizontal inactivity in
half a hour.

The max_db setting in open was because when I started using hamster I
ran into the issue of not being able to use more than 16? 32?
databases in a single ENV; IIRC creation was fine but opening them in
a second run was a no-go. May have been me screwing up, though.

Regarding vi editor and console windows: I always set my console
windows to 132 wide and small font - heck, my eyes were already shot
when I was 5, so reading fine print is perfectly fine for me.
You may consider using uncrustify (see sourceforge) to reformat if you
want; I use that tool myself, but didn't yet on the hamster edits I
made - was a bit of time pressure cooker, really.
I'm glad you could follow what I've done through the comments; they're
there because I tend to forget what I was smoking/thinking when I
revisit my own stuff after 6 months or more, let alone work from
others' that I've been expanding.

Thanks for finding the pointer issue; I'll get a fresh SVN dump now
and see what it shows me later today.



Couple of things I'd like to hear your opinion about:

a)
I'm still two minds about the way I've been exploding your unittest
code; this way was the fastest for me and given the comment section
there you can see where I want to take it, but it's not as clean and
simple as the other checks in there, so a bit of a wart. While cutting
it up into chunks means the interdependencies need to be managed,
given the new state of the art regarding BFC, as that one can now run
a single test method. Architectural thoughts are welcomed.

b)
The other bit is about those get_parameter functions; I like my
diagnostics extremely verbose, and that's where those help out, but
the maxkey calc is sitting there too, not really a param, but ... and
then I find that the maxkey calc is wrong anyway as large keys are
split into small parts plus blobs, so my original intent for testing
by creating pages which would house only 5 keys each is not how it
works - I saw that while debugging the latest LT/GT logic - while the
other purpose: calculating pagesize to fit the maximum of 65534 (even
number) leys in a single page as I will be creating huge tables isn't
straightforward either.
Anyway, the thought of the moment /was/ to dump it into get_parameters
by way oif some extra HAM_PARAM_ constants == param records, but I'm
not sure about that either; doesn't feel 'right' to me still.

c)
If you've got a better idea how the API can assist a user who's
looking for pagesizes derived from keysize and
number-of-keys-per-page, I'm all for it.

d)
Regarding flow for find: I intend to write that one up a bit better as
it's very subtle, especially when you're mixing LT/GT/EQ in there and
consider the scenarios where you end up on a page edge or not. I was
thinking about doing this in a DoxyGen compatible way, but it's been
too long since I last used Doxy (I know it supports dot/neato
diagrams, which are neat) and I recall it was a bitch to get Doxy to
produce output which lists things like additional 'technical design &
elaboration on internal implementation' main chapters as such things
are not really tightly related to a function/class or group of those.
Any pointers there? I'll check out doxy's documentation anyway, but
maybe you've got a few ideas there.

I noticed that not all functions came up in the main sections of the
DoxyGen generated documentation; they are there, but I only found them
after opening the html for the header files, which is not how I like
things to work for documentation. I know/recall this was always
troublesome -- while DoxyGen was one of the easier-to-manipulate tools
out there; don't get me started on JavaDoc :-(


Enough for tonight. I'll grab a midnight snack and enjoy sleep again.

Ciao,

Ger


PS: you're fast with the merge and fixing stuff. Nice to see that. :-)

PPS: regarding performance and freelists and all: I've had this
thought about adding a sort of skiplist-ish thing in there for faster
bitslot searching, but that's just a vague idea at the back of the
head right now. Before I go on a wild goose chase there I should
recover my old MSVC2003 rig with DevPartner (formerly NuMega) setup --
if the license keys and the software still wish to install on XP32 --
so that I can run by-line profiling again. It's sorely lacking in
2005/2008 and all I see now is the CPU maxing out when I run the
stresstest using local file I/O.
But that's probably not happening this month - or the next. There's
only 24 hours in a day anyway.
- Show quoted text -



On Mon, Jul 20, 2009 at 10:48 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger!
>
> I merged all your changes, some with modifications.
>
> In general i try to avoid tabs, but i don't mind if some of them "slip
> through". But i use vi as text editor in a console window (yes,
> really! :) ) and therefore i put line-breaks after 80 characters.
>
> in hamsterdb.h i made some changes - i.e. renamed ham_get_env_params
> to ham_env_get_parameters, for consistency reasons (same for
> ham_get_db_params). also, i moved the HAM_FIND_* flags closer to the
> ham_cursor_find function. so basically my modifications were
> cosmetically.
>
> in hamsterdb.c i found something critical, i think. ham_open_ex sets
> the env_set_max_databases value. But this value can only be set in
> ham_create. in hamsterdb, the freelist starts in the header page after
> the database descriptors. if the max_databases value is incremented,
> this array must be resized and the freelist start must be moved, but
> that would be too complicated. Therefore i didn't merge this change.
>
> The unittest segfaults at the new test, because a pointer value was
> bogus. then i had to change some parameters which were not allowed in
> create_db/open_db. i fixed these things and now it works fine. I saw
> that valgrind found some memory leaks, but i'll get them tomorrow or
> so (they're so easy to find with valgrind).
>
> The CppApi tests now fail with a strange message, i'll have a look
> tomorrow evening. I'll call it a day now. Today's my last day of
> vacation :( I'll have a closer look tomorrow, and i think i'll also
> find the memory leak till then. And then i want to add more tests for
> the approximate matching. If i have enough time then i really hope i
> can make a release till next weekend, but i can't promise anything.
>
> I'm really glad that the weird freelist bug disappeared. i'll add a
> new entry to the FAQ describing this issue.
>
> Looking at the code i think i understood what you're doing (thanks to
> your clean coding style and the extensive comments), but i don't yet
> say that i grok it (for that i will need more time that i'll invest
> happily).
>
> The tests which i started this morning are still running. Everything's fine.
>
> Good night! :)
> Christoph
>



- Show quoted text -
--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 21
	
	
Reply
	
	Follow up message
Hi Ger!

Back from work, baby in bed, household done -> time for coding!

Regarding the max_db issue there's a unittest which modifies the
max_db value, then reopens the environment and makes sure that the
value is still modified (env.cpp, maxDatabasesReopenTest).
If the issue you encountered pops up again then pls tell me, then i'll
extend the test and fix it.

i'll have a look at uncrustify - not for your code, but for the stuff
i encounter at work :)

regarding your other issues:

a) (unittests)
i'm absolutely fine with the tests you added. i understand them, and
i have a pragmatic point of view regarding the tests.

b, c) (get_parameters)
I don't have a better idea how to implement this. an alternative would
be to create one new getter for each parameter - and that would
explode the API and be really a nuisance. I think retrieving the
parameters is a good idea.
regarding the maxkeys thing i'll have a look and fix it, if possible

d) (doxygen)
i have never used doxygen to document the internal implementaiton,
only the header file. if some functions are missing then maybe because
they're not assigned to the correct sections. i'll have a look in the
next days and make sure that the documentation is ok (i anyway wanted
to modify the doxygen stuff because the online documentation is super
ugly).

btw - regarding the client-server architecture: this is actually a
really sexy thing. if it's done well, it could be faster than your
current setup (mmap on top of SMB shares). But even more interesting
for me is a replication mechanism or even high availability, and of
course the usage in a web server (i.e. with PHP). But anyway, this
would not affect you at all. it could be disabled at compile-time and
has to be enabled with a special runtime option (i.e. by specifying a
special filename like "tcp://10.20.30.40/home/chris/database.db").

anyway, these are my short-term plans:

- for the next one or two weeks i'll continue working on the current
code, adding tests and doing some other things i wanted to do (improve
documentation etc). The release itself will take about 2 days - it's
automated, but the tests run really long.

- afterwards, i'll do a few weeks for hamsterdb2 to keep the fire burning :)

- afterwards, i'll implement two new optimizations for hamsterdb; they
should bring good performance wins for your specific use case:
 1) ham_cursor_insert with option (HAM_INSERT_HINT_APPEND) - see a
previous mail
 2) a special option for a "lazy split"; in your specific case, it's
not necessary to split the B+Tree pages in the middle. It's better to
split "far right" in the page. As a result you will need less splits
-> get more performance and less wasted space

- afterwards, another release for hamsterdb2

etc

You see that my development time is limited - during the week i
sometimes just get 1 or 2 hours. That sucks, but work and family have
priority. :-/

Can you send me your postal address? then i send you the stuff for the
commercial license. i'm really happy that i have a second commercial
user :)

Have a nice evening,
Christoph

2009/7/21 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Just woke up after the crash; will return to horizontal inactivity in
> half a hour.
>
> The max_db setting in open was because when I started using hamster I
> ran into the issue of not being able to use more than 16? 32?
> databases in a single ENV; IIRC creation was fine but opening them in
> a second run was a no-go. May have been me screwing up, though.
>
> Regarding vi editor and console windows: I always set my console
> windows to 132 wide and small font - heck, my eyes were already shot
> when I was 5, so reading fine print is perfectly fine for me.
> You may consider using uncrustify (see sourceforge) to reformat if you
> want; I use that tool myself, but didn't yet on the hamster edits I
> made - was a bit of time pressure cooker, really.
> I'm glad you could follow what I've done through the comments; they're
> there because I tend to forget what I was smoking/thinking when I
> revisit my own stuff after 6 months or more, let alone work from
> others' that I've been expanding.
>
> Thanks for finding the pointer issue; I'll get a fresh SVN dump now
> and see what it shows me later today.
>
>
>
> Couple of things I'd like to hear your opinion about:
>
> a)
> I'm still two minds about the way I've been exploding your unittest
> code; this way was the fastest for me and given the comment section
> there you can see where I want to take it, but it's not as clean and
> simple as the other checks in there, so a bit of a wart. While cutting
> it up into chunks means the interdependencies need to be managed,
> given the new state of the art regarding BFC, as that one can now run
> a single test method. Architectural thoughts are welcomed.
>
> b)
> The other bit is about those get_parameter functions; I like my
> diagnostics extremely verbose, and that's where those help out, but
> the maxkey calc is sitting there too, not really a param, but ... and
> then I find that the maxkey calc is wrong anyway as large keys are
> split into small parts plus blobs, so my original intent for testing
> by creating pages which would house only 5 keys each is not how it
> works - I saw that while debugging the latest LT/GT logic - while the
> other purpose: calculating pagesize to fit the maximum of 65534 (even
> number) leys in a single page as I will be creating huge tables isn't
> straightforward either.
> Anyway, the thought of the moment /was/ to dump it into get_parameters
> by way oif some extra HAM_PARAM_ constants == param records, but I'm
> not sure about that either; doesn't feel 'right' to me still.
>
> c)
> If you've got a better idea how the API can assist a user who's
> looking for pagesizes derived from keysize and
> number-of-keys-per-page, I'm all for it.
>
> d)
> Regarding flow for find: I intend to write that one up a bit better as
> it's very subtle, especially when you're mixing LT/GT/EQ in there and
> consider the scenarios where you end up on a page edge or not. I was
> thinking about doing this in a DoxyGen compatible way, but it's been
> too long since I last used Doxy (I know it supports dot/neato
> diagrams, which are neat) and I recall it was a bitch to get Doxy to
> produce output which lists things like additional 'technical design &
> elaboration on internal implementation' main chapters as such things
> are not really tightly related to a function/class or group of those.
> Any pointers there? I'll check out doxy's documentation anyway, but
> maybe you've got a few ideas there.
>
> I noticed that not all functions came up in the main sections of the
> DoxyGen generated documentation; they are there, but I only found them
> after opening the html for the header files, which is not how I like
> things to work for documentation. I know/recall this was always
> troublesome -- while DoxyGen was one of the easier-to-manipulate tools
> out there; don't get me started on JavaDoc :-(
>
>
> Enough for tonight. I'll grab a midnight snack and enjoy sleep again.
>
> Ciao,
>
> Ger
>
>
> PS: you're fast with the merge and fixing stuff. Nice to see that. :-)
>
> PPS: regarding performance and freelists and all: I've had this
> thought about adding a sort of skiplist-ish thing in there for faster
> bitslot searching, but that's just a vague idea at the back of the
> head right now. Before I go on a wild goose chase there I should
> recover my old MSVC2003 rig with DevPartner (formerly NuMega) setup --
> if the license keys and the software still wish to install on XP32 --
> so that I can run by-line profiling again. It's sorely lacking in
> 2005/2008 and all I see now is the CPU maxing out when I run the
> stresstest using local file I/O.
> But that's probably not happening this month - or the next. There's
> only 24 hours in a day anyway.
>
>
>
> On Mon, Jul 20, 2009 at 10:48 PM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger!
>>
>> I merged all your changes, some with modifications.
>>
>> In general i try to avoid tabs, but i don't mind if some of them "slip
>> through". But i use vi as text editor in a console window (yes,
>> really! :) ) and therefore i put line-breaks after 80 characters.
>>
>> in hamsterdb.h i made some changes - i.e. renamed ham_get_env_params
>> to ham_env_get_parameters, for consistency reasons (same for
>> ham_get_db_params). also, i moved the HAM_FIND_* flags closer to the
>> ham_cursor_find function. so basically my modifications were
>> cosmetically.
>>
>> in hamsterdb.c i found something critical, i think. ham_open_ex sets
>> the env_set_max_databases value. But this value can only be set in
>> ham_create. in hamsterdb, the freelist starts in the header page after
>> the database descriptors. if the max_databases value is incremented,
>> this array must be resized and the freelist start must be moved, but
>> that would be too complicated. Therefore i didn't merge this change.
>>
>> The unittest segfaults at the new test, because a pointer value was
>> bogus. then i had to change some parameters which were not allowed in
>> create_db/open_db. i fixed these things and now it works fine. I saw
>> that valgrind found some memory leaks, but i'll get them tomorrow or
>> so (they're so easy to find with valgrind).
>>
>> The CppApi tests now fail with a strange message, i'll have a look
>> tomorrow evening. I'll call it a day now. Today's my last day of
>> vacation :( I'll have a closer look tomorrow, and i think i'll also
>> find the memory leak till then. And then i want to add more tests for
>> the approximate matching. If i have enough time then i really hope i
>> can make a release till next weekend, but i can't promise anything.
>>
>> I'm really glad that the weird freelist bug disappeared. i'll add a
>> new entry to the FAQ describing this issue.
>>
>> Looking at the code i think i understood what you're doing (thanks to
>> your clean coding style and the extensive comments), but i don't yet
>> say that i grok it (for that i will need more time that i'll invest
>> happily).
>>
>> The tests which i started this morning are still running. Everything's fine.
>>
>> Good night! :)
>> Christoph
>>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 21
	
	
Reply
	
	Follow up message
Hi Ger,

i found the issue with max_dbs. it's in a CppApi-unittest. i'll take care of it.

Regards
Christoph

2009/7/21 Christoph Rupp <chris@crupp.de>:
- Show quoted text -
> Hi Ger!
>
> Back from work, baby in bed, household done -> time for coding!
>
> Regarding the max_db issue there's a unittest which modifies the
> max_db value, then reopens the environment and makes sure that the
> value is still modified (env.cpp, maxDatabasesReopenTest).
> If the issue you encountered pops up again then pls tell me, then i'll
> extend the test and fix it.
>
> i'll have a look at uncrustify - not for your code, but for the stuff
> i encounter at work :)
>
> regarding your other issues:
>
> a) (unittests)
> i'm absolutely fine with the tests you added. i understand them, and
> i have a pragmatic point of view regarding the tests.
>
> b, c) (get_parameters)
> I don't have a better idea how to implement this. an alternative would
> be to create one new getter for each parameter - and that would
> explode the API and be really a nuisance. I think retrieving the
> parameters is a good idea.
> regarding the maxkeys thing i'll have a look and fix it, if possible
>
> d) (doxygen)
> i have never used doxygen to document the internal implementaiton,
> only the header file. if some functions are missing then maybe because
> they're not assigned to the correct sections. i'll have a look in the
> next days and make sure that the documentation is ok (i anyway wanted
> to modify the doxygen stuff because the online documentation is super
> ugly).
>
> btw - regarding the client-server architecture: this is actually a
> really sexy thing. if it's done well, it could be faster than your
> current setup (mmap on top of SMB shares). But even more interesting
> for me is a replication mechanism or even high availability, and of
> course the usage in a web server (i.e. with PHP). But anyway, this
> would not affect you at all. it could be disabled at compile-time and
> has to be enabled with a special runtime option (i.e. by specifying a
> special filename like "tcp://10.20.30.40/home/chris/database.db").
>
> anyway, these are my short-term plans:
>
> - for the next one or two weeks i'll continue working on the current
> code, adding tests and doing some other things i wanted to do (improve
> documentation etc). The release itself will take about 2 days - it's
> automated, but the tests run really long.
>
> - afterwards, i'll do a few weeks for hamsterdb2 to keep the fire burning :)
>
> - afterwards, i'll implement two new optimizations for hamsterdb; they
> should bring good performance wins for your specific use case:
>  1) ham_cursor_insert with option (HAM_INSERT_HINT_APPEND) - see a
> previous mail
>  2) a special option for a "lazy split"; in your specific case, it's
> not necessary to split the B+Tree pages in the middle. It's better to
> split "far right" in the page. As a result you will need less splits
> -> get more performance and less wasted space
>
> - afterwards, another release for hamsterdb2
>
> etc
>
> You see that my development time is limited - during the week i
> sometimes just get 1 or 2 hours. That sucks, but work and family have
> priority. :-/
>
> Can you send me your postal address? then i send you the stuff for the
> commercial license. i'm really happy that i have a second commercial
> user :)
>
> Have a nice evening,
> Christoph
>
> 2009/7/21 Ger Hobbelt <ger@hobbelt.com>:
>> Just woke up after the crash; will return to horizontal inactivity in
>> half a hour.
>>
>> The max_db setting in open was because when I started using hamster I
>> ran into the issue of not being able to use more than 16? 32?
>> databases in a single ENV; IIRC creation was fine but opening them in
>> a second run was a no-go. May have been me screwing up, though.
>>
>> Regarding vi editor and console windows: I always set my console
>> windows to 132 wide and small font - heck, my eyes were already shot
>> when I was 5, so reading fine print is perfectly fine for me.
>> You may consider using uncrustify (see sourceforge) to reformat if you
>> want; I use that tool myself, but didn't yet on the hamster edits I
>> made - was a bit of time pressure cooker, really.
>> I'm glad you could follow what I've done through the comments; they're
>> there because I tend to forget what I was smoking/thinking when I
>> revisit my own stuff after 6 months or more, let alone work from
>> others' that I've been expanding.
>>
>> Thanks for finding the pointer issue; I'll get a fresh SVN dump now
>> and see what it shows me later today.
>>
>>
>>
>> Couple of things I'd like to hear your opinion about:
>>
>> a)
>> I'm still two minds about the way I've been exploding your unittest
>> code; this way was the fastest for me and given the comment section
>> there you can see where I want to take it, but it's not as clean and
>> simple as the other checks in there, so a bit of a wart. While cutting
>> it up into chunks means the interdependencies need to be managed,
>> given the new state of the art regarding BFC, as that one can now run
>> a single test method. Architectural thoughts are welcomed.
>>
>> b)
>> The other bit is about those get_parameter functions; I like my
>> diagnostics extremely verbose, and that's where those help out, but
>> the maxkey calc is sitting there too, not really a param, but ... and
>> then I find that the maxkey calc is wrong anyway as large keys are
>> split into small parts plus blobs, so my original intent for testing
>> by creating pages which would house only 5 keys each is not how it
>> works - I saw that while debugging the latest LT/GT logic - while the
>> other purpose: calculating pagesize to fit the maximum of 65534 (even
>> number) leys in a single page as I will be creating huge tables isn't
>> straightforward either.
>> Anyway, the thought of the moment /was/ to dump it into get_parameters
>> by way oif some extra HAM_PARAM_ constants == param records, but I'm
>> not sure about that either; doesn't feel 'right' to me still.
>>
>> c)
>> If you've got a better idea how the API can assist a user who's
>> looking for pagesizes derived from keysize and
>> number-of-keys-per-page, I'm all for it.
>>
>> d)
>> Regarding flow for find: I intend to write that one up a bit better as
>> it's very subtle, especially when you're mixing LT/GT/EQ in there and
>> consider the scenarios where you end up on a page edge or not. I was
>> thinking about doing this in a DoxyGen compatible way, but it's been
>> too long since I last used Doxy (I know it supports dot/neato
>> diagrams, which are neat) and I recall it was a bitch to get Doxy to
>> produce output which lists things like additional 'technical design &
>> elaboration on internal implementation' main chapters as such things
>> are not really tightly related to a function/class or group of those.
>> Any pointers there? I'll check out doxy's documentation anyway, but
>> maybe you've got a few ideas there.
>>
>> I noticed that not all functions came up in the main sections of the
>> DoxyGen generated documentation; they are there, but I only found them
>> after opening the html for the header files, which is not how I like
>> things to work for documentation. I know/recall this was always
>> troublesome -- while DoxyGen was one of the easier-to-manipulate tools
>> out there; don't get me started on JavaDoc :-(
>>
>>
>> Enough for tonight. I'll grab a midnight snack and enjoy sleep again.
>>
>> Ciao,
>>
>> Ger
>>
>>
>> PS: you're fast with the merge and fixing stuff. Nice to see that. :-)
>>
>> PPS: regarding performance and freelists and all: I've had this
>> thought about adding a sort of skiplist-ish thing in there for faster
>> bitslot searching, but that's just a vague idea at the back of the
>> head right now. Before I go on a wild goose chase there I should
>> recover my old MSVC2003 rig with DevPartner (formerly NuMega) setup --
>> if the license keys and the software still wish to install on XP32 --
>> so that I can run by-line profiling again. It's sorely lacking in
>> 2005/2008 and all I see now is the CPU maxing out when I run the
>> stresstest using local file I/O.
>> But that's probably not happening this month - or the next. There's
>> only 24 hours in a day anyway.
>>
>>
>>
>> On Mon, Jul 20, 2009 at 10:48 PM, Christoph Rupp<chris@crupp.de> wrote:
>>> Hi Ger!
>>>
>>> I merged all your changes, some with modifications.
>>>
>>> In general i try to avoid tabs, but i don't mind if some of them "slip
>>> through". But i use vi as text editor in a console window (yes,
>>> really! :) ) and therefore i put line-breaks after 80 characters.
>>>
>>> in hamsterdb.h i made some changes - i.e. renamed ham_get_env_params
>>> to ham_env_get_parameters, for consistency reasons (same for
>>> ham_get_db_params). also, i moved the HAM_FIND_* flags closer to the
>>> ham_cursor_find function. so basically my modifications were
>>> cosmetically.
>>>
>>> in hamsterdb.c i found something critical, i think. ham_open_ex sets
>>> the env_set_max_databases value. But this value can only be set in
>>> ham_create. in hamsterdb, the freelist starts in the header page after
>>> the database descriptors. if the max_databases value is incremented,
>>> this array must be resized and the freelist start must be moved, but
>>> that would be too complicated. Therefore i didn't merge this change.
>>>
>>> The unittest segfaults at the new test, because a pointer value was
>>> bogus. then i had to change some parameters which were not allowed in
>>> create_db/open_db. i fixed these things and now it works fine. I saw
>>> that valgrind found some memory leaks, but i'll get them tomorrow or
>>> so (they're so easy to find with valgrind).
>>>
>>> The CppApi tests now fail with a strange message, i'll have a look
>>> tomorrow evening. I'll call it a day now. Today's my last day of
>>> vacation :( I'll have a closer look tomorrow, and i think i'll also
>>> find the memory leak till then. And then i want to add more tests for
>>> the approximate matching. If i have enough time then i really hope i
>>> can make a release till next weekend, but i can't promise anything.
>>>
>>> I'm really glad that the weird freelist bug disappeared. i'll add a
>>> new entry to the FAQ describing this issue.
>>>
>>> Looking at the code i think i understood what you're doing (thanks to
>>> your clean coding style and the extensive comments), but i don't yet
>>> say that i grok it (for that i will need more time that i'll invest
>>> happily).
>>>
>>> The tests which i started this morning are still running. Everything's fine.
>>>
>>> Good night! :)
>>> Christoph
>>>
>>
>>
>>
>> --
>> Met vriendelijke groeten / Best regards,
>>
>> Ger Hobbelt
>>
>> --------------------------------------------------
>> web:    http://www.hobbelt.com/
>>        http://www.hebbut.net/
>> mail:   ger@hobbelt.com
>> mobile: +31-6-11 120 978
>> --------------------------------------------------
>>
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
Easy there. ;-)

I've been digging into it, and it's not soo much the CppApi tests as
much that those tests do something (legal from the documentation way,
but I am currently investigating if this case is actually coped with
in the core - seems it isn't); the other test fictures do not include
this scenario -- which is currently resolved, so I get b0rk-b0rk-b0rk
over in env.cpp too now, thansk to an extra test.

The scenario:

create and env, set the params, such as maxdb

close it

open it at a later time and add / create DBs to the env: the env
settings such as max_databases is not set, as the header page is not
loaded (or not saved in the original create+close before; currently I
am debugging this)



And regarding the freelist speedup: hold your horses for a little
while there as well, because there's a couple of ideas I like to check
out there and it would be really nice to have your and my ideas there
in there in a way that makes for an easy mix.
Besides, there's a couple of things in freelist that are buggy, unless
I am mistaken. Will post about this later today, for sure. (One of the
bugs is: max_bits is a 16-bit var, but the freelist bitarray  size is
larger than 2^16 bits when you have nicely large pages (Windows!).
This has probably not led to crashes and other buggery uptil today as
the first effect of this is artificial sparseness as a bunch of slots
are lost once the size of the bitarray is set (clipped), while the
max_allocated_bits has a similar issue.

There's already a few tweaks and tugs in there, but the additional
asserts say it all when run (freelist code as of now attached; will
cause compiler errors, so only for looking at to see the intermediate
state of affairs as you've little dev time right now; there's way more
coming your way regarding this later on anyhow. [See next mail which
is a forward of a braindump I wrote for myself yesterday to get an
inkling of where I'll be going with this thing if I've enough time /
dev speed. Pray I don't make too many bugs ;-) ]




--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
2 attachments  Download all attachments  
freelist.c	freelist.c
18K   Download  
freelist.h	freelist.h
6K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
On Tue, Jul 21, 2009 at 10:15 PM, Christoph Rupp<chris@crupp.de> wrote:
> d) (doxygen)
> i have never used doxygen to document the internal implementaiton,

No sweat. I checked out their website and have a couple of ideas how I
can approach this. Second priority right now, though.
(Typical dev: postponing documentation till beyond the end ;-)  Still
not learned my lesson at 40, sigh.)

> only the header file. if some functions are missing then maybe because
> they're not assigned to the correct sections. i'll have a look in the
> next days and make sure that the documentation is ok (i anyway wanted
> to modify the doxygen stuff because the online documentation is super
> ugly).

Oh? The latest doxy run here on the Ubuntu box tunred up some cute
looking stuff; far nicer than the JavaDoc cruft I've had to wade
through in the past.

> btw - regarding the client-server architecture: this is actually a
> really sexy thing. if it's done well, it could be faster than your
> current setup (mmap on top of SMB shares). But even more interesting
> for me is a replication mechanism or even high availability, and of
> course the usage in a web server (i.e. with PHP). But anyway, this
> would not affect you at all. it could be disabled at compile-time and
> has to be enabled with a special runtime option (i.e. by specifying a
> special filename like "tcp://10.20.30.40/home/chris/database.db").

I can see why you want it, but local mmap is faster than any network
I/O, at least on the machines I've been working with.

With doesn't invalidate your concept; I can see the benefit for
embedded and other apps, who like a 'ppor mans Oracle' db server with
fastest possible I/O (and sans the SQL overhead).

> You see that my development time is limited - during the week i
> sometimes just get 1 or 2 hours. That sucks, but work and family have
> priority. :-/

I already assumed as much ;-)
Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
version bump? And architecture/code-wise, is the LT/GT concept
portable to v2, or does that one already have this functionality? (I'm
wondering if an upgrade path is advisable, because, when H2 is a major
version jump, it might be better to put this functionality in the
freshest version, i.e. v2.)


> Can you send me your postal address? then i send you the stuff for the
> commercial license. i'm really happy that i have a second commercial
> user :)

Me too ;-)

address:

G.E.G. Hobbelt
Oude Boomgaardstraat 55
2513TP
Den Haag
The Netherlands (Holland)




--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 22
	
	
Reply
	
	Follow up message
Hi Ger,

2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
> (Typical dev: postponing documentation till beyond the end ;-)  Still
> not learned my lesson at 40, sigh.)
Ha! Now i know why you know about ancient systems like VMS (which i
never used or even saw!) Although - i'm 31 and i still use vi in a
text shell...

>> they're not assigned to the correct sections. i'll have a look in the
>> next days and make sure that the documentation is ok (i anyway wanted
>> to modify the doxygen stuff because the online documentation is super
>> ugly).
>
> Oh? The latest doxy run here on the Ubuntu box tunred up some cute
> looking stuff; far nicer than the JavaDoc cruft I've had to wade
> through in the past.

I mean the special CSS i hand-crafted for the uploaded documentation
which is online at hamsterdb.com :)

> Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
> version bump? And architecture/code-wise, is the LT/GT concept
> portable to v2, or does that one already have this functionality? (I'm
> wondering if an upgrade path is advisable, because, when H2 is a major
> version jump, it might be better to put this functionality in the
> freshest version, i.e. v2.)

No, it's not a new big version. actually it's a completely different
database for a completely different market. See
http://crupp.de/2009/07/19/hamsterdb-products-reorganized/

hamsterdb2 is still unstable and also slower than hamsterdb. That's
also due to architectural issues - i don't think i can reach this
performance. But it's multithreaded and supports ACID transactions, so
it's more in the direction of business applications and modern
hardware while hamsterdb is for raw performance and runs on embedded
devices.

The LT/GT concept will be portable, but so far i don't even have
database cursors; they will be difficult to implement compared to
hamsterdb1, because the index trees in hamsterdb2 are split (each
transaction can create an index tree) and then are merged when the
transaction is committed. A cursor will have to walk over all these
trees and always picking the right "next" element.

Regarding your other mail and the CppApi test: yes, as far as i saw
yesterday it fails when an environment is created, closed and then a
new database is created (and the header file is not read). You really
don't need to spend time on this - i'll fix it.

Have a nice day,
Christoph
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
> Ha! Now i know why you know about ancient systems like VMS (which i
> never used or even saw!) Although - i'm 31 and i still use vi in a
> text shell...

ancient... tsk tsk tsk... only 3 years ago I worked on another VMS
machine which managed the entire production of a Dutch plant of the
major international Aluminium processing company.    But, yeah, it's
lovable antique. Me Tyrannosaurus Rex, you VMS.
Though it's a real pity you haven't run into it; it is a huge contrast
to UNIX and some of the Windows concepts would be easier to grok when
you've met VMS: some of the Digital engineers moved to MS and helped
design the NT kernel. And don't get it wrong: VMS machines are among
the most rock-solid platforms out there: this particular box I've been
working on has been serving 20 years 24/7 on a single stretch.
For flavor, here's the SYS RMS manual; check out the $FIND section for
a scare (or love, in my case). FAB$ RAB$ etc. are comparable to key
and record structures. The whole thing is language-independent, BTW.
(LT/GT you do there by mixing 'reverse' and 'ge/gt' flags to get lt/le
behaviour.) VMS is the only mainline OS I know with build-in HamsterDB
;-)  When creating files you can state whether those are line-based
record files or otherwise and how you wish to access them.


> I mean the special CSS i hand-crafted for the uploaded documentation
> which is online at hamsterdb.com :)

Ah, well. My way out there is to beg a CSS friend to do these things
for me. Except my CSS friends come few and don't have time to help me
out. Sigh. So you get hebbut.net as it is. Hamster web is far better
than that, so never mind.

>> Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
[...]
> it's more in the direction of business applications and modern
> hardware while hamsterdb is for raw performance and runs on embedded
> devices.

Stupid remark, but isn't that in direct competition with BerkeleyDB,
which is now Oracle? My tea leaves read that as 'uphill battle', or I
am way off?


> The LT/GT concept will be portable, but so far i don't even have
> database cursors; they will be difficult to implement compared to
> hamsterdb1, because the index trees in hamsterdb2 are split (each
> transaction can create an index tree) and then are merged when the
> transaction is committed. A cursor will have to walk over all these
> trees and always picking the right "next" element.

Hm, sounds like copy-on-write on a per-transaction basis. I can see
the trouble with handling the merging at transaction completion and
the cursors are indeed non-trivial. Challenging.


> Regarding your other mail and the CppApi test: yes, as far as i saw
> yesterday it fails when an environment is created, closed and then a
> new database is created (and the header file is not read). You really
> don't need to spend time on this - i'll fix it.

No worries. I am looking into the freelist stuff (ah, should forward
you that braindump, heck) and this is a sideline; when I find
something, you can always vet it. Right now I concentrate on hamster
for a few days as it needs to be rock-solid for my use, which is multi
DBs in an env. And as long as I get things working and you check them
we have a faster process than me waiting for you to fix it - that
simply too much pressure for you at a 1-2 hour tyime slot per day, no
matter how fast you are.
Better to use that time to check and correct me.

Don't worry, when this is done, I'll certainly drop activity on
hamster for a long time as other parts of my systems will take up my
time then ;-)
Meanwhile, I benefit more from you being clearheaded and able to check
the things than pushing your limits as I do my own: when I'd do that I
lose the most important reviewer and bugs have a higher risk of
lingering. And I know for a fact Mr. Murphy is a big fan of mine,
though he always takes his time to arrive.
- Show quoted text -

--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
OpenVMS 7.3.1 RMS reference manual.pdf	OpenVMS 7.3.1 RMS reference manual.pdf
2545K   View   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 22
	
	
Reply
	
	Follow up message
Hi Ger,

regarding the environment bug with the lost maxkey-value:

the problem is a bit more tricky.

When an environment is created, and then immediately closed, a
0-byte-file is written. All the settings are lost.

I'll rewrite the routines so that
- ham_env_create always writes the header page
- ham_env_open always reads the header page

This is difficult esp. because a page can only be written by a
database; and this database does not yet exist.

i'll keep you updated, but i try to fix this now.

Regards
Christoph

2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
>> Ha! Now i know why you know about ancient systems like VMS (which i
>> never used or even saw!) Although - i'm 31 and i still use vi in a
>> text shell...
>
> ancient... tsk tsk tsk... only 3 years ago I worked on another VMS
> machine which managed the entire production of a Dutch plant of the
> major international Aluminium processing company.    But, yeah, it's
> lovable antique. Me Tyrannosaurus Rex, you VMS.
> Though it's a real pity you haven't run into it; it is a huge contrast
> to UNIX and some of the Windows concepts would be easier to grok when
> you've met VMS: some of the Digital engineers moved to MS and helped
> design the NT kernel. And don't get it wrong: VMS machines are among
> the most rock-solid platforms out there: this particular box I've been
> working on has been serving 20 years 24/7 on a single stretch.
> For flavor, here's the SYS RMS manual; check out the $FIND section for
> a scare (or love, in my case). FAB$ RAB$ etc. are comparable to key
> and record structures. The whole thing is language-independent, BTW.
> (LT/GT you do there by mixing 'reverse' and 'ge/gt' flags to get lt/le
> behaviour.) VMS is the only mainline OS I know with build-in HamsterDB
> ;-)  When creating files you can state whether those are line-based
> record files or otherwise and how you wish to access them.
>
>
>> I mean the special CSS i hand-crafted for the uploaded documentation
>> which is online at hamsterdb.com :)
>
> Ah, well. My way out there is to beg a CSS friend to do these things
> for me. Except my CSS friends come few and don't have time to help me
> out. Sigh. So you get hebbut.net as it is. Hamster web is far better
> than that, so never mind.
>
>>> Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
> [...]
>> it's more in the direction of business applications and modern
>> hardware while hamsterdb is for raw performance and runs on embedded
>> devices.
>
> Stupid remark, but isn't that in direct competition with BerkeleyDB,
> which is now Oracle? My tea leaves read that as 'uphill battle', or I
> am way off?
>
>
>> The LT/GT concept will be portable, but so far i don't even have
>> database cursors; they will be difficult to implement compared to
>> hamsterdb1, because the index trees in hamsterdb2 are split (each
>> transaction can create an index tree) and then are merged when the
>> transaction is committed. A cursor will have to walk over all these
>> trees and always picking the right "next" element.
>
> Hm, sounds like copy-on-write on a per-transaction basis. I can see
> the trouble with handling the merging at transaction completion and
> the cursors are indeed non-trivial. Challenging.
>
>
>> Regarding your other mail and the CppApi test: yes, as far as i saw
>> yesterday it fails when an environment is created, closed and then a
>> new database is created (and the header file is not read). You really
>> don't need to spend time on this - i'll fix it.
>
> No worries. I am looking into the freelist stuff (ah, should forward
> you that braindump, heck) and this is a sideline; when I find
> something, you can always vet it. Right now I concentrate on hamster
> for a few days as it needs to be rock-solid for my use, which is multi
> DBs in an env. And as long as I get things working and you check them
> we have a faster process than me waiting for you to fix it - that
> simply too much pressure for you at a 1-2 hour tyime slot per day, no
> matter how fast you are.
> Better to use that time to check and correct me.
>
> Don't worry, when this is done, I'll certainly drop activity on
> hamster for a long time as other parts of my systems will take up my
> time then ;-)
> Meanwhile, I benefit more from you being clearheaded and able to check
> the things than pushing your limits as I do my own: when I'd do that I
> lose the most important reviewer and bugs have a higher risk of
> lingering. And I know for a fact Mr. Murphy is a big fan of mine,
> though he always takes his time to arrive.
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
Christoph,

I already have a fix for that. Like I said: no sweat! ;-)

There's some more stuff that I'm checking, including a little glitch
in the BFC material (multiple invocations), which is also fixed by
now, but I'm still checking other stuff as well, so that's why I
didn't send a new set yet.
Hold your horses.
- Show quoted text -



On Wed, Jul 22, 2009 at 8:55 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> regarding the environment bug with the lost maxkey-value:
>
> the problem is a bit more tricky.
>
> When an environment is created, and then immediately closed, a
> 0-byte-file is written. All the settings are lost.
>
> I'll rewrite the routines so that
> - ham_env_create always writes the header page
> - ham_env_open always reads the header page
>
> This is difficult esp. because a page can only be written by a
> database; and this database does not yet exist.
>
> i'll keep you updated, but i try to fix this now.
>
> Regards
> Christoph
>
> 2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
>>> Ha! Now i know why you know about ancient systems like VMS (which i
>>> never used or even saw!) Although - i'm 31 and i still use vi in a
>>> text shell...
>>
>> ancient... tsk tsk tsk... only 3 years ago I worked on another VMS
>> machine which managed the entire production of a Dutch plant of the
>> major international Aluminium processing company.    But, yeah, it's
>> lovable antique. Me Tyrannosaurus Rex, you VMS.
>> Though it's a real pity you haven't run into it; it is a huge contrast
>> to UNIX and some of the Windows concepts would be easier to grok when
>> you've met VMS: some of the Digital engineers moved to MS and helped
>> design the NT kernel. And don't get it wrong: VMS machines are among
>> the most rock-solid platforms out there: this particular box I've been
>> working on has been serving 20 years 24/7 on a single stretch.
>> For flavor, here's the SYS RMS manual; check out the $FIND section for
>> a scare (or love, in my case). FAB$ RAB$ etc. are comparable to key
>> and record structures. The whole thing is language-independent, BTW.
>> (LT/GT you do there by mixing 'reverse' and 'ge/gt' flags to get lt/le
>> behaviour.) VMS is the only mainline OS I know with build-in HamsterDB
>> ;-)  When creating files you can state whether those are line-based
>> record files or otherwise and how you wish to access them.
>>
>>
>>> I mean the special CSS i hand-crafted for the uploaded documentation
>>> which is online at hamsterdb.com :)
>>
>> Ah, well. My way out there is to beg a CSS friend to do these things
>> for me. Except my CSS friends come few and don't have time to help me
>> out. Sigh. So you get hebbut.net as it is. Hamster web is far better
>> than that, so never mind.
>>
>>>> Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
>> [...]
>>> it's more in the direction of business applications and modern
>>> hardware while hamsterdb is for raw performance and runs on embedded
>>> devices.
>>
>> Stupid remark, but isn't that in direct competition with BerkeleyDB,
>> which is now Oracle? My tea leaves read that as 'uphill battle', or I
>> am way off?
>>
>>
>>> The LT/GT concept will be portable, but so far i don't even have
>>> database cursors; they will be difficult to implement compared to
>>> hamsterdb1, because the index trees in hamsterdb2 are split (each
>>> transaction can create an index tree) and then are merged when the
>>> transaction is committed. A cursor will have to walk over all these
>>> trees and always picking the right "next" element.
>>
>> Hm, sounds like copy-on-write on a per-transaction basis. I can see
>> the trouble with handling the merging at transaction completion and
>> the cursors are indeed non-trivial. Challenging.
>>
>>
>>> Regarding your other mail and the CppApi test: yes, as far as i saw
>>> yesterday it fails when an environment is created, closed and then a
>>> new database is created (and the header file is not read). You really
>>> don't need to spend time on this - i'll fix it.
>>
>> No worries. I am looking into the freelist stuff (ah, should forward
>> you that braindump, heck) and this is a sideline; when I find
>> something, you can always vet it. Right now I concentrate on hamster
>> for a few days as it needs to be rock-solid for my use, which is multi
>> DBs in an env. And as long as I get things working and you check them
>> we have a faster process than me waiting for you to fix it - that
>> simply too much pressure for you at a 1-2 hour tyime slot per day, no
>> matter how fast you are.
>> Better to use that time to check and correct me.
>>
>> Don't worry, when this is done, I'll certainly drop activity on
>> hamster for a long time as other parts of my systems will take up my
>> time then ;-)
>> Meanwhile, I benefit more from you being clearheaded and able to check
>> the things than pushing your limits as I do my own: when I'd do that I
>> lose the most important reviewer and bugs have a higher risk of
>> lingering. And I know for a fact Mr. Murphy is a big fan of mine,
>> though he always takes his time to arrive.
>>
>> --
>> Met vriendelijke groeten / Best regards,
>>
>> Ger Hobbelt
>>
>> --------------------------------------------------
>> web:    http://www.hobbelt.com/
>>        http://www.hebbut.net/
>> mail:   ger@hobbelt.com
>> mobile: +31-6-11 120 978
>> --------------------------------------------------
>>
>
>
>



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 22
	
	
Reply
	
	Follow up message
Here's a quick snap of the src+unittests source tree as it is now; the
fix for the empty env file due to create+close is in hamsterdb.c;
there's a couple of other things as well: blob.cpp fix to prevent
multiple runs of the same functions, and additional assertions around
the freelist code, which fired due to 16-bit int value overflow on
write: the old _bits() macros in there changed to _bytes(), but that
last stuff is VERY preliminary and not rigorously tested with large
pages; something that's still my list now, as I need to ponder this a
bit more.

It's a rough grab as
 make dist
doesn't do what I expect it to do (it's merging in all the Win32
binary build material too, and that's just bloat)

Anyway, no worries, I'm on it. Spend your time checking my stuff,
that's the spare hour a day best spent IMO. I have more hours a day to
put into this this week anyway, so that's the better division of
labour right now. I a couple of days, you're 'it' again, as I'll wind
down and move on to the next partition.




--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb.snap20090722-001.7z	hamsterdb.snap20090722-001.7z
201K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 22
	
	
Reply
	
	Follow up message
Whoa, you were a little bit faster than me :)

i'll check your fixes tomorrow and integrate them.

The make dist is not a big problem because i create the .tar.gz on
unix and therefore the win32 directory is empty. Otherwise in
Makefile.am the EXTRA_DIST section must be modified.

Going to answer your other mail...

2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Here's a quick snap of the src+unittests source tree as it is now; the
> fix for the empty env file due to create+close is in hamsterdb.c;
> there's a couple of other things as well: blob.cpp fix to prevent
> multiple runs of the same functions, and additional assertions around
> the freelist code, which fired due to 16-bit int value overflow on
> write: the old _bits() macros in there changed to _bytes(), but that
> last stuff is VERY preliminary and not rigorously tested with large
> pages; something that's still my list now, as I need to ponder this a
> bit more.
>
> It's a rough grab as
>  make dist
> doesn't do what I expect it to do (it's merging in all the Win32
> binary build material too, and that's just bloat)
>
> Anyway, no worries, I'm on it. Spend your time checking my stuff,
> that's the spare hour a day best spent IMO. I have more hours a day to
> put into this this week anyway, so that's the better division of
> labour right now. I a couple of days, you're 'it' again, as I'll wind
> down and move on to the next partition.
>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 22
	
	
Reply
	
	Follow up message
Thanks for the documentation of VMS - i will really have a look at it.
It's always interesting to learn from other stuff.

2009/7/22 Ger Hobbelt <ger@hobbelt.com>:
>>> Hm, what's hamsterDB2 compared to 1? Is it commercial-only or a major
> [...]
>> it's more in the direction of business applications and modern
>> hardware while hamsterdb is for raw performance and runs on embedded
>> devices.
>
> Stupid remark, but isn't that in direct competition with BerkeleyDB,
> which is now Oracle? My tea leaves read that as 'uphill battle', or I
> am way off?

Yes! Downhill battles are boring! :)

Actually, BerkeleyDB has 3 modi of operandi:
- raw storage, same as hamsterdb
- concurrent storage (shared memory, concurrent between processes)
- transactional storage (with concurrency)

hamsterdb2 is only concurrent between threads, and my transaction
support is much better than the one of bdb. bdb locks complete pages,
which means that transactions often block each other. That's actually
a problem for me because my acceptance tests rely on bdb (i execute
test scripts into hamsterdb and bdb and frequently check if both
databases are identical). and now i can't test my transaction routines
because bdb doesn't support them..

my plans are:
- hamsterdb embedded storage for raw performance, embedded devices and
(in a future far far away) client/server-support with replication and
high availability
(for fun i also want to use hamsterdb as a mysql backend)
- hamsterdb transactional storage for modern hardware, also
client-server stuff for distributed transactions, high availability,
and (in a future far far far far far away) SQL support. I have a
couple of good idea which would safe me lots of time and then my
uphill battle is against sqlite and mysql (from an architecture point
of view sqlite and mysql have huge drawbacks regarding to locking,
transactions and performance).

So that will keep me busy for the next few years :)

Have a nice evening, i'm going to bed now...
Christoph

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 23
	
	
Reply
	
	Follow up message
On Wed, Jul 22, 2009 at 11:11 PM, Christoph Rupp<chris@crupp.de> wrote:
> Whoa, you were a little bit faster than me :)

I had all day ;-)

> i'll check your fixes tomorrow and integrate them.

Keep the 7z I sent you around for comparison: that's regarding the
next bug & fix coming in: freelist _max_bits integer overflow.

The 7z snapshot shows how I changes the storage unit from bits to
bytes in a first attempt to see if I could compress the bugger into
word-size like it was before; one thing was already abundantly clear:
_allocated_bits couldn't have this reduction, so it has to be enlarged
to 32-bit dword storage size, thus *breaking* DB format already.
Which is my least worry right now; anyway, there'a another change
coming up as I reverted _max_bytes back to _max_bits as it still
overflows when you use larger pages (I expected/suspected as much, but
it's nice to see the asserts fail when expected).

That means the freelist format is changing, in memory and on disc, and
it has to because this caused DB store fragmentation as a lot of bits
per freelist page were lost: as in: never used.

(write 131072*8 into a 16-bit word and you're a goner. Which also
explains, at least partly, why my own stuff was so bloody darn slow:
it wasn't so much freelist as is, but rather freelist waiting for
freelist bitarray sizes which weren't tiny when MOD 65536'd, and that
would only happen when a new record would be inserted in newly
allocated storage. Rinse & repeat a million times and you've got a
time and disc space hog. Disc space, I've got plenty, CPU time is
always at a premium.)


Anyway, it makes sense it didn't hit UNIX as there pagesize ~ 4K by
default. On Win32/64, pagesize default is 64K thus leading to
_max_bits overflow (64K*8 - a bit of header) leading to a disc store
chunk loss of (L = 64K*8-E) : L - L MOD 64K ~ 86%.

Take a larger pagesize (like I did) and sparseness increases and
freelist page count increases accordingly: slow down.



***edit*** bumping up freelist vars to 32-bit size not only breaks DB
format compatibility, but I may have screwed up along the way, as now
I get a heap corruption on Win32. More debugging to do, then....


--
- Show quoted text -

































































\section hamster - the latest fixes & ruin
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 23
	
	
Reply
	
	Follow up message
Be warned: these fixes break binary compatibility with previous
versions (see the ham_u32_t in freelist.h and a probable issue in
btree as the offset from header to nodes isn't 11 but 12 bytes when
one follows the (packed) structure organization.

Crypto is completely BROKEN as this clashes very badly with the 'empty
env stores settings to disc' fix in hamsterdb.c

unittests which fail misserably have been disabled usin either
 return; // hack
or
 #if 0 // hack
lines.

Be aware that the constants in a btree unittest (11+28 --> 12+28,
which is correct as far as I can see) have been changed.


What's troubling (and hopefully fixed now)?

1)
create env
close env
open env
--> all settings, such as max_dbs, etc. are LOST.
This is fixed in hamsterdb.c by forcibly generating a header page when
creating the ENV: a dummy DB is created, then deleted again; the side
effect of that is that the header page remains. Unfortunately this fix
clashes with the assumptions and checks done in the enable_crypto
filter call set, resulting in access denied errors, while encryption
is turned into a write-only effort. I have no idea how to fix it,
apart from taking the empty-env fix apart and hand-crafting a custom
header page writing piece of code.

(By the way: env_open also accesses (opens) the database file to grab
those constants, otherwise the parameters won't make it into the env
structure; and the real env_open_db calls don't take care of it, while
other APIs (get_parameters, etc.) need these config info bits too, so
the sensible thing to do was add it to env_create and env_open.


2)
while looking at freelist, I noticed a few weird things happening.
See the freelist code, which is brimming with asserts; these asserts
will FAIL on Win32/64 platforms as the pagesize is > 8224 bytes:
that's the largest possible page when you don't want to lose space in
the freelist as it's length (in bits) is a 16-bit word; the fix turns
this into a 32-bit word, but that will BREAK the format backwards
compatibility!



3)
The dbname and other index header fields are now put in their own
(packed) structure and use accessor macros like the rest of 'em: no
more indexdata array read/write code. Purely cleanup.



4)
BFC has been augmented to allow running not only single tests, but
also sets of tests; very handy when developing and debugging /
checking particular sections.
On the commandline, things like this can be specified:

 unittest fixture1::test1 fixture2complete ::testInAnyFxture

where multiple entries in a single commandline will be simply executed
in order of appearance.

Ranges can be had by dropping in a literal star '*', e.g. execute all
tests in all fixtures up to and INcluding fixtureA:

 unittest '*' fixtureA

execute all tests starting at fixA::test5 up to and including fixC::test7:

 unittest fixA::test5 '*' fixC::test7

and executing any test including and following fixA::test7

 unittest fixA::test7 '*'

these different range and single fixture / test instructions can be
combined on a single command line, e.g.:

 unittest '*' uptohere_fixC   fixD::test8 '*' all_of_fixE  ::any_test8



5)
BFC can selectively catch C++ and hardware exceptions (the latter only
on Windows); turn off to allow exceptions to propagate into the
debugger, for instance. See main.cpp


6) Hacky: added Microsoft Heap Checking code to unittests/main.cpp --
there are a couple of ham_db_t structures not released in the current
unittest code, it seems.


7) Note that all those FFFFFFFF00000000 checking asserts in freelist
were pure systemdependent hacks of mine and have gone.


8) Various other tidbits I need to rediscover by looking at the diff.


9) final note: the OFFSETOF() macro use is important as that doesn't
break on systems where structs may be padded after all; the sizeof()-1
hacks don't cut it then.
I really hope I didn't screw it up, as some was rather hard to grok to
a level where this kind of editing is doable.

Note that a freelist on my Win32 system can store 500+K bits in a 64K
page, so a sequential scan as it is, indeed needs looking at then. But
I'm working on it; this transmission is so that we have a 'baseline'
and a comparison point which is still somewhat easy on the eyes when
comparing with vanilla 1.0.9 - I intend to augment the new freelist
structures with at least a 'model' entity, which distates which
freelist management system is being used for this env/db: that way,
different methods can be deployed and tested. That's where both our
mind-dumps from yesterday come in then.



Anyway, new package coming your way.

This is the preliminary before I dive into the freelist optimization;
that will break binary compatibility with previous versions for sure.

I am not able to find a solution for the broken crypto without major
hacking of the empty-env fix; I didn't (yet) feel like dedicating
enough time to write a custom header page write/read code there;
besides, I wonder if that would save us from trouble, as the crypto
still checks the page at setup time and performs a check - which, of
course - would still fail, as the header page would ALWAYS have been
crafted at a time BEFORE the crypto's been activated.
The alternative is an API change, where the crypto is folded into the
params, but then again, there's the whole business with the filters:
given the crypto break, my guess is all filters are broken that way
(zlib compression too).

Pleas review what I did; I can answer questions tomorrow; weekend is
questionable, but I'll see what I can do.





I'm unavailable from now till tomorrow at least: this afternoon and
this evening I have a couple of business meetings. Money to make,
hands to grease. ;-)





--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.20090723-001.tar.gz	hamsterdb-1.0.9.20090723-001.tar.gz
1084K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 23
	
	
Reply
	
	Follow up message
Hi Ger!

Wow, you've been busy :) Thanks a lot.

Regarding the env/headerpage issue: i also had the idea to create a
dummy database. I also have an idea how to do this without the need to
delete it again -> no wasted space, and it's faster because less I/O.
I'll check your code changes and then will merge it with my idea.

Regarding the freelist: i think it can be fixed without breaking
binary compatibility. Your idea of the model would help: on old
versions, the old model will be used, with new databases the new one.
But nevertheless i will try to fix the old model (alternatively, i'll
create an update mechanism, but i'm not sure if that's a good idea)...

Also, i'm worried about the endianness of the freelist. I'll have to
review the current code if it's endian-clean. Endian-agnostic
algorithms suck :(

I also found the memory leaks, they were missing ham_delete(db) calls
in the new unittests. i have fixed it, but not yet committed. But the
idea with a heap checker is cool.
(I also thought about replacing the malloc routines with a faster
algorithm, i.e. http://www.nedprod.com/programs/portable/nedmalloc/),
but that's for the future.

I wish you good luck for your meetings! I'll take care of the code
tonight and tomorrow, and i think i also will need the weekend time
till everything is polished and i can finally try to understand the
freelist problems...

Thanks again!
Christoph

2009/7/23 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Be warned: these fixes break binary compatibility with previous
> versions (see the ham_u32_t in freelist.h and a probable issue in
> btree as the offset from header to nodes isn't 11 but 12 bytes when
> one follows the (packed) structure organization.
>
> Crypto is completely BROKEN as this clashes very badly with the 'empty
> env stores settings to disc' fix in hamsterdb.c
>
> unittests which fail misserably have been disabled usin either
>  return; // hack
> or
>  #if 0 // hack
> lines.
>
> Be aware that the constants in a btree unittest (11+28 --> 12+28,
> which is correct as far as I can see) have been changed.
>
>
> What's troubling (and hopefully fixed now)?
>
> 1)
> create env
> close env
> open env
> --> all settings, such as max_dbs, etc. are LOST.
> This is fixed in hamsterdb.c by forcibly generating a header page when
> creating the ENV: a dummy DB is created, then deleted again; the side
> effect of that is that the header page remains. Unfortunately this fix
> clashes with the assumptions and checks done in the enable_crypto
> filter call set, resulting in access denied errors, while encryption
> is turned into a write-only effort. I have no idea how to fix it,
> apart from taking the empty-env fix apart and hand-crafting a custom
> header page writing piece of code.
>
> (By the way: env_open also accesses (opens) the database file to grab
> those constants, otherwise the parameters won't make it into the env
> structure; and the real env_open_db calls don't take care of it, while
> other APIs (get_parameters, etc.) need these config info bits too, so
> the sensible thing to do was add it to env_create and env_open.
>
>
> 2)
> while looking at freelist, I noticed a few weird things happening.
> See the freelist code, which is brimming with asserts; these asserts
> will FAIL on Win32/64 platforms as the pagesize is > 8224 bytes:
> that's the largest possible page when you don't want to lose space in
> the freelist as it's length (in bits) is a 16-bit word; the fix turns
> this into a 32-bit word, but that will BREAK the format backwards
> compatibility!
>
>
>
> 3)
> The dbname and other index header fields are now put in their own
> (packed) structure and use accessor macros like the rest of 'em: no
> more indexdata array read/write code. Purely cleanup.
>
>
>
> 4)
> BFC has been augmented to allow running not only single tests, but
> also sets of tests; very handy when developing and debugging /
> checking particular sections.
> On the commandline, things like this can be specified:
>
>  unittest fixture1::test1 fixture2complete ::testInAnyFxture
>
> where multiple entries in a single commandline will be simply executed
> in order of appearance.
>
> Ranges can be had by dropping in a literal star '*', e.g. execute all
> tests in all fixtures up to and INcluding fixtureA:
>
>  unittest '*' fixtureA
>
> execute all tests starting at fixA::test5 up to and including fixC::test7:
>
>  unittest fixA::test5 '*' fixC::test7
>
> and executing any test including and following fixA::test7
>
>  unittest fixA::test7 '*'
>
> these different range and single fixture / test instructions can be
> combined on a single command line, e.g.:
>
>  unittest '*' uptohere_fixC   fixD::test8 '*' all_of_fixE  ::any_test8
>
>
>
> 5)
> BFC can selectively catch C++ and hardware exceptions (the latter only
> on Windows); turn off to allow exceptions to propagate into the
> debugger, for instance. See main.cpp
>
>
> 6) Hacky: added Microsoft Heap Checking code to unittests/main.cpp --
> there are a couple of ham_db_t structures not released in the current
> unittest code, it seems.
>
>
> 7) Note that all those FFFFFFFF00000000 checking asserts in freelist
> were pure systemdependent hacks of mine and have gone.
>
>
> 8) Various other tidbits I need to rediscover by looking at the diff.
>
>
> 9) final note: the OFFSETOF() macro use is important as that doesn't
> break on systems where structs may be padded after all; the sizeof()-1
> hacks don't cut it then.
> I really hope I didn't screw it up, as some was rather hard to grok to
> a level where this kind of editing is doable.
>
> Note that a freelist on my Win32 system can store 500+K bits in a 64K
> page, so a sequential scan as it is, indeed needs looking at then. But
> I'm working on it; this transmission is so that we have a 'baseline'
> and a comparison point which is still somewhat easy on the eyes when
> comparing with vanilla 1.0.9 - I intend to augment the new freelist
> structures with at least a 'model' entity, which distates which
> freelist management system is being used for this env/db: that way,
> different methods can be deployed and tested. That's where both our
> mind-dumps from yesterday come in then.
>
>
>
> Anyway, new package coming your way.
>
> This is the preliminary before I dive into the freelist optimization;
> that will break binary compatibility with previous versions for sure.
>
> I am not able to find a solution for the broken crypto without major
> hacking of the empty-env fix; I didn't (yet) feel like dedicating
> enough time to write a custom header page write/read code there;
> besides, I wonder if that would save us from trouble, as the crypto
> still checks the page at setup time and performs a check - which, of
> course - would still fail, as the header page would ALWAYS have been
> crafted at a time BEFORE the crypto's been activated.
> The alternative is an API change, where the crypto is folded into the
> params, but then again, there's the whole business with the filters:
> given the crypto break, my guess is all filters are broken that way
> (zlib compression too).
>
> Pleas review what I did; I can answer questions tomorrow; weekend is
> questionable, but I'll see what I can do.
>
>
>
>
>
> I'm unavailable from now till tomorrow at least: this afternoon and
> this evening I have a couple of business meetings. Money to make,
> hands to grease. ;-)
>
>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 23
	
	
Reply
	
	Follow up message
On Thu, Jul 23, 2009 at 1:32 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger!
>
> Wow, you've been busy :) Thanks a lot.
>
> Regarding the env/headerpage issue: i also had the idea to create a
> dummy database. I also have an idea how to do this without the need to
> delete it again -> no wasted space, and it's faster because less I/O.
> I'll check your code changes and then will merge it with my idea.
>
> Regarding the freelist: i think it can be fixed without breaking
> binary compatibility. Your idea of the model would help: on old
> versions, the old model will be used, with new databases the new one.
> But nevertheless i will try to fix the old model (alternatively, i'll
> create an update mechanism, but i'm not sure if that's a good idea)...

Just popped back in at home; will go to bed in sec; was a busy day.

Anyway: about the binary backwards compatibility of the DB file: I had
thought about, but I didn't like coding it (forced backwards
compatible stuff is always a real dragon and the worst thing to
maintain. Anyway, here's the idea:

old format can store up to 64K bits, that's good enough for pages up
to 8224 bytes (including headers); assuming my calculations are
correct. Anyhow, 4K pages a la UNIX fit nicely.
How to recognize any 'new' format? Simple; the key though tis that the
_max_bits 16-bit word is always non-zero for any freelist (or so I
assume).
That measn that $0000 is a kind of 'magic value', and we can use that,
i.e. put it at that location, to signal Hamster it needs to switch to
the 'new format', which, of course, has a bigger header and entirely
different freelist layout.

Coding it implies a two-stage header loading process that way, which
is a bit of a slow-down, though very little.

Alternatively, we could bump de DB version number to make sure no old
software will accept the new DB format; then code Hamster so as to
check for 2 versions instead of 1 and accept both, setting a flag 'am
I old vx.x or am I new vy.y?' in the header, which must be tested
every time we load/save a freelist. The in-memory freelist entry
records can be changed without any harm, so that would take care of it
as well.


Now that I write these ideas down, I already like #2 better than #1,
because it offers a modicum of protection against hard-to-diagnose db
failures in the field.

The clean way is, indeed, providing an upgrade path, which means
dumping with the old hamster and loading the dumped DB with the new
Hamster. I'm sure people won't like that, even while it's the most
safe and maintainable way of doing this.


> Also, i'm worried about the endianness of the freelist. I'll have to
> review the current code if it's endian-clean. Endian-agnostic
> algorithms suck :(

Your bitaddressing is byte-based (pos DIV 8  and pos MOD 8); the
faster outer scan is just a 'is there ANY 1 bit in here' QWORD
oriented, but that outer loop doesn't mind what sort of endianess it's
fed, while the inner is byte-oriented so Endian-safe.

Good to mention Endian-safe here: I had forgotten about it while I've
been thinking a bit about faster freelist implementations.


> I also found the memory leaks, they were missing ham_delete(db) calls
> in the new unittests. i have fixed it, but not yet committed. But the
> idea with a heap checker is cool.

It's valgrind for MS, but with the drawback that non-instrumented
new/delete/malloc/free/... don't get their source location listed, so
all I could see were a series of 112 byte-sized chunks in the leak
dump; from experience thanks to debug sesseions, the 112 size was a
hint it were lingering db's, but there was no proof nor info where
they might originate. valgrind is far better in that regard.
With 2003 I had Numega (DevParner) tools (BoundsChecker and SoftICE)
which are the valgrind + kernel-level debugger for Win32.
Unfortunately, they froze the product: I haven't seen upgrades for
2005/2008. :-(


> (I also thought about replacing the malloc routines with a faster
> algorithm, i.e. http://www.nedprod.com/programs/portable/nedmalloc/),
> but that's for the future.
>
> I wish you good luck for your meetings! I'll take care of the code
> tonight and tomorrow, and i think i also will need the weekend time
> till everything is polished and i can finally try to understand the
> freelist problems...

Take your time; the meetings were a bit of a mixed success - it went
well, but the issues were a headcruncher, so I'm already pooped.
Besides, had a long night yesterday on Hamster, so I've reached my
quota for this week ;-)
Tomorrow, I need to do some accounting and billing and work on a Flash
mockup (or think of something similar to give the gentlemen I spoke
with tonight a bit of an idea what the technical issues and
possibilities are for their challenges. So I won't be working on
Hamster; it didn't b0rk and doesn't act weird any more in my own stock
data processing tools, so I'm really happy with that.)


Take care,

Ger




--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 25
	
	
Reply
	
	Follow up message
Hi Ger,

a very short update:

the Environment/Header page issue is now fixed. I changed your patch a
bit - i still create/open a dummy database, but with a reserved
database name. When this special database is created/opened, only the
header file is created/opened, and not the Btree root page.

All unittests are working fine and i even was able to remove some
duplicate code!

i will now continue with merging your other changes.

Best regards & have a nice evening!
Christoph

2009/7/23 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> On Thu, Jul 23, 2009 at 1:32 PM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger!
>>
>> Wow, you've been busy :) Thanks a lot.
>>
>> Regarding the env/headerpage issue: i also had the idea to create a
>> dummy database. I also have an idea how to do this without the need to
>> delete it again -> no wasted space, and it's faster because less I/O.
>> I'll check your code changes and then will merge it with my idea.
>>
>> Regarding the freelist: i think it can be fixed without breaking
>> binary compatibility. Your idea of the model would help: on old
>> versions, the old model will be used, with new databases the new one.
>> But nevertheless i will try to fix the old model (alternatively, i'll
>> create an update mechanism, but i'm not sure if that's a good idea)...
>
> Just popped back in at home; will go to bed in sec; was a busy day.
>
> Anyway: about the binary backwards compatibility of the DB file: I had
> thought about, but I didn't like coding it (forced backwards
> compatible stuff is always a real dragon and the worst thing to
> maintain. Anyway, here's the idea:
>
> old format can store up to 64K bits, that's good enough for pages up
> to 8224 bytes (including headers); assuming my calculations are
> correct. Anyhow, 4K pages a la UNIX fit nicely.
> How to recognize any 'new' format? Simple; the key though tis that the
> _max_bits 16-bit word is always non-zero for any freelist (or so I
> assume).
> That measn that $0000 is a kind of 'magic value', and we can use that,
> i.e. put it at that location, to signal Hamster it needs to switch to
> the 'new format', which, of course, has a bigger header and entirely
> different freelist layout.
>
> Coding it implies a two-stage header loading process that way, which
> is a bit of a slow-down, though very little.
>
> Alternatively, we could bump de DB version number to make sure no old
> software will accept the new DB format; then code Hamster so as to
> check for 2 versions instead of 1 and accept both, setting a flag 'am
> I old vx.x or am I new vy.y?' in the header, which must be tested
> every time we load/save a freelist. The in-memory freelist entry
> records can be changed without any harm, so that would take care of it
> as well.
>
>
> Now that I write these ideas down, I already like #2 better than #1,
> because it offers a modicum of protection against hard-to-diagnose db
> failures in the field.
>
> The clean way is, indeed, providing an upgrade path, which means
> dumping with the old hamster and loading the dumped DB with the new
> Hamster. I'm sure people won't like that, even while it's the most
> safe and maintainable way of doing this.
>
>
>> Also, i'm worried about the endianness of the freelist. I'll have to
>> review the current code if it's endian-clean. Endian-agnostic
>> algorithms suck :(
>
> Your bitaddressing is byte-based (pos DIV 8  and pos MOD 8); the
> faster outer scan is just a 'is there ANY 1 bit in here' QWORD
> oriented, but that outer loop doesn't mind what sort of endianess it's
> fed, while the inner is byte-oriented so Endian-safe.
>
> Good to mention Endian-safe here: I had forgotten about it while I've
> been thinking a bit about faster freelist implementations.
>
>
>> I also found the memory leaks, they were missing ham_delete(db) calls
>> in the new unittests. i have fixed it, but not yet committed. But the
>> idea with a heap checker is cool.
>
> It's valgrind for MS, but with the drawback that non-instrumented
> new/delete/malloc/free/... don't get their source location listed, so
> all I could see were a series of 112 byte-sized chunks in the leak
> dump; from experience thanks to debug sesseions, the 112 size was a
> hint it were lingering db's, but there was no proof nor info where
> they might originate. valgrind is far better in that regard.
> With 2003 I had Numega (DevParner) tools (BoundsChecker and SoftICE)
> which are the valgrind + kernel-level debugger for Win32.
> Unfortunately, they froze the product: I haven't seen upgrades for
> 2005/2008. :-(
>
>
>> (I also thought about replacing the malloc routines with a faster
>> algorithm, i.e. http://www.nedprod.com/programs/portable/nedmalloc/),
>> but that's for the future.
>>
>> I wish you good luck for your meetings! I'll take care of the code
>> tonight and tomorrow, and i think i also will need the weekend time
>> till everything is polished and i can finally try to understand the
>> freelist problems...
>
> Take your time; the meetings were a bit of a mixed success - it went
> well, but the issues were a headcruncher, so I'm already pooped.
> Besides, had a long night yesterday on Hamster, so I've reached my
> quota for this week ;-)
> Tomorrow, I need to do some accounting and billing and work on a Flash
> mockup (or think of something similar to give the gentlemen I spoke
> with tonight a bit of an idea what the technical issues and
> possibilities are for their challenges. So I won't be working on
> Hamster; it didn't b0rk and doesn't act weird any more in my own stock
> data processing tools, so I'm really happy with that.)
>
>
> Take care,
>
> Ger
>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 26
	
	
Reply
	
	Follow up message
Hi Ger,

another short update - i committed all your changes with the exception
of the freelist. Also, i have one unittest which now runs in an
endless loop - i want to fix this tomorrow.

Regarding the freelist i suggest that i wrap the freelist in function
pointers. For databases <= 1.0.9 we'll use the old code and for newer
databases the fixed code. Sounds ok?

Good night :)
Christoph

2009/7/23 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> On Thu, Jul 23, 2009 at 1:32 PM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger!
>>
>> Wow, you've been busy :) Thanks a lot.
>>
>> Regarding the env/headerpage issue: i also had the idea to create a
>> dummy database. I also have an idea how to do this without the need to
>> delete it again -> no wasted space, and it's faster because less I/O.
>> I'll check your code changes and then will merge it with my idea.
>>
>> Regarding the freelist: i think it can be fixed without breaking
>> binary compatibility. Your idea of the model would help: on old
>> versions, the old model will be used, with new databases the new one.
>> But nevertheless i will try to fix the old model (alternatively, i'll
>> create an update mechanism, but i'm not sure if that's a good idea)...
>
> Just popped back in at home; will go to bed in sec; was a busy day.
>
> Anyway: about the binary backwards compatibility of the DB file: I had
> thought about, but I didn't like coding it (forced backwards
> compatible stuff is always a real dragon and the worst thing to
> maintain. Anyway, here's the idea:
>
> old format can store up to 64K bits, that's good enough for pages up
> to 8224 bytes (including headers); assuming my calculations are
> correct. Anyhow, 4K pages a la UNIX fit nicely.
> How to recognize any 'new' format? Simple; the key though tis that the
> _max_bits 16-bit word is always non-zero for any freelist (or so I
> assume).
> That measn that $0000 is a kind of 'magic value', and we can use that,
> i.e. put it at that location, to signal Hamster it needs to switch to
> the 'new format', which, of course, has a bigger header and entirely
> different freelist layout.
>
> Coding it implies a two-stage header loading process that way, which
> is a bit of a slow-down, though very little.
>
> Alternatively, we could bump de DB version number to make sure no old
> software will accept the new DB format; then code Hamster so as to
> check for 2 versions instead of 1 and accept both, setting a flag 'am
> I old vx.x or am I new vy.y?' in the header, which must be tested
> every time we load/save a freelist. The in-memory freelist entry
> records can be changed without any harm, so that would take care of it
> as well.
>
>
> Now that I write these ideas down, I already like #2 better than #1,
> because it offers a modicum of protection against hard-to-diagnose db
> failures in the field.
>
> The clean way is, indeed, providing an upgrade path, which means
> dumping with the old hamster and loading the dumped DB with the new
> Hamster. I'm sure people won't like that, even while it's the most
> safe and maintainable way of doing this.
>
>
>> Also, i'm worried about the endianness of the freelist. I'll have to
>> review the current code if it's endian-clean. Endian-agnostic
>> algorithms suck :(
>
> Your bitaddressing is byte-based (pos DIV 8  and pos MOD 8); the
> faster outer scan is just a 'is there ANY 1 bit in here' QWORD
> oriented, but that outer loop doesn't mind what sort of endianess it's
> fed, while the inner is byte-oriented so Endian-safe.
>
> Good to mention Endian-safe here: I had forgotten about it while I've
> been thinking a bit about faster freelist implementations.
>
>
>> I also found the memory leaks, they were missing ham_delete(db) calls
>> in the new unittests. i have fixed it, but not yet committed. But the
>> idea with a heap checker is cool.
>
> It's valgrind for MS, but with the drawback that non-instrumented
> new/delete/malloc/free/... don't get their source location listed, so
> all I could see were a series of 112 byte-sized chunks in the leak
> dump; from experience thanks to debug sesseions, the 112 size was a
> hint it were lingering db's, but there was no proof nor info where
> they might originate. valgrind is far better in that regard.
> With 2003 I had Numega (DevParner) tools (BoundsChecker and SoftICE)
> which are the valgrind + kernel-level debugger for Win32.
> Unfortunately, they froze the product: I haven't seen upgrades for
> 2005/2008. :-(
>
>
>> (I also thought about replacing the malloc routines with a faster
>> algorithm, i.e. http://www.nedprod.com/programs/portable/nedmalloc/),
>> but that's for the future.
>>
>> I wish you good luck for your meetings! I'll take care of the code
>> tonight and tomorrow, and i think i also will need the weekend time
>> till everything is polished and i can finally try to understand the
>> freelist problems...
>
> Take your time; the meetings were a bit of a mixed success - it went
> well, but the issues were a headcruncher, so I'm already pooped.
> Besides, had a long night yesterday on Hamster, so I've reached my
> quota for this week ;-)
> Tomorrow, I need to do some accounting and billing and work on a Flash
> mockup (or think of something similar to give the gentlemen I spoke
> with tonight a bit of an idea what the technical issues and
> possibilities are for their challenges. So I won't be working on
> Hamster; it didn't b0rk and doesn't act weird any more in my own stock
> data processing tools, so I'm really happy with that.)
>
>
> Take care,
>
> Ger
>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 26
	
	
Reply
	
	Follow up message
Good to hear (almost) all is doing well.

Regarding freelist into function pointers: perfect idea; that was my
intent too (so we'd be able to switch freelist algorithms at run-time
and have their own header extensions; that means it's not just
function pointers (though that a good start) but also a little
tweaking and tugging at the freelist structure, so we can persist the
knowledge about which 'freelist algorithm' we're currently using, and
also store the extended freelist header that goes with that particular
algo.

A bit like you've done with the db structure (that would be the
abstract base class in C++) and the btree inplementation (which is
currently the only one), which comes with it's own structures, next to
the function pointers and common data members. (the derived class
providing the selected implementation).



Have not yet had time to look at the latest code; that will have to
wait until wednesday as I'm currently working on a prototype for a
business concept we've been discussing last thursday. Got a few ideas
I need to test and present on tuesday, so hamster is on the backburner
for a few days over here. Business before pleasure and all that ;-)


Take care and good morning to you,

Ger




On Sun, Jul 26, 2009 at 12:45 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> another short update - i committed all your changes with the exception
> of the freelist. Also, i have one unittest which now runs in an
> endless loop - i want to fix this tomorrow.
>
> Regarding the freelist i suggest that i wrap the freelist in function
> pointers. For databases <= 1.0.9 we'll use the old code and for newer
> databases the fixed code. Sounds ok?
>
> Good night :)
> Christoph

--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 27
	
	
Reply
	
	Follow up message
Hi Ger!

i'm happy to tell you that i finally committed all your changes!

(all but the freelist)

All unittests are running now, also those which were commented out and
those with encryption/compression.

Next thing i'll do is to create an abstraction layer for the freelist
and to load the current freelist code for everything <= 1.0.9. But not
tonight - tonight's our wedding day and my wife would kill me if i
spend more time in front of the computer :)

Anyway, i'm happy to hear that you're busy (business-wise) - that
means money for you and more time for me to implement this. :)
Anyway - if you want to start changing the freelist code then just
change it in freelist.h/freelist.c and i'll merge it when i''m ready
to.

There's one thing i have to look into - bfc currently crashes if an
exception is thrown (or a BFC_ASSERT fails) (gcc 4.1, linux 64bit).
I'll fix this asap.

But the next thing on my plate is to send you the commercial license.
i haven't forgotten about that!

Have a nice week
Christoph

2009/7/26 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Good to hear (almost) all is doing well.
>
> Regarding freelist into function pointers: perfect idea; that was my
> intent too (so we'd be able to switch freelist algorithms at run-time
> and have their own header extensions; that means it's not just
> function pointers (though that a good start) but also a little
> tweaking and tugging at the freelist structure, so we can persist the
> knowledge about which 'freelist algorithm' we're currently using, and
> also store the extended freelist header that goes with that particular
> algo.
>
> A bit like you've done with the db structure (that would be the
> abstract base class in C++) and the btree inplementation (which is
> currently the only one), which comes with it's own structures, next to
> the function pointers and common data members. (the derived class
> providing the selected implementation).
>
>
>
> Have not yet had time to look at the latest code; that will have to
> wait until wednesday as I'm currently working on a prototype for a
> business concept we've been discussing last thursday. Got a few ideas
> I need to test and present on tuesday, so hamster is on the backburner
> for a few days over here. Business before pleasure and all that ;-)
>
>
> Take care and good morning to you,
>
> Ger
>
>
>
>
> On Sun, Jul 26, 2009 at 12:45 AM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger,
>>
>> another short update - i committed all your changes with the exception
>> of the freelist. Also, i have one unittest which now runs in an
>> endless loop - i want to fix this tomorrow.
>>
>> Regarding the freelist i suggest that i wrap the freelist in function
>> pointers. For databases <= 1.0.9 we'll use the old code and for newer
>> databases the fixed code. Sounds ok?
>>
>> Good night :)
>> Christoph
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 27
	
	
Reply
	
	Follow up message
Congratulations to you and your wife regarding your wedding's anniversary. :-)

Also thanks for all the fixes; I'll check them out tomorrow; I need
all the time I've got to finish up the commercial work before
tomorrow.

When time allows, I'll try and run a few profiling runs to see where
the hotspots really are in the hamster code; that would help direct my
efforts before I go off and blindly take freelist apart.

Best wishes to your wife, and I hope you've had a lovely evening together.


Take care,

Ger




On Mon, Jul 27, 2009 at 3:01 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger!
>
> i'm happy to tell you that i finally committed all your changes!
>
> (all but the freelist)
>
> All unittests are running now, also those which were commented out and
> those with encryption/compression.
>
> Next thing i'll do is to create an abstraction layer for the freelist
> and to load the current freelist code for everything <= 1.0.9. But not
> tonight - tonight's our wedding day and my wife would kill me if i
> spend more time in front of the computer :)
>
> Anyway, i'm happy to hear that you're busy (business-wise) - that
> means money for you and more time for me to implement this. :)
> Anyway - if you want to start changing the freelist code then just
> change it in freelist.h/freelist.c and i'll merge it when i''m ready
> to.
>
> There's one thing i have to look into - bfc currently crashes if an
> exception is thrown (or a BFC_ASSERT fails) (gcc 4.1, linux 64bit).
> I'll fix this asap.
>
> But the next thing on my plate is to send you the commercial license.
> i haven't forgotten about that!
>
> Have a nice week
> Christoph



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 27
	
	
Reply
	
	Follow up message
One note (I had a quick glance; got sick of Flash coding so I played
truant for a short while there): in BFC, the FUT_invoker virtual
method in the testrunner class should _NOT_ try/catch any errors; the
bfc::error is the 'regular' exception type expected and is, as such,
caught in the outer call cpp_eh_run() -- see main.c

The idea is that you can turn exception catching on (default) or off -
the latter helps when you wish to run test cases in a debugger and
have the debugger catch those exceptions for you, including a full
stack trace and everything else you'd get when catching an unhandled
exception in a debugger.

(Side note: In MSVC, there's also the hardware exception catching
(__try/__catch) layer, which can catch pointer NULL dereferences, etc.
in the code, which is another thing you'd want a debugger to catch, so
that can be disabled too. Since C++ exceptions in MSVC2008 at least
travel through the RaiseException path, which will have them caught by
__try/__Catch as well, though with rather obscure exception codes, the
call sequence in BFC is very important. See below for the call chain
leading up to the actual function-under-test (FUT):)

call seqeunce from outer to inner, i.e. how a FUT is invoked by BFC:

BFC framework (run(...)) calls:
-->
run(fixture, test) -- which calls setup, FUT and cleanup in that
order, and each of them through the exception-catching methods below
-->
bfc::exec_testfun() -- which is currently only relevant for MSVC as it
catches hardware exceptions, and hence a simple pass-through on UNIX,
until we implement some sort of signal(SIGSEGV), etc. scheme in
there...
-->
bfc::cpp_eh_run() -- which catches any exception thrown by the
BFC_ASSERT*** macro's, i.e. it will catch all bfc::error exceptions
-->
fixture::FUT_invoker()
-->
fixture::FUT() == function_under_test. This is any of the
functions/methods, which contain all the BFC_ASSERT*** tests.

The indirection through the virtual fixture::FUT_invoker() is ONLY
there to allow test code to be simplified when used in a setting where
the tested code can throw its own exceptions -- which is the case when
we test the C++ wrapper class for HamsterDB as it will throw
exceptions on almost all error conditions. Hence, you'll find an
OVERRIDEN FUT_invoker() method for that fixture: see cppapi.cpp
Also note that smart, custom fixture::FUT_invoker()s like that
'inherit' the logic which checks if the BFC user has turned on
exception handling fall-through (to aid debugging).
The _default_ fixture::FUT_invoker() is really a no-op, which only
invokes the function-under-test. It's there because this was easier
than having a nil base virtual method, as you can't have those in
non-abstract base classes in C++.


For completeness of 'documentation' regarding this scheme, it should
be noted that ANY custom, i.e. overriding fixture::FUT_invoker()
should catch all C++ exceptions _except_ the regular bfc::error()
exception (as that one is caught in cpp_eh_run() ) and when it catches
an exception, such exceptions should be transformed into bfc::error()
exceptions, either by encoding it as a bfc::error instance and
throw()-ing it, or by returning it in the bfc::error() instance
parameter and returning 'true' to signal the callers a failure was
caught and the bfc::error() parameter has been initialised
accordingly.



The enabling (yes/no) of the exception catching / passthrough is not
new; I did this after reading the feature list of other unit test
frameworks, such as cppunit. I don't recall which of them offered this
feature -- had a look at a few wegsites there --, but one of them did
and included the explanation why it was useful (debugging assistance)
-- which I agreed with instantly. Hence this augmentation of BFC: the
idea is ripped; the implementation is new (done from scratch, without
looking how the others did it).

This whole business was done this way so I can re-use BFC for
unit-testing my own exception-throwing C++ class hierarchies. I know
there's a few ('one .H file only') unittest frameworks out there that
got my interest, but since I've been working with BFC, I like it. Now
that it also features the ability to test any subset/subrange (or set
thereof) of the tests collective, I'm quite satisfied with it's
feature set. All it needs (though rather more for completeness sake)
is segmentation fault catching on UNIX platforms that allow catching
such failures within the application itself (signal(SIGSEGV) or
sigaction(SIGSEGV), etc.)
- Show quoted text -




































































\section HamsterDB - now with an improved, yet old-sk00l compatible, freelist management - orders of magnitude speedup


Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 31
	
	
Reply
	
	Follow up message
Bragging? You bet. But then this beast got an overhaul that hasn't
seen it's equal in a long time.

First the bad news: this is work in progress; the DAM handling code is
still buggy as Hell: luck has it that all the current DAM modes are
close to 1.0.x compatible - the only real bug there is that the
_mgt_mode is written to the DB file, thus creating a non-compatible DB
word value in the header as that value should have remained ZERO.
Alas, something to debug in the next few days.

However, that's it for the bad news. The good news is:

freelist (and company) now uses a, in increasing order of importance:

- a slightly changed cache aging system: no more fixed value setting
(1000/50/10/20) but adding as page accesses accumulate - though with a
pagetype-specific upper bounds. See the source files db.c, page.c,
etc. and the code changes and comments there. THIS HAS NO RELEVANCE
FOR THE FREELIST SPEEDUP

--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 31
	
	
Reply
	
	Follow up message
!@#$%T^ gmail and the ENTER key. ~!@#$%^&

anyway, continuing below....

******* WARNING: code tree does not pass the unittests
(MarkMallocsomething) on my UNIX dev box; treat with care (i.e. just
have a look-see but do NOT merge yet)
However, running
 ./test HamsterdbTest
to see the nearFind stress test speed up works fine overhere :-)


On Fri, Jul 31, 2009 at 6:58 PM, Ger Hobbelt<ger@hobbelt.com> wrote:
> Bragging? You bet. But then this beast got an overhaul that hasn't
> seen it's equal in a long time.
>
> First the bad news: this is work in progress; the DAM handling code is
> still buggy as Hell: luck has it that all the current DAM modes are
> close to 1.0.x compatible - the only real bug there is that the
> _mgt_mode is written to the DB file, thus creating a non-compatible DB
> word value in the header as that value should have remained ZERO.
> Alas, something to debug in the next few days.
>
> However, that's it for the bad news. The good news is:
>
> freelist (and company) now uses a, in increasing order of importance:
>
> - a slightly changed cache aging system: no more fixed value setting
> (1000/50/10/20) but adding as page accesses accumulate - though with a
> pagetype-specific upper bounds. See the source files db.c, page.c,
> etc. and the code changes and comments there. THIS HAS NO RELEVANCE
> FOR THE FREELIST SPEEDUP

- freelist now does use improved bitarray scanning: see the (huge)
__freel_search_ex function (which should optimize nicely by your
compiler; there's a reason this beast is one call) inspired on
Boyer-Moore-Sunday-Hume string pattern searching/matching, applied to
a domain with an alphabet of size = 2 (bit-level: {0,1}, and when
expanded to byte or qword range, the alphabet is 2.5 characters wide:
{all-0s, all-1s} and under particular edge conditions, there's the
'some-1s' character too. Anyway, the major free slot scanner has been
augmented to employ a custom-tailored BM scanner, which operates on
qwords, bytes and/or individual bits.

This is a MAJOR speed improvement.


- freelist / HamsterDB now gathers insert/delete/expand
freelist-related statistics, which help the search routine to better
dimension the actual search range: see the get_hints code and the
surrounding block alloc routine, where the search is further reduced
to selected pages, depending on the current data access mode.

This is another MAJOR speed improvement: in particular do the
statistics help to cut down significantly on the upper edge of the
search range: freelist pages store a huge number of bits, few of which
have been allocated most of the time. The original old-sk00l code did
not take this into account, but the new statistics gatherer and search
hinter monitors which bits in the freelist array actually get
allocated and which are not (yet): this cuts worst-case scan cases
from scanning over near to 64K bits to fewer than 1K bits per page,
and often even less than that as the statistics gatherer also keeps
track of the approximate amount of free slots at the top of the page -
a datum which is used to good effect by the FAST data access modes to
further speed up the search for empty slots by discarding pages (and
their scanning) when the statistics hint that there might not be
enough allocated free slot bits available in the given page after all.



Code-wise, the major effort is in the freelist _freel_search_ex
function, which is quite large, carrying all the various alternative
setup and scan subsystems.
Maybe I'll refactor it later on, but given its purpose and its
desperate need for speed, I currently feel like keeping it as it is.
Note that the data access modes combined with the various size ranges
we can search for result in
(a) slightly different startup, where the major chaoices are between
the usual fast-BM linear skip loop (classic) or a approximate
binary-search style BM-scan (note that the bsearch is non-trivial as
the probe step is the same as for TBM: the length of the searched
pattern, expressed in qwords/bytes/bits. However, this setup mix is
the mandatory prelude to a semi-classic TBM-like (Tuned Boyer Moore)
main scanner loop, though this time around it comes in three flavours,
depending on the width of the search pattern (size_bits): a
qword-based scanner, a byte-scanner and a bit-scanner. Where the first
two are really each two scanners: a byte/qword-level scanner for the
search of the midsection of the pattern, while the bit-sized edges are
checked using bit-level rev/fwd partial traversals (inspired on the
descriptions in the Sunday/Hume paper (attached).

Anyway, run-time speed says it all: switch between the
aligned/non-aligned old search functions and the new search_ex and see
the difference. On my Win32 machine, it's the difference between about
10 seconds and several minutes for my own custom HamsterDB using tool.
In the nearFind unittest test, the sparser database generated using
the fast setting still does not bloat much, most probably because the
inserts are, indeed, entirely sequential and no records are erased
there.



Anyway, keep apart for a little while; tonight I'm gone, acting social
and all that ;-)) and helping somebody else, so tomorrow evening is
the earliest time I can attend the issues still lingering in there.
Now, insert is faster than [near]find. Bugger!  ;-(
- Show quoted text -




--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
4 attachments  Download all attachments  
Fast String Searching - Hume and Sunday, 1991 - spe063ah.pdf	Fast String Searching - Hume and Sunday, 1991 - spe063ah.pdf
205K   View   Download  
Boyer-Moore algorithm - fstrpos.pdf	Boyer-Moore algorithm - fstrpos.pdf
1164K   View   Download  
String Searching over Small Alphabets - sustik-moore.pdf	String Searching over Small Alphabets - sustik-moore.pdf
64K   View   Download  
hamsterdb-1.0.9.20090730-001.tar.gz	hamsterdb-1.0.9.20090730-001.tar.gz
1383K   Download   



















































\section hamster - fixes for latest CVS and my source tree (backwards compatibility issues)
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 28
	
	
Reply
	
	Follow up message
Fixes (and an addition - hope you like the design there ;-) ):

1)
Note that I screwed up in the index structure definition: maxkeys is
16-bit but the byte offset for the next field is +4, so it should have
read in db.h:

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\src\db.h   2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\src\db.h 2009-07-28
16:31:57.000000000 +-0200
@@ -179,12 +206,15 @@

               /* maximum keys in an internal page */
               ham_u16_t _maxkeys;

        /* key size in this page */
               ham_u16_t _keysize;
+
+        /* reserved in 1.0.x up to 1.0.9 */
+               ham_u16_t _reserved;

               /** address of this page */
               ham_offset_t _self;

        /* flags for this database */
               ham_u32_t _flags;
---

Added 'feature' (which is currently a no-op, but this is the starting
shot for the freelist stuff): max_databases is assumed to top out at
0xF000 and can thus safely be assumed to be a 16-bit int; the db
header reserves 32 bits for it though, so we can re-use the MSW of
that (which is 0x0000 always!) as the spot to store 'expected data
access mode', which is a bitmixed flagfield (whatever ;-) -- see
hamsterdb.h for the mockup: HAM_DAM_* constants) which tells hamster
how to treat freelists, etc.etc.
For backwards compatibility, keep this set to zero ('classic' mode).

This new stuff to be is called 'data_access_mode' something-or-other
throughout the code (quick grab of local source tree included)



2)

Another fix: this is VERY probably what made your BFC crash in Linux64
as it ddid on my Win64: argv[] indexed beyond array bounds. Fixed in
main.cpp:

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\unittests\main.cpp 2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\unittests\main.cpp       2009-07-28
16:46:32.000000000 +-0200
@@ -259,8 +273,8 @@
-                               if (!lead
-                                       && (i+1 == argc || std::string(argv[i+1]) != "*"))
+                               if (!lead && (i < argc)
+                                       && (i+1 >= argc || std::string(argv[i+1]) != "*"))
                               {
                                       // single case:
                                       r = testrunner::get_instance()->run(
                                                       fixture_name.c_str(), test_name.c_str());
                                       inclusive_begin = true;
                               }
---



3)
preprocessor woes in main.cpp:

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\unittests\main.cpp 2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\unittests\main.cpp       2009-07-28
16:46:32.000000000 +-0200
@@ -43,7 +43,7 @@

-#if (defined(WIN32) || defined(_WIN32) || defined(_WIN64) || defined(WIN64))
+#if (defined(WIN32) || defined(_WIN32) || defined(_WIN64) || defined(WIN64)) \
        && defined(_DEBUG)

 _CrtMemState crm_memdbg_state_snapshot1;
 int trigger_memdump = 0;
 int trigger_debugger = 0;
---

and

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\unittests\main.cpp 2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\unittests\main.cpp       2009-07-28
16:46:32.000000000 +-0200
@@ -134,7 +148,8 @@
-#if (defined(WIN32) || defined(_WIN32) || defined(_WIN64) || defined(WIN64))
+
+#if (defined(WIN32) || defined(_WIN32) || defined(_WIN64) || defined(WIN64)) \
        && defined(_DEBUG)
    /*
     * Hook in our client-defined reporting function.
     * Every time a _CrtDbgReport is called to generate
     * a debug report, our function will get called first.
     */
---




4)

See previous email of mine (different subject line): this must be
fixed for BFC to work as expected (described there; exceptions are
caught at other call levels; this is just a base dummy FUT invoker):

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\unittests\bfc-testsuite.hpp        2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\unittests\bfc-testsuite.hpp      2009-07-23
12:37:30.000000000 +-0200
@@ -181,11 +181,5 @@
       virtual bool FUT_invoker(testrunner *me, method m, const char
*funcname, error &ex)
       {
-        try {
-                   (this->*m)();
-        }
-        catch (bfc::error &e) {
-                       ex = e;
-            return (true);
-        }
-               return (false);
+               (this->*m)();
+               return false;
       }
---




5)
And this is entirely superfluous but it's now there for completeness
sake; so nobody ever will get a chance to doubt what is caught here:

---
--- \\Debbie\ger\prj\1original\hamsterdb\trunk\trunk\unittests\main.cpp 2009-07-27
20:41:39.000000000 +-0200
+++ \\Debbie\ger\prj\3actual\hamsterdb\unittests\main.cpp       2009-07-28
16:46:32.000000000 +-0200
@@ -334,17 +348,17 @@
       if (me->m_catch_exceptions || me->m_catch_coredumps)
       {
               try
               {
                       return f->FUT_invoker(me, m, funcname, ex);
               }
-               catch (error &e)
+               catch (bfc::error &e)
               {
                       ex = e;
                       return true;
               }
       }
       else
       {
               return f->FUT_invoker(me, m, funcname, ex);
       }
 }
---




6)
meanwhile, blob.cpp has been extended to test hamster forcibly with
64K pages a la Win32/Win64 on ANY platform, including UNIX (@ page =
4K). I have NOT tested cross-platform portability yet, except simply
running the few BFC tests that already do this through pregenerated
DBs.

However, given what I've run into up to now, I would only 'buy' that
feature's validity when thoroughly tested with a DB carrying about 1M
records (1 ham_u32_t as key, and another ham_u32_t as record data
would be okay). And not just find(), but also create, insert and
delete (and thus: byte comparison of two files: the master and the one
just generated; since the whole process is entirely deterministic and
does not store timestamps in headers, the DBs should be byte-for-byte
identical. When time allows, I'll code such a set of BFC tests.

This 'doubt' is mostly due to the latest corrections/alterations in
the offset calculation #defines: a few of them were off by 1 (or 2)
bytes and the risk may still exist that large, multi-page spanning DBs
created by 1.0.8 or older, will trigger conditions which are not hit
with the current (smaller) BFC test DBs.




7)
freelist has been 'objectified': the freelist code has been edited to
allow us to assign a different freelist handling method set once those
new 'modern' data_access_modes get implemented. For now, freelist.c
has had a few asserts commented out which would trigger for 64K pages
in backwards compatibility mode (those asserts were not there in
vanilla 1.0.9 release and only trigger when you feed this animal
pagesizes > 8K, i.e. Win32/64 default settings. Without the asserts,
no /direct/ damage is done, but you lose a large chunk of the page
space for the freelist.

The idea here is that the freelist 'algorithm', i.e. the appropriate
'derived class instance' which will handle the freelist for us, is
derived from the 'data access mode' setting; the user hence controls
the freelist way of working /indirectly/. I think the mechanism, as
laid out, enables us to implement each and every idea we listed for
the freelist; if we add more freelist 'approaches', it's simply a
matter of introducing additional Data Access Modes and we're good to
go; while these Data Access Modes can also be used to 'tweak' other
parts of the Hamster mechanism (such as the Btree).




8)
fputs --> fwrite in MSVC code to prevent stack overflow due to
recursive calls to this debug reporting routine when a heap corruption
occurs. :-(




9)
freelist: the (ham_u16_t) casts are MANDATORY, especially near the
freel_entry_set_* macro's as the in-memory (not persisted)
freelist_entry_t uses the ham_size_t type for both max_bits and
allocated_bits (so we can use the same structure for >8K pages);
asserts that trigger on overflow have been kept enabled as much as
possible: the 'FORCE_UNITTEST_PASS' disabled ones had to go to pass
the unittests (while these asserts raise valid concerns); any user who
uses pagesizes beyond 64K (e.g. me ;-) ) will suffer from the
remaining asserts triggering then: I consider that a good thing as
these folks will thus, though indirectly, become aware of the 16-bit
size limit issue in the freelist implementation as it exists till
today. That's why I didn't kick out all the size asserts.

[***edit***] The HamsterdbTest tests already the other asserts in
freelist as well, thanks to larger pagesizes used in there (my own
damn work ;-) ) so 'FORCE_UNITTEST_PASS' now spans a few more
asserts...





10)
open issue: on Win32/64 the current code tolerates two writing
applications to access a DB file simultaneously, as it would on Linux.
However... As far as I can see, having two asynchronous writers is
dangerous, as the transaction mechanism does not protect us from the
scenario where both A and B asynchronously flush their pages at the
end of a transaction each, or when A and B write transactions overlap,
which would result in losing the edits from at least one of them as
pages are overwritten with partially outdated data -- after all, there
is no interlocking spanning multiple (local or remote) processes. As
such, I preferred the behaviour (now commented out) the latest
Win32/64 code exhibited: one writer, multiple read-onlys, no more.
EXCLUSIVE locking is even more rigorous than that: that would mean
*nobody* would be allowed near the file while the current binary
accesses it.

A better way to serve those who need this kind of feature
(simultaneous DB write access) is the deamon idea; just like
professional SQL DBs, which thus sequence parallel incoming requests
(though the 'sequencing' there is rather sophisticated, given load
sharing db server nodes and all. Ah well, I digress.)

My pointhere: "Simultananeous Asynchronous Writers Considered Harmful"
(bloody harmful, really)





--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 28
	
	
Reply
	
	Follow up message
Attachment: source tree as 7zip archive


On Tue, Jul 28, 2009 at 7:50 PM, Ger Hobbelt<ger@hobbelt.com> wrote:
> Fixes (and an addition - hope you like the design there ;-) ):



Tested on Ubuntu AMD64: crashes in RecnoTest:endianTestOpenDatabase
--> Ubuntu / GCC 4.3.2-1ubuntu12 apparently doesn't pack the data
structs like MSVC and it does not produce code which can process older
DBs, i.e. not backwards compatible (yet). The packing is just my
guess; I haven't been able to get Kdbg to eat unittest (lots of crap
being uttered about the hamsterdb.so and I got sick of it; any hints
regarding this are much appreciated - debugging code with dynamic
loadable libs in Linux is not something I do usually).


Tested on Win32, MSVC2008: all pass. (one memleak reported about the
filename being allocated into the ENV structure, but I can't see where
this would have gone wrong in the tests.)



Tested on Win64 (XP64), MSVC2008, all pass OK. (and the one memleak:
hamsterdb.c @ line 1043    :-S  )
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb.snap20090728.001.7z	hamsterdb.snap20090728.001.7z
2174K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 29
	
	
Reply
	
	Follow up message
Hi Ger,

just saw your mail - thanks a lot for the changes and explanations!

I'll merge them as soon as i can, and i will take care of the
recno-problem. I actually had something similar a few days ago
(RecnoTest::endianXXX failed). As far as i remember the problem was
the db_header_t structure, it had a inner union which also was packed.
I removed the packing macros from the inner union and then it worked.

I'll get back to you as soon as i have some results!

have a nice day,
Christoph

2009/7/28 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Attachment: source tree as 7zip archive
>
>
> On Tue, Jul 28, 2009 at 7:50 PM, Ger Hobbelt<ger@hobbelt.com> wrote:
>> Fixes (and an addition - hope you like the design there ;-) ):
>
>
>
> Tested on Ubuntu AMD64: crashes in RecnoTest:endianTestOpenDatabase
> --> Ubuntu / GCC 4.3.2-1ubuntu12 apparently doesn't pack the data
> structs like MSVC and it does not produce code which can process older
> DBs, i.e. not backwards compatible (yet). The packing is just my
> guess; I haven't been able to get Kdbg to eat unittest (lots of crap
> being uttered about the hamsterdb.so and I got sick of it; any hints
> regarding this are much appreciated - debugging code with dynamic
> loadable libs in Linux is not something I do usually).
>
>
> Tested on Win32, MSVC2008: all pass. (one memleak reported about the
> filename being allocated into the ENV structure, but I can't see where
> this would have gone wrong in the tests.)
>
>
>
> Tested on Win64 (XP64), MSVC2008, all pass OK. (and the one memleak:
> hamsterdb.c @ line 1043    :-S  )
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 29
	
	
Reply
	
	Follow up message
Coming up next on this channel: BFC revisited: hardware failures +
extended test exception handling for UNIX. --> BFC kernel overhauled
and split into hpp + cpp for compile-time speedup.


> recno-problem. I actually had something similar a few days ago
> (RecnoTest::endianXXX failed). As far as i remember the problem was
> the db_header_t structure, it had a inner union which also was packed.
> I removed the packing macros from the inner union and then it worked.

Hm... hadn't thought of that one!
I'll see if I can get the error to go away; this might have something
to do with the different packing on 64- and 32-bit compiler platforms
(as the packing macro's had been applied 100% consistently throughout,
there's a big chance the old code had it look like there was packing
applied, but not in actuality as the macros were misplaced (those
pack_0/1/2 #defines, I mean).


--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 29
	
	
Reply
	
	Follow up message
Ubuntu/AMD64: tested:
----- error #1 in RecNoTest::endianTestOpenDatabase
hamster_fixture.hpp:70 ASSERT FAILED in file hamsterdb.c, line 3147:
       "st!=HAM_DUPLICATE_KEY"
duplicate key in recno db!
-----------------------------------------
total: 1 errors, 605 tests

No errors on Win64/Win32


Changes/Fixes:

* all BFC fixtures now use the same pattern -- see also
empty_sample.cpp -- no more
   if (name) return;
but properly using the existing method: clear_tests()


* augmented BFC: now catches and reports hardware exceptions (division
by zero, invalid pointers, floating point errors, etc.) just like the
MSVC code already did for a while (this is done through signal()s) --
as the hpp file was growing to an insane size, the implementation has
been split off into bfc-testsuite.cpp, while bfc-testsuite.hpp still
contains all the macros, classes, etc. so nothing is changed in usage:
include that header file and you're good to go.

Note (see also the hpp and other files in the set) that a special 'C'
file is required to make this work on UNIX: bfc_signal.c --> see
unittests/Makefile.am for the new files list in there (and autoconf
says CPPFLAGS should be removed in there; this may depend on the
autoconf version, though :-S (I'm running the latest here))



* BFC fixtures have been checked: some had not been registered
correctly and thus were missing from the unittests run.




* couple of fixes for when compiling this beast without
--enable-internal (I was wondering why my UNIX runs delivered
different error sets than my Win32/64 runs and that was why).




* added extra BFC checks which check the exact size of various packed
structs and related offsets and lengths -- these tests now finally
produce identical results on Win32/64 vs. UNIX: it turns out MSVC
packs /everything/ inside a union or struct when the outermost
struct/union has the pack0/1/2 macros applied, while GCC 4.x does NOT
do this: each and every level of struct and union must be soaked in
those pack0/1/2 macros or you'll have unpacked sections within packed
ones!

I fear this artifact doesn't really help with 'backwards
compatibility' as those older DBs will have been created using hamster
sources which didn't have /all/ of these pack instructions in all the
right places (and applied to each level of nested struct/union) so
YMMV. How one should resolve this, I do not know: without an
export/import cycle, and a desire for 100% backwards compat, there's
several issues then:
- Win32 64K pages and their 'sparse' use by btree and freelist
- I recall one or two offset calculations in there were 'off by one' before
- Win64 default packing is different from Win32 and from gcc for
platform xyz: as some structs/unions were lacking some of those
pack0/1/2 macros, they were basically UNpacked. To 'mimic' that, the
'best' approach would be to 'pad' the fully packed structs/unions,
where applicable, but this would still be platform-dependent, given
the various 'default packings'.

The alternative is list this as a bug, and list the resolution as
export+import. Not funny.




* couple of fixes regarding those pack0/1/2 macro applications in
hamsterDB proper. (blob)



* introduction of the 'data access mode', which currently only
supports 'classic', i.e. backward compatibility mode. Anyhow, this
mode parameter can be used to switch to alternative freelist
processing systems and/or adjust other parts of the hamsterDB core for
maximized performance. The current code is just a drop-in replacement
of the old; no new 'modes' have been added yet. The DAM (Data Access
Mode) can be persisted without altering the DB format with respect to
the DB header -- see the relevant header file.



* other tidbits I already forgot about....



attachment: new source archive. The first one created with
  make dist
thanks to adjusted Makefile.am files. (uses latest automake/autoconf/...)
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.200907292333.tar.gz	hamsterdb-1.0.9.200907292333.tar.gz
1333K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 29
	
	
Reply
	
	Follow up message
> recno-problem. I actually had something similar a few days ago
> (RecnoTest::endianXXX failed). As far as i remember the problem was
> the db_header_t structure, it had a inner union which also was packed.
> I removed the packing macros from the inner union and then it worked.

Forgot to mention just now: I made sure *all* (nested) persisted
structs/unions have those pack0/1/2 macros in the right places (I
still may have missed a struct, but I don't think I did this time
around). I believe that is important to have as a baseline; when
things don't 'work' out well regarding loading/accessing older DBs
(where you say you needed to remove a couple of those pack0/1/2's), my
take is that it's okay to do that (removing) for now to get the code
into a state where your own test set passed 100%, but once we're
there, we need to investigate what alignment is actually employed by
gcc and reintroduce all the removed pack0/1/2's, while 'padding' the
those structs to ensure the actual layout remains as it was before
(when it was essentially UNpacked).

That the code needed this means that those old DBs won't be portable
between platforms anyhow, or better put like this: we will be binary
backwards compatible after the padding job, but only with the selected
platform (x86/Linux/gcc 3.?/4.? ???) and Win32/64 users will need to
export+import while upgrading, just to be on the safe side.



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 30
	
	
Reply
	
	Follow up message
Hi Ger,

thanks again for the big load of changes - i'll start merging them today!

I'll keep you updated.

Have a nice day!
Christoph

2009/7/29 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
>> recno-problem. I actually had something similar a few days ago
>> (RecnoTest::endianXXX failed). As far as i remember the problem was
>> the db_header_t structure, it had a inner union which also was packed.
>> I removed the packing macros from the inner union and then it worked.
>
> Forgot to mention just now: I made sure *all* (nested) persisted
> structs/unions have those pack0/1/2 macros in the right places (I
> still may have missed a struct, but I don't think I did this time
> around). I believe that is important to have as a baseline; when
> things don't 'work' out well regarding loading/accessing older DBs
> (where you say you needed to remove a couple of those pack0/1/2's), my
> take is that it's okay to do that (removing) for now to get the code
> into a state where your own test set passed 100%, but once we're
> there, we need to investigate what alignment is actually employed by
> gcc and reintroduce all the removed pack0/1/2's, while 'padding' the
> those structs to ensure the actual layout remains as it was before
> (when it was essentially UNpacked).
>
> That the code needed this means that those old DBs won't be portable
> between platforms anyhow, or better put like this: we will be binary
> backwards compatible after the padding job, but only with the selected
> platform (x86/Linux/gcc 3.?/4.? ???) and Win32/64 users will need to
> export+import while upgrading, just to be on the safe side.
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 31
	
	
Reply
	
	Follow up message
Hi Ger!

it's a shame but i'm still not done with merging... we have some
troubles here at work - my employer was bought and now we're merging
with other departments and everything is restructured. These are
interesting times...

Regarding DAM - it's a good idea! Just one issue - right now,
HAM_DAM_CLASSIC (0) is for <= 1.0.9, and it shouldn't be used anymore
because it's buggy, right? so the new default (set in
hamsterdb.c/_check_create_parameters) should be
HAM_DAM_RANDOM_WRITE_ACCESS, i think.

I already merged the blob.c/h and the packing things yesterday, and
today i'll merge the remaining few files, the header file and the
unittests.

Have a nice day,
Christoph

2009/7/30 Christoph Rupp <chris@crupp.de>:
- Show quoted text -
> Hi Ger,
>
> thanks again for the big load of changes - i'll start merging them today!
>
> I'll keep you updated.
>
> Have a nice day!
> Christoph
>
> 2009/7/29 Ger Hobbelt <ger@hobbelt.com>:
>>> recno-problem. I actually had something similar a few days ago
>>> (RecnoTest::endianXXX failed). As far as i remember the problem was
>>> the db_header_t structure, it had a inner union which also was packed.
>>> I removed the packing macros from the inner union and then it worked.
>>
>> Forgot to mention just now: I made sure *all* (nested) persisted
>> structs/unions have those pack0/1/2 macros in the right places (I
>> still may have missed a struct, but I don't think I did this time
>> around). I believe that is important to have as a baseline; when
>> things don't 'work' out well regarding loading/accessing older DBs
>> (where you say you needed to remove a couple of those pack0/1/2's), my
>> take is that it's okay to do that (removing) for now to get the code
>> into a state where your own test set passed 100%, but once we're
>> there, we need to investigate what alignment is actually employed by
>> gcc and reintroduce all the removed pack0/1/2's, while 'padding' the
>> those structs to ensure the actual layout remains as it was before
>> (when it was essentially UNpacked).
>>
>> That the code needed this means that those old DBs won't be portable
>> between platforms anyhow, or better put like this: we will be binary
>> backwards compatible after the padding job, but only with the selected
>> platform (x86/Linux/gcc 3.?/4.? ???) and Win32/64 users will need to
>> export+import while upgrading, just to be on the safe side.
>>
>>
>>
>> --
>> Met vriendelijke groeten / Best regards,
>>
>> Ger Hobbelt
>>
>> --------------------------------------------------
>> web:    http://www.hobbelt.com/
>>        http://www.hebbut.net/
>> mail:   ger@hobbelt.com
>> mobile: +31-6-11 120 978
>> --------------------------------------------------
>>
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 31
	
	
Reply
	
	Follow up message
Buy-out.... indeed, interesting times. Well, good luck there.

Regarding DAM mode 0: yes, it's buggy, but for now it's still the only
mode available.
As I want o make sure the backwards compatibility stuff is nailed down
before I move into the new DAM modes, all current development is still
done in DAM #0: the freelist has seen a major overhaul in the
freel_search() department, but not much gain yet -- we're currently
profiling that bunch and the new code has a very nice hotspot, thanks
to a lot of tiny blocks being allocated in the unittests -- a
representative example, I'd say.

Anyway, it now features Boyer-Moore-inspired searching and a couple of
speedup features which are certain to appear also in DAM mode 1+ --
while these optimizations can also be used safely in DAM #0 backwards
compat mode :-)))

When I get my order-of-magnitude speedup nailed, you'll see another
freelist appear.

Take care and enjoy the day,

Ger
- Show quoted text -




On Fri, Jul 31, 2009 at 7:51 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger!
>
> it's a shame but i'm still not done with merging... we have some
> troubles here at work - my employer was bought and now we're merging
> with other departments and everything is restructured. These are
> interesting times...
>
> Regarding DAM - it's a good idea! Just one issue - right now,
> HAM_DAM_CLASSIC (0) is for <= 1.0.9, and it shouldn't be used anymore
> because it's buggy, right? so the new default (set in
> hamsterdb.c/_check_create_parameters) should be
> HAM_DAM_RANDOM_WRITE_ACCESS, i think.
>
> I already merged the blob.c/h and the packing things yesterday, and
> today i'll merge the remaining few files, the header file and the
> unittests.
>
> Have a nice day,
> Christoph
>
> 2009/7/30 Christoph Rupp <chris@crupp.de>:
>> Hi Ger,
>>
>> thanks again for the big load of changes - i'll start merging them today!
>>
>> I'll keep you updated.
>>
>> Have a nice day!
>> Christoph
>>
>> 2009/7/29 Ger Hobbelt <ger@hobbelt.com>:
>>>> recno-problem. I actually had something similar a few days ago
>>>> (RecnoTest::endianXXX failed). As far as i remember the problem was
>>>> the db_header_t structure, it had a inner union which also was packed.
>>>> I removed the packing macros from the inner union and then it worked.
>>>
>>> Forgot to mention just now: I made sure *all* (nested) persisted
>>> structs/unions have those pack0/1/2 macros in the right places (I
>>> still may have missed a struct, but I don't think I did this time
>>> around). I believe that is important to have as a baseline; when
>>> things don't 'work' out well regarding loading/accessing older DBs
>>> (where you say you needed to remove a couple of those pack0/1/2's), my
>>> take is that it's okay to do that (removing) for now to get the code
>>> into a state where your own test set passed 100%, but once we're
>>> there, we need to investigate what alignment is actually employed by
>>> gcc and reintroduce all the removed pack0/1/2's, while 'padding' the
>>> those structs to ensure the actual layout remains as it was before
>>> (when it was essentially UNpacked).
>>>
>>> That the code needed this means that those old DBs won't be portable
>>> between platforms anyhow, or better put like this: we will be binary
>>> backwards compatible after the padding job, but only with the selected
>>> platform (x86/Linux/gcc 3.?/4.? ???) and Win32/64 users will need to
>>> export+import while upgrading, just to be on the safe side.
>>>
>>>
>>>
>>> --
>>> Met vriendelijke groeten / Best regards,
>>>
>>> Ger Hobbelt
>>>
>>> --------------------------------------------------
>>> web:    http://www.hobbelt.com/
>>>        http://www.hebbut.net/
>>> mail:   ger@hobbelt.com
>>> mobile: +31-6-11 120 978
>>> --------------------------------------------------
>>>
>>
>
>



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Jul 31
	
	
Reply
	
	Follow up message
Hi Ger,

thanks - the first round of layoffs are already behind us. It seems
that i'm profiting from the restructuring (but i'm only one of few).
We'll see... not everything's written in stone yet.

Regarding DAM - it's getting damn complicated :)

CLASSIC mode is 0 and therefore used in every database created with <=
1.0.9. The next version 1.0.10 will have a compatible CLASSIC mode.
Currently, the freelist.h/.c code is compatible? I saw in freelist.h
that the _max_bits and the _allocated_bits are now ham_size_t, they
used to be ham_u16_t.

If we decide to change the default implementation from CLASSIC to
something newer, then older hamsterdbs (<= 1.0.9) should not be able
to open the Database. In order to enforce this, we could use a
different minor version - 1.1.0 - in that case the database format is
newer. 1.1.0 could be able to load older databases, but not vice
versa. I think the new functionality of approx. matching is big enough
for a version change. What do you think?

on *MY* todo-list are two more items:
- if a unknown DAM is set which is not supported, then fail to load
the database (because the DAM influences the fileformat)
- when creating a RECORD_NUMBER database: automatically set the
DAM_SEQUENTIAL_INSERT

Regarding the freelist changes: my gcc compiler said that some
assert-statements are always true because the 16bit-range was
exceeded. I commented them out.

Regarding HAM_LVL_DBG_*: i renamed them to HAM_LEVEL_DEBUG_* because i
found it a bit too cryptic :) But that was just a cosmetic change.

I re-enabled the endian tests (i think it was in recno.cpp) because
they are working.

I fixed memory leaks in the unittests (missing ham_delete and
ham_env_delete  - if i would do it again, i would remove those
functions...)

regarding BFC: i'm learning a lot from reading your code! I think it
has really grown in a mature and helpful library. If you're
interested, we can create a project (sourceforge or somewhere else)
and release it separately.

also regarding BFC: the bfc_signal.* was renamed to bfc-signal.*, now
it's more consistent. And the bfc-signal.c was missing - i just added
an empty function. Can you send it?

What else? I have to check this one:
http://www.nabble.com/Possible-bug-in-os_mmap-to24706902.html

6 more hours till weekend :)

Best regards
Christoph

2009/7/31 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Buy-out.... indeed, interesting times. Well, good luck there.
>
> Regarding DAM mode 0: yes, it's buggy, but for now it's still the only
> mode available.
> As I want o make sure the backwards compatibility stuff is nailed down
> before I move into the new DAM modes, all current development is still
> done in DAM #0: the freelist has seen a major overhaul in the
> freel_search() department, but not much gain yet -- we're currently
> profiling that bunch and the new code has a very nice hotspot, thanks
> to a lot of tiny blocks being allocated in the unittests -- a
> representative example, I'd say.
>
> Anyway, it now features Boyer-Moore-inspired searching and a couple of
> speedup features which are certain to appear also in DAM mode 1+ --
> while these optimizations can also be used safely in DAM #0 backwards
> compat mode :-)))
>
> When I get my order-of-magnitude speedup nailed, you'll see another
> freelist appear.
>
> Take care and enjoy the day,
>
> Ger
>
>
>
>
> On Fri, Jul 31, 2009 at 7:51 AM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger!
>>
>> it's a shame but i'm still not done with merging... we have some
>> troubles here at work - my employer was bought and now we're merging
>> with other departments and everything is restructured. These are
>> interesting times...
>>
>> Regarding DAM - it's a good idea! Just one issue - right now,
>> HAM_DAM_CLASSIC (0) is for <= 1.0.9, and it shouldn't be used anymore
>> because it's buggy, right? so the new default (set in
>> hamsterdb.c/_check_create_parameters) should be
>> HAM_DAM_RANDOM_WRITE_ACCESS, i think.
>>
>> I already merged the blob.c/h and the packing things yesterday, and
>> today i'll merge the remaining few files, the header file and the
>> unittests.
>>
>> Have a nice day,
>> Christoph
>>
>> 2009/7/30 Christoph Rupp <chris@crupp.de>:
>>> Hi Ger,
>>>
>>> thanks again for the big load of changes - i'll start merging them today!
>>>
>>> I'll keep you updated.
>>>
>>> Have a nice day!
>>> Christoph
>>>
>>> 2009/7/29 Ger Hobbelt <ger@hobbelt.com>:
>>>>> recno-problem. I actually had something similar a few days ago
>>>>> (RecnoTest::endianXXX failed). As far as i remember the problem was
>>>>> the db_header_t structure, it had a inner union which also was packed.
>>>>> I removed the packing macros from the inner union and then it worked.
>>>>
>>>> Forgot to mention just now: I made sure *all* (nested) persisted
>>>> structs/unions have those pack0/1/2 macros in the right places (I
>>>> still may have missed a struct, but I don't think I did this time
>>>> around). I believe that is important to have as a baseline; when
>>>> things don't 'work' out well regarding loading/accessing older DBs
>>>> (where you say you needed to remove a couple of those pack0/1/2's), my
>>>> take is that it's okay to do that (removing) for now to get the code
>>>> into a state where your own test set passed 100%, but once we're
>>>> there, we need to investigate what alignment is actually employed by
>>>> gcc and reintroduce all the removed pack0/1/2's, while 'padding' the
>>>> those structs to ensure the actual layout remains as it was before
>>>> (when it was essentially UNpacked).
>>>>
>>>> That the code needed this means that those old DBs won't be portable
>>>> between platforms anyhow, or better put like this: we will be binary
>>>> backwards compatible after the padding job, but only with the selected
>>>> platform (x86/Linux/gcc 3.?/4.? ???) and Win32/64 users will need to
>>>> export+import while upgrading, just to be on the safe side.
>>>>
>>>>
>>>>
>>>> --
>>>> Met vriendelijke groeten / Best regards,
>>>>
>>>> Ger Hobbelt
>>>>
>>>> --------------------------------------------------
>>>> web:    http://www.hobbelt.com/
>>>>        http://www.hebbut.net/
>>>> mail:   ger@hobbelt.com
>>>> mobile: +31-6-11 120 978
>>>> --------------------------------------------------
>>>>
>>>
>>
>>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Jul 31
	
	
Reply
	
	Follow up message
On Fri, Jul 31, 2009 at 11:23 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> thanks - the first round of layoffs are already behind us. It seems
> that i'm profiting from the restructuring (but i'm only one of few).
> We'll see... not everything's written in stone yet.

Let's hope you keep on benefiting, eh? ;-)

>
> Regarding DAM - it's getting damn complicated :)

Oh? <looks innocent />

> CLASSIC mode is 0 and therefore used in every database created with <=
> 1.0.9. The next version 1.0.10 will have a compatible CLASSIC mode.
> Currently, the freelist.h/.c code is compatible? I saw in freelist.h
> that the _max_bits and the _allocated_bits are now ham_size_t, they
> used to be ham_u16_t.

Heh. Yeah, the in-memory (RAM) struct has it's types expanded so it
can carry both old-sk00l and 'modern' freelist page sizes; no harm
done, as the important part is now in two sections:

the freelist.c code is - and will remain - old-sk00l compatible: to
accomplish this each write to those RAM-based (nonpersisted) _max_bits
and _allocated_bits MUST (and is) cast to (ham_u16_t) _before_ writing
to the inmem copy: that way, even though it's a ahm_size_t type, it
still will carry exactly the same values as the ham_u16_t types in the
disc page... which is the second part of the trick: the on-disc format
stays as it is for old-sk00l: ham_u16_t.


>
> If we decide to change the default implementation from CLASSIC to
> something newer, then older hamsterdbs (<= 1.0.9) should not be able
> to open the Database. In order to enforce this, we could use a
> different minor version - 1.1.0 - in that case the database format is
> newer. 1.1.0 could be able to load older databases, but not vice
> versa. I think the new functionality of approx. matching is big enough
> for a version change. What do you think?

Yup. I agree. Had the same idea myself, but have not changed the code
yet so hamster accepts /two/ different DB versions; current code only
accepts /one/ type, ever.
That bit can be augmented to auto-detect the correct DAM flags as well
(there are one or two more now, including a flag which
demands/enforced 'pre-1.1.0' file compatibility.

To be seen in the next dist archive I'm going to send to you.


>
> on *MY* todo-list are two more items:
> - if a unknown DAM is set which is not supported, then fail to load
> the database (because the DAM influences the fileformat)

Currently, DAM is only validated on input to create_ex/env_create_ex.
Haven't tested that bit very thoroughly, band besides, those flags can
be combined -- or at least some of them. See the latest (to be sent to
you) hamsterdb.c validation code, which checks for these permutations.

> - when creating a RECORD_NUMBER database: automatically set the
> DAM_SEQUENTIAL_INSERT

Check. Done. (see new hamsterdb.c)


> Regarding the freelist changes: my gcc compiler said that some
> assert-statements are always true because the 16bit-range was
> exceeded. I commented them out.

Ah. Ehm. You mean those asserts fired all the time? That shouldn't be.
Or do you mean the compiler optimized the checks out as the type
checked against was maximum integer size anyway, so it would 'never'
overflow like that? (I take it it's the latter case of the two; all
firing asserts had been #if 0'd by me already). This message was not
shown by MSVC -- worse optimizer, I expect.


> Regarding HAM_LVL_DBG_*: i renamed them to HAM_LEVEL_DEBUG_* because i
> found it a bit too cryptic :) But that was just a cosmetic change.

Seen the change. Applied.

> I re-enabled the endian tests (i think it was in recno.cpp) because
> they are working.

My mistake was I hadn't double-checked if I had kicked all those
//hack unittest disablers in there; thanks for getting rid of those
and fixing it all up.


> I fixed memory leaks in the unittests (missing ham_delete and
> ham_env_delete  - if i would do it again, i would remove those
> functions...)

Which made me wonder when I did the manual source tree merge an hour
ago: some of those _delete calls shouldn't be necessary IMHO as they
are followed by another env_delete(AUTO...) which, if I understand
things correctly, should take care of those. Or is the 'taking care'
limited to just _closing_ those 'handles', etc. and NOT extended to
free()ing the handles as well?


> regarding BFC: i'm learning a lot from reading your code! I think it
> has really grown in a mature and helpful library. If you're
> interested, we can create a project (sourceforge or somewhere else)
> and release it separately.

It might be useful to keep a split-off copy of that; I'm not sure if
it merits it's own SF page, but it would harm it.

What is severely lacking from that code is documentation; especially
diagnostics/tech design, as that is important, given the magick/voodoo
BFC is throwing at the tests under particular instances. Especially
the bfc_signal() handling is a study in itself some times. ;-)  (BTW:
do you have those books by W. Richard Stevens? When you don't, check
them out at a local store or internet shop - they're quite expensive,
but when it comes to coding UNIX and sockets, they are still my main
reference, 10+ years after I bought the lot. Sure, today, there's IPv6
and more which he did not cover in his books (he died before he could,
hence the R.I.P.), but I've learned a lot of my professional network
programming skills from his texts.


> also regarding BFC: the bfc_signal.* was renamed to bfc-signal.*, now
> it's more consistent. And the bfc-signal.c was missing - i just added
> an empty function. Can you send it?

Saw it; as I merged your CVS edits with my latest, you're going to
receive this as paert of the next transmission.

> What else? I have to check this one:
> http://www.nabble.com/Possible-bug-in-os_mmap-to24706902.html

Oops. Missed that one on the way through. Normally I code that using
MSVC LONG_INTEGER stuff, so no shifting or masking needed as it's done
through a union.

> 6 more hours till weekend :)

tick tack ... tick tack ... ;-)



(Still debugging / correcting the statistics gathering code in the
freelist; there's some research opportunities there later...)


--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 2
	
	
Reply
	
	Follow up message
Hi Ger!

Let me answer that mail as long as the kid's sleeping... my wife has
to work today, so it's my turn to keep him busy. I'll start merging
your changes and hope that i'm done tonight.

Regarding version: i've incremented it to 1.1.0 and changed
hamsterdb.c so that older databases will still be opened.

Regarding DAM: i've seen that you have added more DAM constants. I
just have one request: we have to limit the possible combinations. For
a simple reason: they have to be tested.

Currently, i have several thousand acceptance tests running for more
than a day, and then i ran a subset of them under valgrind (another 8
hours). One of these tests executes 250 scripts (some of them short,
others very big) under different configurations (in-memory, different
pagesizes, different keysizes, with or without cursors etc). And
theoretically i would even have to run them again for each different
platform...

Now if i add tests for 10 different DAM combinations things will run
10 times as long :-/

For me the combinations could be limited to:
- DAM_CLASSIC (and nothing else)
- RANDOM_WRITE_ACCESS
- RANDOM_WRITE_ACCESS and FAST_INSERT (does that make sense?)
- SEQUENTIAL_INSERT
- SEQUENTIAL_INSERT and FAST_INSERT
- AUTOTUNE_MODE

and all of these in combination with ENFORCE_PRE110_FORMAT - yuck,
that's 12 combinations :-/


2009/7/31 Ger Hobbelt <ger@hobbelt.com>:
>> Regarding the freelist changes: my gcc compiler said that some
>> assert-statements are always true because the 16bit-range was
>> exceeded. I commented them out.
>
> Ah. Ehm. You mean those asserts fired all the time? That shouldn't be.
> Or do you mean the compiler optimized the checks out as the type
> checked against was maximum integer size anyway, so it would 'never'
> overflow like that? (I take it it's the latter case of the two; all
> firing asserts had been #if 0'd by me already). This message was not
> shown by MSVC -- worse optimizer, I expect.

No - they never fired, the statement was always 1

>> I fixed memory leaks in the unittests (missing ham_delete and
>> ham_env_delete  - if i would do it again, i would remove those
>> functions...)
>
> Which made me wonder when I did the manual source tree merge an hour
> ago: some of those _delete calls shouldn't be necessary IMHO as they
> are followed by another env_delete(AUTO...) which, if I understand
> things correctly, should take care of those. Or is the 'taking care'
> limited to just _closing_ those 'handles', etc. and NOT extended to
> free()ing the handles as well?

the AUTO thing is just for the close. The flag-parameter for
ham_delete/ham_env_delete was a stupid idea. These functions should
return void, but now it's too late :(

BTW - i think i have a network programming book of Stevens. The Unix
books are in the office, i think. I'll have a look (so much to read,
so little time!)

OK - i'll start merging, i'll keep you updated.

Have a nice social evening :)
Christoph


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 2
	
	
Reply
	
	Follow up message
Christoph,

a few notes, or rather: WARNINGS

1)
yes, I realize the constants issue. FYI: this is me getting a bit
Buck-wild ( IceAge3 ;-) ) as I tackle with the freelist and other
stuff as I am working on upping performance for my 10M+ record db
tables (which are slow as snails right now).
This is my way of dumping half-baked ideas into the code so I don't
forget them entirely, as I run into them every time and again now.
When this stabilizes we have to sit and DELETE several of those; right
now it's 99% guesswork and 1% real metrics of what will work and what
doesn't. AUTOTUNE is just a thought 'this is feasible' but a scary
animal to get right, for example.

2)
Same goes for the freelist code: merging this is DANGEROUS; it's
buggy, it's halfbaked, it's simply me in a coding frenzy as I download
my brain into the sources.
As it turns out, my order-of-magnitude speedup was A FAILURE, repeat:
A SERIOUS BUG and further investigation turns up two major candidates
for improvement: the cache page aging system - which now uses fixed
values and an aging decrement round which is buggy in principle: old
pages can stay indefinitely, theoretically, and secondly, especially
for TINY record and keysizes, the Boyer-Moore is not a win (which I
knew) but what I hadn't realize yet was that the statistics
gatherer/hinter - added on a whim - is a MAJOR contributor to speed
improvements as search range is cut down in a big way, especially for
the last freelist page in there, which generally tends to have the
most free space available. BM helps out quite a bit too, but only when
you are processing bigger chunks of data per key/record, and that's
just a specific part of my app.

3)
The last code distro I sent you is compiling, yes, but it was meant to
show you what is going on over here, and it is DEFINITELY UNFINISHED
BUSINESS in there!
I send these (and the next few distros as well, most probably) to give
you a bit of an audit trail on progress over here; when the 'final'
result comes in in a few weeks or so, dropping it in your as it is
then is just murder, as a LOT of thought has gone into it by then and
some of the code sections will be, probably, far from trivial to
understand; diff-viewing the steps that built up to such a state of
affairs helps one to understand what has been going on; what has been
grown and what has been discarded as jolly bad ideas. ;-)

Currently the code has sprouted a very cruddy page cache monitor and
the freelist code has seen a few changes, an improvement and bug
fixes.

4)
The whole DAM flag setting/using business is UTTER CRAP - it simply
does not work.
WARNING: do not even try to fix it yet, as that is carrying water to the sea.
The current line of thought - as of this hour - is that the DAM can be
steered on a per environment-al *default* plus a PER REQUEST (insert
or delete operation) hint/directive: the way the freelist is accessed
enables this; so DAM-per-DB is NOT in the picture any longer. It can
happen, but it'll rather be like a per-DB default then; apart from the
fact that _SOME_ parts of the DAM techniques are per-ENV-only and NOT
reconfigurable/alter on a per-DB or per-REQUEST basis: searching for a
suitably large free slot is per-request configurable, but cache
management is a singular once-per-ENV thing.

And, yup, inside this whole anthill mess are a few viable
bugfixes/alterations for other parts of the code, but I'd rather see
you acting 'slow' on the upgrade/integrate process than seeing you
having to re-change huge chunks of code as you track my changes into
your CVS.


To press the point: currently unittests are /still/ failing all over;
I know this, I realize this, and it's something I don't mind right now
as I'm still busy getting the concept exactly right. I'm just glad you
have these unittests in there as they already uncovered a couple of
serious edge case mistakes - which means those are FAULTS in my
thought process. That is the biggest gain I get out of all this.
Trying to fix stuff that still mentally half-baked is an exercise in
efficiency loss.



Attached is the latest - again: do NOT merge this; just have a look
when you feel like it, 's all. It compiles, 's all. Nothing more, and
NOT finished yet, either! (This is so you can monitor a bit how this
thing progresses.)
- Show quoted text -









On Sun, Aug 2, 2009 at 1:22 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger!
>
> Let me answer that mail as long as the kid's sleeping... my wife has
> to work today, so it's my turn to keep him busy. I'll start merging
> your changes and hope that i'm done tonight.
>
> Regarding version: i've incremented it to 1.1.0 and changed
> hamsterdb.c so that older databases will still be opened.
>
> Regarding DAM: i've seen that you have added more DAM constants. I
> just have one request: we have to limit the possible combinations. For
> a simple reason: they have to be tested.
>
> Currently, i have several thousand acceptance tests running for more
> than a day, and then i ran a subset of them under valgrind (another 8
> hours). One of these tests executes 250 scripts (some of them short,
> others very big) under different configurations (in-memory, different
> pagesizes, different keysizes, with or without cursors etc). And
> theoretically i would even have to run them again for each different
> platform...
>
> Now if i add tests for 10 different DAM combinations things will run
> 10 times as long :-/
>
> For me the combinations could be limited to:
> - DAM_CLASSIC (and nothing else)
> - RANDOM_WRITE_ACCESS
> - RANDOM_WRITE_ACCESS and FAST_INSERT (does that make sense?)
> - SEQUENTIAL_INSERT
> - SEQUENTIAL_INSERT and FAST_INSERT
> - AUTOTUNE_MODE
>
> and all of these in combination with ENFORCE_PRE110_FORMAT - yuck,
> that's 12 combinations :-/



- Show quoted text -
--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.20090802-001.tar.gz	hamsterdb-1.0.9.20090802-001.tar.gz
1388K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 2
	
	
Reply
	
	Follow up message
Hi Ger,

thanks for the mail :)

I merged all your changes from the last mail, but have not yet
committed them. Thanks to your extensive comments i even understood
the search function in freelist.c, but i don't grok it yet... :)

But i feel sorry that BM didn't bring the results that you expected -
seems that you really invested lots of time! The cache problem makes
sense - the counters are never decremented for old pages. That's a
crappy design bug.

Regarding DAM - some of the DAM items are for environments (cache,
freelist), others for databases (INSERT_SEQUENTIALLY), others maybe
per insert. We could also use normal flags for this and still have
full control.
Alternatively, we could use 3 other flags:
HAM_OPTIMIZE_AUTOTUNE (default)
HAM_OPTIMIZE_FOR_SIZE
HAM_OPTIMIZE_FOR_SPEED
and then it's up to hamsterdb to decide about the details and the
implementation.

Another thing on my TODO list is to always generate a Environment
structure even if no environment is used. That would simplify the code
a lot. But it doesn't have a high priority.

Regarding tests: yes, the unittests are great. Annoying but great. I
force myself to extend them whenever i add a feature or a bugfix.

I have more tests, i already told you about my acceptance tests. If
you want to try them:
svn co svn://crupp.de/home/chris/repos/hamsterdb-tests/trunk hamsterdb-tests
cd hamsterdb-tests
sh bootstrap.sh
./configure --enable-debug
make
cd env/posix
vi prepare.sh --->> modify the paths in the first few lines
sh prepare.sh --->> builds the test application
./test --help --->> shows you all command line options
./test --->> reads from stdin; see examples in testfiles/*/*.tst
./test ../../testfiles/1/01.tst --->> runs a short test
../common/run-tests.sh 1 --->> executes all tests in testfiles/1/*
../common/monster.sh 2  --->> runs all tests from testfiles/1/* and
testfiles/2/* in all possible combinations

if you look at monster.sh you know why a release takes about 2 days...
And env/posix/valgrind.sh runs some tests under valgrind which takes
another day.

In case you want to build this for windows, you have to open (and
maybe update) the solution file in env/win32. ../common/run-tests.sh
should also work on windows, but you will need cygwin to run the
bash-script.

these tests also have a dumb profiler built-in (run with --profile=1).
They run against berkeleydb - if they succeed then hamsterdb is
bug-free :)

Especially tests 1/45.tst and 1/220.tst turned out to discover lots of bugs.

BTW - some tests in some combinations fail in berkeleydb; it's their
bug, i informed them but i don't know if they've already fixed them
(it's 220.tst and 100.tst when running with --use-cursors=1).

I'll wait for your updates - tell me when i can merge. in the meantime
i will build the special option for ham_cursor_insert(...
HAM_HINT_APPEND). We can later still replace the flag with a DAM
setting.

Have a nice evening!
Christoph


2009/8/2 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Christoph,
>
> a few notes, or rather: WARNINGS
>
> 1)
> yes, I realize the constants issue. FYI: this is me getting a bit
> Buck-wild ( IceAge3 ;-) ) as I tackle with the freelist and other
> stuff as I am working on upping performance for my 10M+ record db
> tables (which are slow as snails right now).
> This is my way of dumping half-baked ideas into the code so I don't
> forget them entirely, as I run into them every time and again now.
> When this stabilizes we have to sit and DELETE several of those; right
> now it's 99% guesswork and 1% real metrics of what will work and what
> doesn't. AUTOTUNE is just a thought 'this is feasible' but a scary
> animal to get right, for example.
>
> 2)
> Same goes for the freelist code: merging this is DANGEROUS; it's
> buggy, it's halfbaked, it's simply me in a coding frenzy as I download
> my brain into the sources.
> As it turns out, my order-of-magnitude speedup was A FAILURE, repeat:
> A SERIOUS BUG and further investigation turns up two major candidates
> for improvement: the cache page aging system - which now uses fixed
> values and an aging decrement round which is buggy in principle: old
> pages can stay indefinitely, theoretically, and secondly, especially
> for TINY record and keysizes, the Boyer-Moore is not a win (which I
> knew) but what I hadn't realize yet was that the statistics
> gatherer/hinter - added on a whim - is a MAJOR contributor to speed
> improvements as search range is cut down in a big way, especially for
> the last freelist page in there, which generally tends to have the
> most free space available. BM helps out quite a bit too, but only when
> you are processing bigger chunks of data per key/record, and that's
> just a specific part of my app.
>
> 3)
> The last code distro I sent you is compiling, yes, but it was meant to
> show you what is going on over here, and it is DEFINITELY UNFINISHED
> BUSINESS in there!
> I send these (and the next few distros as well, most probably) to give
> you a bit of an audit trail on progress over here; when the 'final'
> result comes in in a few weeks or so, dropping it in your as it is
> then is just murder, as a LOT of thought has gone into it by then and
> some of the code sections will be, probably, far from trivial to
> understand; diff-viewing the steps that built up to such a state of
> affairs helps one to understand what has been going on; what has been
> grown and what has been discarded as jolly bad ideas. ;-)
>
> Currently the code has sprouted a very cruddy page cache monitor and
> the freelist code has seen a few changes, an improvement and bug
> fixes.
>
> 4)
> The whole DAM flag setting/using business is UTTER CRAP - it simply
> does not work.
> WARNING: do not even try to fix it yet, as that is carrying water to the sea.
> The current line of thought - as of this hour - is that the DAM can be
> steered on a per environment-al *default* plus a PER REQUEST (insert
> or delete operation) hint/directive: the way the freelist is accessed
> enables this; so DAM-per-DB is NOT in the picture any longer. It can
> happen, but it'll rather be like a per-DB default then; apart from the
> fact that _SOME_ parts of the DAM techniques are per-ENV-only and NOT
> reconfigurable/alter on a per-DB or per-REQUEST basis: searching for a
> suitably large free slot is per-request configurable, but cache
> management is a singular once-per-ENV thing.
>
> And, yup, inside this whole anthill mess are a few viable
> bugfixes/alterations for other parts of the code, but I'd rather see
> you acting 'slow' on the upgrade/integrate process than seeing you
> having to re-change huge chunks of code as you track my changes into
> your CVS.
>
>
> To press the point: currently unittests are /still/ failing all over;
> I know this, I realize this, and it's something I don't mind right now
> as I'm still busy getting the concept exactly right. I'm just glad you
> have these unittests in there as they already uncovered a couple of
> serious edge case mistakes - which means those are FAULTS in my
> thought process. That is the biggest gain I get out of all this.
> Trying to fix stuff that still mentally half-baked is an exercise in
> efficiency loss.
>
>
>
> Attached is the latest - again: do NOT merge this; just have a look
> when you feel like it, 's all. It compiles, 's all. Nothing more, and
> NOT finished yet, either! (This is so you can monitor a bit how this
> thing progresses.)
>
>
>
>
>
>
>
>
>
> On Sun, Aug 2, 2009 at 1:22 PM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger!
>>
>> Let me answer that mail as long as the kid's sleeping... my wife has
>> to work today, so it's my turn to keep him busy. I'll start merging
>> your changes and hope that i'm done tonight.
>>
>> Regarding version: i've incremented it to 1.1.0 and changed
>> hamsterdb.c so that older databases will still be opened.
>>
>> Regarding DAM: i've seen that you have added more DAM constants. I
>> just have one request: we have to limit the possible combinations. For
>> a simple reason: they have to be tested.
>>
>> Currently, i have several thousand acceptance tests running for more
>> than a day, and then i ran a subset of them under valgrind (another 8
>> hours). One of these tests executes 250 scripts (some of them short,
>> others very big) under different configurations (in-memory, different
>> pagesizes, different keysizes, with or without cursors etc). And
>> theoretically i would even have to run them again for each different
>> platform...
>>
>> Now if i add tests for 10 different DAM combinations things will run
>> 10 times as long :-/
>>
>> For me the combinations could be limited to:
>> - DAM_CLASSIC (and nothing else)
>> - RANDOM_WRITE_ACCESS
>> - RANDOM_WRITE_ACCESS and FAST_INSERT (does that make sense?)
>> - SEQUENTIAL_INSERT
>> - SEQUENTIAL_INSERT and FAST_INSERT
>> - AUTOTUNE_MODE
>>
>> and all of these in combination with ENFORCE_PRE110_FORMAT - yuck,
>> that's 12 combinations :-/
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 2
	
	
Reply
	
	Follow up message
On Sun, Aug 2, 2009 at 9:53 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> thanks for the mail :)
>
> I merged all your changes from the last mail, but have not yet
> committed them. Thanks to your extensive comments i even understood
> the search function in freelist.c, but i don't grok it yet... :)
>
> But i feel sorry that BM didn't bring the results that you expected -

No sweat; it brought a lot of good, but only for part of my DB - the
part where I dump larger sized records in there: that is where BM wins
over classic linear search anyway: for BM to be effective _anywhere_
you need search pattern sizes > 1, and the larger the pattern, the
better. In HamsterDB terms, that means BM can give a gain when you
look for slots which are N*32 bytes for N larger than 1 (e.g. 8 or
more).

> seems that you really invested lots of time! The cache problem makes
> sense - the counters are never decremented for old pages. That's a
> crappy design bug.

The current cache aging scheme fixes this, or at least tries to -- I'm
still unsure giving the various page types different 'priorities' is a
good thing, especially when we're working small caches. (it probably
is, as then at least _some_ pages stick in there, but I digress).
Anyway, FYI: the new aging system is based on this line of thought:

We'd like to leave the flush-page loop, where the current -1 decrement
takes place, as fast as possible (sideline #2: that's why I am angry
at the fact that my edit to convert your doublelinked lists to
_Cyclic_ double linked lists somehow failed misserably: when that
would work, I could simply traverse the ->prev chain in there and
start at the oldest page in cache instead of at the youngest, as
happens now, as new pages are added at the ->next side of the chain.
End of sideline #2)

Anyway, to make the aging happen equally to all, the fastest aging is
NOT DOING ANY AGING (DECREMENTING) AT ALL.
Instead, you get the 'aging' by not setting a cache entry to a fixed
starting age and counting down, but by keeping a kind of 'timestamp'
and assigning cache entries which are 'touched' (used) the latest
'timestamp': that way the entries with the oldest 'timestamp' values
are the ones which were not used for the longest time.
For this you don't need a timer, just an ever-increasing counter
suffices: +1 on the 'timestamp' after each cache page update, combined
with assigning pages the latest timestamp when they are added or used
('touched').

The current scheme adds 'priorities' on top of that, by adding a kind
of 'priority number' to the timestamp value, so 'important' pages ago
slower than 'unimportant' ones.

Anyway, the cache monitor cache code shows some anomalies with this
(or the original scheme) so there's probably a bug in the monitor.
Sigh, nearly 2 decades in the business and still I need multiple
rounds to get anything right. Ah, well.


> Regarding DAM - some of the DAM items are for environments (cache,
> freelist), others for databases (INSERT_SEQUENTIALLY), others maybe
> per insert. We could also use normal flags for this and still have
> full control.

Yup, that was what I've been thinking too: the flags that come with
insert() are well suited to give the inner system a hint how we want
it done.
One thing I have to disagree - but that is because I have a different
concept in my head for this - SEQUENTIAL is not a DB item per se: it
can be considered a per-insert thing too, as each insert() will go
through the statistics-based hinter now, which is the place this is
done: when you keep track of where the front wave of your free slots
is (the 'statistics gathering' in there ;-) ), the SEQUENTIAL mode
simply becomes the hinter telling you, Mr. Insert, to forget about the
whole freelist, except for that last part where I happen to know
there's lots of free space to be had. And it now, we get a FAIL and
you'll add a fresh page to the DB file, which is all new free space
and the insert op can commence.

That way (and that's being done this way right now) we can have two
modes: space-saving (classic') and sequential (add at the front, do
not spend a lot of time of searching for free space every time) and
each of these modes can have a 'regular' level and an 'Uber' level:
ultra/uber-fast insert is sequential+uber/fast and simply does as I
just wrote: jump to the last free zone in the freelist, check if it's
large enough, if yes, we have a slot, if no, it's time to add another
page to the DB.

Then there's 'regular' sequential, which is a bit of a clever nasty
bugger: as we are 'sequential' we jump to the end of the freelist
again, see if there space for us, use if yes, but in case of NO, we do
not yet fail, but we revert to checking the rest of the freelist
anyway. Only this time we scan the freelist front-to-back, so we fill
any gaps in there introduced by delete/erase operations, before we go
off and expand the DB file.
This way, you have two benefits: when you have few deletes, you'll
only do the front-to-back full scan only a few times as the statistics
gatherer will keep track of the scanning results per freelist page:
when freelist pages fill up, the hinter will tell us looking before a
certain point in there is useless, so we can escape quickly. The
benefit here is that we still use all the space in the DB file, as in
'classic' mode, but when we insert a lot, the full freelist scans will
be few in between and that means the DB efficiency is ameliorated as
the cost of those few full scans is divided over far more inserts in
total. This is due to the statistics gatherer+hinter keeping track of
our first free bit in each freelist page and the reversed scan: any
insert which adds another page to the DB immediately helps the next
series of inserts as those will 'see' all that new fresh free space
and go there, before any reverse scanning will happen again.
The second benefit thus is speed improvement for ANY database that
does a lot of insert work (and few deletes) and thus the SEQUENTIAL
mode becomes more than the word itself signifies: it can be used in
scenarios where you normally wouldn't expect 'sequential'.
The drawback I can see is that the cost-per-insert may be low, but a
few inserts in the collective will take longer, so it is not well
suited for Hard Real Time operations; the 'uber' level of 'sequential'
would be the thing there, as it will NEVER backtrace; however, the
UBER mode will produce an ever-expanding DB file, which is a problem
in its own, depending on the system Hamster is running on, but someday
uber+sequential _will_ hit that brick wall of 'out of disc space'...

Meanwhile, we also have 'classic' contrasting the 'sequential': here,
the freelist is scanned as before, start to end, looking for free
space. Nothing remarkable there. The UBER mode here is a little
wicked, but now that we have this statistics gathered, why not use it?
One thing it can (and does) tell us, is the number of failed searches
versus successful searches (and I get some amazing numbers from that
section of the reporting, when I create larger DBs! :-( ), so we can
say we now have some sort of indicator for the level of 'space
utilization' in the DB proper. Now, if we have a lot of failing
searches, i.e. searches which reach at or near the end of the freelist
before hitting a match or reporting an utter fail, this results in our
'utilization ratio' (fail/success) going UP; in UBER mode we can
decide to forget about full scan when we hit a certain high water mark
in utilization and then simply act as if we're in SEQUENTIAL mode,
which will give us another page added to the database in a while
(remember that 'sequential still does the full scan, so in that regard
it's 'classic'); utilization will decrease when there's a sufficient
number of delete/erase operations happening, which will turn the
system back to the usual scanning the freelist start-to-end.
For this we track the deletes: their number and their position: the
latter is implicitly tracked as we already kept track of where the
_first_ free bit in the freelist is and any delete near the start of
the DB will decrement that offset; if we keep track of the total
number of free bits in there (we did already: _allocated_bits !) then
we have our utilization number telling us when to expect,
statistically, when it's time to try a regular start-to-finish scan
again.

This means 'classic' vs. 'sequential' only means that 'classic' is for
those scenarios where you assume a lot of deletes happening to the
'older' data, i.e. data _probably_ located near the start of the DB
file. An example of this is a rotating log DB: old entries are
discarded as new ones are added.

Anyway, enough of this tought train; now you know what's going on in
this head here - and that is slowly copied to the code as well ;-)


> Alternatively, we could use 3 other flags:
> HAM_OPTIMIZE_AUTOTUNE (default)
> HAM_OPTIMIZE_FOR_SIZE
> HAM_OPTIMIZE_FOR_SPEED
> and then it's up to hamsterdb to decide about the details and the
> implementation.

Yup, size vs. speed is my 'uber' vs. 'regular' level. Hamster can keep
track of the number of deletes (erase in your api; sometimes I get a
bit confused with the _delete() there, when doing a global text
search, but don't mind me: knowledge leaks out the back as fast as I
cram it into the front ;-)


> Another thing on my TODO list is to always generate a Environment
> structure even if no environment is used. That would simplify the code
> a lot. But it doesn't have a high priority.

Hm, yeah, I can see that. Quite a few conditionals, hidden by macros, gone.

> Regarding tests: yes, the unittests are great. Annoying but great. I
> force myself to extend them whenever i add a feature or a bugfix.

I know how that feels. Some times (like this week) I don't feel like
adding them before I go in and do something and that has led to some
disasters in the past, as the unittests then never made it later on,
at least not in a fashion they would have grown if I'd done them up
front; lots of fringe cases get caught when you write the unit tests
while not recalling all the intricacies of the code they have to
check. I make that mistake once and again.
- Show quoted text -

>
> I have more tests, i already told you about my acceptance tests. If
> you want to try them:
> svn co svn://crupp.de/home/chris/repos/hamsterdb-tests/trunk hamsterdb-tests
> cd hamsterdb-tests
> sh bootstrap.sh
> ./configure --enable-debug
> make
> cd env/posix
> vi prepare.sh --->> modify the paths in the first few lines
> sh prepare.sh --->> builds the test application
> ./test --help --->> shows you all command line options
> ./test --->> reads from stdin; see examples in testfiles/*/*.tst
> ./test ../../testfiles/1/01.tst --->> runs a short test
> ../common/run-tests.sh 1 --->> executes all tests in testfiles/1/*
> ../common/monster.sh 2  --->> runs all tests from testfiles/1/* and
> testfiles/2/* in all possible combinations
>
> if you look at monster.sh you know why a release takes about 2 days...
> And env/posix/valgrind.sh runs some tests under valgrind which takes
> another day.
>
> In case you want to build this for windows, you have to open (and
> maybe update) the solution file in env/win32. ../common/run-tests.sh
> should also work on windows, but you will need cygwin to run the
> bash-script.
>
> these tests also have a dumb profiler built-in (run with --profile=1).
> They run against berkeleydb - if they succeed then hamsterdb is
> bug-free :)
>
> Especially tests 1/45.tst and 1/220.tst turned out to discover lots of bugs.
>
> BTW - some tests in some combinations fail in berkeleydb; it's their
> bug, i informed them but i don't know if they've already fixed them
> (it's 220.tst and 100.tst when running with --use-cursors=1).
>
> I'll wait for your updates - tell me when i can merge. in the meantime
> i will build the special option for ham_cursor_insert(...
> HAM_HINT_APPEND). We can later still replace the flag with a DAM
> setting.

Awesome! I'll check out the acceptance test rig tomorrow night or the
day after; it's already late and I need to dump a few things in my
head before I go off and do other work tomorrow.


> Have a nice evening!

Same to you!

Tschuess und gute Nacht ;-)

Ger









--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 2
	
	
Reply
	
	Follow up message
> Anyway, to make the aging happen equally to all, the fastest aging is
> NOT DOING ANY AGING (DECREMENTING) AT ALL.
> Instead, you get the 'aging' by not setting a cache entry to a fixed
> starting age and counting down, but by keeping a kind of 'timestamp'
> and assigning cache entries which are 'touched' (used) the latest
> 'timestamp': that way the entries with the oldest 'timestamp' values
> are the ones which were not used for the longest time.
> For this you don't need a timer, just an ever-increasing counter
> suffices: +1 on the 'timestamp' after each cache page update, combined
> with assigning pages the latest timestamp when they are added or used
> ('touched').

Forgot to mention here: the 'ever increasing counter' is, of course, a
little troublesome in that integers are, by definition, not unlimited.
Naive systems solve this by using 64-bit counters and saying that's
enough for us and the next alien race.

Old--fash guys like me, coming from the pre-64-bit era, do it
differently: when we close in on upper limit, we take the whole lot
and DIVIDE their ages by a constant: old entries remain old, fresh
entries remain fresh, and this way we have created a whole lot of
headroom again. Imagine dividing a cache with a 32-bit timestamp
counter by 2^16: given the principle of a cache, there will be really
no age at or below 2^16 in there when our current 'timestamp' is near,
say, 2^32, but even in case of bugs in there, it still works: all
timestamps - and the counter itself - are divided by 2^16, leading to
the latest timestamp now being ~ 2^16, giving us  headroom for 2^32 -
2^16 ~ 4M timestamp increments again.

Only drawback here is that once in a while, the entire cache must be
'divided' like that, but averaged out, this cost is negligible.





So far the algorithm thoughts described.
- Show quoted text -








































\section latest news on freelist / Hamster speed

Just a short update this time. (I'll try to keep it short, I promise ;-) )

Still several assertion failures and unittests is just so-so, but the
big one: the near-find stress test, now dialled up to representative
numbers, i.e. inserting, searching and traversing 5M records, finally
performs near to what I'd like to see.

It turns out the whole statistics gathering business is far more
powerful than I'd given it credit up to now -- now that I'm doing it
to Hamster I understand much better why tuning the statistics of the
big iron out there is always so effective (Oracle, SQL server, those
folks). I've tuned databases to a reasonable performance level before,
but I always wondered why the statistics were almost negligible in
cost.
Now Hamster has the same.

To the tuning there's really three stages:

first stage: cut down on the number of freelist page ENTRIES we're
going to check out. This is the latest addition and seemed like
nonsense at the time, but a 5M record DB table with 4K pages also
implies there's a nice FAT number of freelist entries in the freelist
entry array. And the sequential traversal in there, testing each
freelist ENTRY to see if it's got sufficient space available, is just
hurting bad. --> now there's a 'global hinter' and a 'per page entry'
hinter, the first of which limits the number of probes into the
freelist entry array in such a way that we get an (almost) flat
distribution of entry probes over time (using random number generator
technology), while significantly cutting down on the number of probes
per request.

second stage: cut down on the number of freelist bitarray traversals,
i.e. reduce the number of times the freelist entry BITARRAY is going
to be probed. This is where the statistics gathering pays off the most
at the moment: as we keep track of successful and failed probes - and
the related request sizes - we can state with reasonable certainty
which sections of the bitarray are useless to check as those will
contain only 0-bits (occupied) or -- for fast DAM modes -- how many
free ersus occupied bits we have in the midrange, so that we can skip
to a suitable starting point somewhere in the bitarray.
Some times, this gets even better as the hinter will deliver data
which tells us we won't be able to get a match for the requested size
anyhow, shortcutting the bitarray probe, so we can on to the next
freelist page entry -- or allocate a fresh page at the end.

third stage: cut down on the number of probes _While_ traversing the
freelist bitarray. This is very useful for larger requests, as those
don't need to have every bit probed to still guarantee a good match;
this is where the Boyer-Moore-Sunday techniques come in to help us
find a matching free slot quickly. The BM approach can be further sped
up by applying binary search techniques to find a first probable hit;
even while the freelist bitarray is not 'sorted' in the usual sense,
one can still treat it as such when an 'approximate' match is good
enough, i.e. a match where we don't mind skipping a few opportunities
further towards to beginning of the freelist. (the last free chunk in
there is an all-1s area: if we know the outer edge, we can postulate
that the bitarray is sorted in ascending order and the binary search
will then help us to locate a first free '1' bit _quickly_ (O(log(N)
where N ~ size of bitarray); despite not having a sorted bitarray you
can still show it will _always_ deliver the first '1' bit of that last
free chunk -- or a '1' bit further towards the start, which is
perfectly fine as well.

End result: stress test with 5M records doesn't take a multitude of
MINUTES anymore, but just a few.

The number of freelist entry page probes have been cut down from >>
100 in the older code to an average of ~ 10-20 now; and that's just
the slow version, which still carries the guarantee that free space is
reused as soon as possible. The FAST/UBER mode takes less rounds than
this, but it comes at a cost of a faster growing DB. How much, I
cannot say yet, as those numbers are currently still too much impacted
by bugs and tweaking of the code.

Now the slowest part is the near-find part of the run. Hm.... what can
we do about that....


So far the results as a summary of the technology used in Hamster;
might be handy to add it to the tech documentation later on, because
the freelist management code is, ahhhh, nontrivial by now.


--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 4 (13 days ago)
	
	
Reply
	
	Follow up message
Hi Ger,

thanks for the update!

5M records is quite a bit - i'm really excited to test this soon.

Some updates from me: I started with the APPEND optimization for
ham_cursor_insert. It compiles but so far i haven't tested it and i
don't know how fast it became. But the code is in and i really hope it
will speed up things for larger databases. Sadly it only works when
you use a cursor, which means that the record numbers are only fast if
ham_cursor_insert is used, not ham_insert. But i don't find a way
around this...

Also, most likely i will get access to klocwork (a static code
analysis tool) soon.
http://www.klocwork.com/products/insightDefects.asp?_kk=static%20code%20analysis&_kt=25cecd44-ad37-4be8-bd53-249be834279f&gclid=CMjbtYHMiZwCFZF_3godozH5Yw

This thing is expensive as hell (starting at 16000  or so) and i know
that it found about 20 issues in an older hamsterdb version (i think
1.0.4). I'll run it through the current code and maybe it returns some
helpful results.

And i will write a new sample for the approx. matching functionality.
I think it can have really cool use cases and people should be
informed about it :)

Have a nice day!
Christoph

2009/8/3 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Just a short update this time. (I'll try to keep it short, I promise ;-) )
>
> Still several assertion failures and unittests is just so-so, but the
> big one: the near-find stress test, now dialled up to representative
> numbers, i.e. inserting, searching and traversing 5M records, finally
> performs near to what I'd like to see.
>
> It turns out the whole statistics gathering business is far more
> powerful than I'd given it credit up to now -- now that I'm doing it
> to Hamster I understand much better why tuning the statistics of the
> big iron out there is always so effective (Oracle, SQL server, those
> folks). I've tuned databases to a reasonable performance level before,
> but I always wondered why the statistics were almost negligible in
> cost.
> Now Hamster has the same.
>
> To the tuning there's really three stages:
>
> first stage: cut down on the number of freelist page ENTRIES we're
> going to check out. This is the latest addition and seemed like
> nonsense at the time, but a 5M record DB table with 4K pages also
> implies there's a nice FAT number of freelist entries in the freelist
> entry array. And the sequential traversal in there, testing each
> freelist ENTRY to see if it's got sufficient space available, is just
> hurting bad. --> now there's a 'global hinter' and a 'per page entry'
> hinter, the first of which limits the number of probes into the
> freelist entry array in such a way that we get an (almost) flat
> distribution of entry probes over time (using random number generator
> technology), while significantly cutting down on the number of probes
> per request.
>
> second stage: cut down on the number of freelist bitarray traversals,
> i.e. reduce the number of times the freelist entry BITARRAY is going
> to be probed. This is where the statistics gathering pays off the most
> at the moment: as we keep track of successful and failed probes - and
> the related request sizes - we can state with reasonable certainty
> which sections of the bitarray are useless to check as those will
> contain only 0-bits (occupied) or -- for fast DAM modes -- how many
> free ersus occupied bits we have in the midrange, so that we can skip
> to a suitable starting point somewhere in the bitarray.
> Some times, this gets even better as the hinter will deliver data
> which tells us we won't be able to get a match for the requested size
> anyhow, shortcutting the bitarray probe, so we can on to the next
> freelist page entry -- or allocate a fresh page at the end.
>
> third stage: cut down on the number of probes _While_ traversing the
> freelist bitarray. This is very useful for larger requests, as those
> don't need to have every bit probed to still guarantee a good match;
> this is where the Boyer-Moore-Sunday techniques come in to help us
> find a matching free slot quickly. The BM approach can be further sped
> up by applying binary search techniques to find a first probable hit;
> even while the freelist bitarray is not 'sorted' in the usual sense,
> one can still treat it as such when an 'approximate' match is good
> enough, i.e. a match where we don't mind skipping a few opportunities
> further towards to beginning of the freelist. (the last free chunk in
> there is an all-1s area: if we know the outer edge, we can postulate
> that the bitarray is sorted in ascending order and the binary search
> will then help us to locate a first free '1' bit _quickly_ (O(log(N)
> where N ~ size of bitarray); despite not having a sorted bitarray you
> can still show it will _always_ deliver the first '1' bit of that last
> free chunk -- or a '1' bit further towards the start, which is
> perfectly fine as well.
>
> End result: stress test with 5M records doesn't take a multitude of
> MINUTES anymore, but just a few.
>
> The number of freelist entry page probes have been cut down from >>
> 100 in the older code to an average of ~ 10-20 now; and that's just
> the slow version, which still carries the guarantee that free space is
> reused as soon as possible. The FAST/UBER mode takes less rounds than
> this, but it comes at a cost of a faster growing DB. How much, I
> cannot say yet, as those numbers are currently still too much impacted
> by bugs and tweaking of the code.
>
> Now the slowest part is the near-find part of the run. Hm.... what can
> we do about that....
>
>
> So far the results as a summary of the technology used in Hamster;
> might be handy to add it to the tech documentation later on, because
> the freelist management code is, ahhhh, nontrivial by now.
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 5 (13 days ago)
	
	
Reply
	
	Follow up message
Wouldn't that append optimization clash with the freelist optimization
I'm working on? (In fact, it's about finding free slots in the
freelist and the change is quite pervasive, and my understanding of
append is just the same; the latest here auto-adjusts to using
'sequential+fast' access when the statistics indicate that performance
is going down; see the attached distro and check out the statistics
gatherer and hinter code (two hinters); it's all sitting behind the
alloc + alloc_page, but that was a _huge_ bottleneck for me; I'm still
a bit disappointed that CPU is still the the bottleneck when running
the nearFindStressTest on a 3.6GHz AMD64, but at least it's improved
quite a bit anyway.

Creating a 50M record DB table (yup, that 50.000.000 records) still
takes a while, but at least it's done by today instead of Sint
Juttemis (Dutch eqv. of the Spanish 'manana' ('tomorrow' ~ forever)).

When traversing such a beast though, or find()-seeking through it, at
a certain point there is a noticable slowdown there, which I have not
yet been able to decypher.

Also another extremely weird thing: after calling the database
integrity check call, the page cache is thrashing for quite a while,
and this phenomenon seems DB size related. Of course I expect a
thrashed cache when such an integrity call terminates, but if the '~'
characters printed for every 100 cache page discards (purge) are any
indication, several 1000's of page purge actions happen rapidly one
after another when the the integrity check call has been finished
already; the cache should recover faster than that. Or I'm just
looking at it the wrong way, that may be.

Anyway, distro included as it is nearing something that's 'stable'
(ahem) and useful; at least it doesn't b0rk all over the place any
longer.

This includes an update to the DAM code: AUTOTUNE is **GONE** as it is
a nonsense option: the hinters take care of the 'auto-tuning' anyway
as they switch DAM modes when performance degrades -- see comments in
freelist.c et al -- so hamsterdb.h is now get close to final; the
thing that still needs to be added here is the extra flags for the
insert calls so that they can drive the hinters into switching to
SEQUENTIAL (pick free slot at the end of the freelist first, than
search in reverse) or SEQUENTIAL+FAST (pick slot at end of freelist,
or allocate fresh when none there). See also previous email about the
three stages of this alloc process: two levels of hinting and only
when you get past those you'll be accessing database [freelist] pages,
which cuts down on db page usage and thus optimized page cache usage
indirectly as well.


I'm interested to see your APPEND patch and what klocwork will be
saying about the new code; there's one function now in freelist.c that
takes up about 50% of the entire source file. Some folks who've known
me for a while would say here: 'typical... Hobbelt Code...' ;-)


(NB: I played a bit with a comment reflow patch in uncrustify as the
comments were becoming a mess, but a couple of hours hacking on such a
beast didn't deliver a beauty either. Expect improved improved comment
formatting in due time.)




Note to self:

TODO:

improve the hinters so that large databases too get a sequential scan
instead of a random subsampling, at least on the first request -->
during a sequential scan we mark off every entry as 'unusable' when it
doesn't have enough free space; that way, we can quickly bump up the
start index for any subsequent scans; delete ops might shift that
first_start down again --> another sequential scan to find out where
to start looking for free slots using random sampling --> statistics
gathering should track what the last op was and include that knowledge
in the hinting to speed up insert even more: first insert will be
slow, but subsequent inserts will be faster as entry range is further
reduced.

+ for find() check what the depth of the btree is; after a certain
(high) number, the performance visibly drops, which may be due to
either page cache thrashing (but I don't see it happen, no '~' on
screen, so minimal page flushing) or a bug/feature that I have yet to
discover. I haven't timed it exactly yet, but the perception is that
find() is (slightly) slower than insert now, which shouldn't be, as
both would traverse the Btree and insert has to do more than find()
anyway, at least theoretically. profiler says btree_get_node is the
biggest cpu hog right now, but that makes some sense.
- Show quoted text -




On Tue, Aug 4, 2009 at 10:38 AM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> thanks for the update!
>
> 5M records is quite a bit - i'm really excited to test this soon.
>
> Some updates from me: I started with the APPEND optimization for
> ham_cursor_insert. It compiles but so far i haven't tested it and i
> don't know how fast it became. But the code is in and i really hope it
> will speed up things for larger databases. Sadly it only works when
> you use a cursor, which means that the record numbers are only fast if
> ham_cursor_insert is used, not ham_insert. But i don't find a way
> around this...
>
> Also, most likely i will get access to klocwork (a static code
> analysis tool) soon.
> http://www.klocwork.com/products/insightDefects.asp?_kk=static%20code%20analysis&_kt=25cecd44-ad37-4be8-bd53-249be834279f&gclid=CMjbtYHMiZwCFZF_3godozH5Yw
>
> This thing is expensive as hell (starting at 16000  or so) and i know
> that it found about 20 issues in an older hamsterdb version (i think
> 1.0.4). I'll run it through the current code and maybe it returns some
> helpful results.
>
> And i will write a new sample for the approx. matching functionality.
> I think it can have really cool use cases and people should be
> informed about it :)
>
> Have a nice day!
> Christoph
>
> 2009/8/3 Ger Hobbelt <ger@hobbelt.com>:
>> Just a short update this time. (I'll try to keep it short, I promise ;-) )
>>
>> Still several assertion failures and unittests is just so-so, but the
>> big one: the near-find stress test, now dialled up to representative
>> numbers, i.e. inserting, searching and traversing 5M records, finally
>> performs near to what I'd like to see.
>>
>> It turns out the whole statistics gathering business is far more
>> powerful than I'd given it credit up to now -- now that I'm doing it
>> to Hamster I understand much better why tuning the statistics of the
>> big iron out there is always so effective (Oracle, SQL server, those
>> folks). I've tuned databases to a reasonable performance level before,
>> but I always wondered why the statistics were almost negligible in
>> cost.
>> Now Hamster has the same.
>>
>> To the tuning there's really three stages:
>>
>> first stage: cut down on the number of freelist page ENTRIES we're
>> going to check out. This is the latest addition and seemed like
>> nonsense at the time, but a 5M record DB table with 4K pages also
>> implies there's a nice FAT number of freelist entries in the freelist
>> entry array. And the sequential traversal in there, testing each
>> freelist ENTRY to see if it's got sufficient space available, is just
>> hurting bad. --> now there's a 'global hinter' and a 'per page entry'
>> hinter, the first of which limits the number of probes into the
>> freelist entry array in such a way that we get an (almost) flat
>> distribution of entry probes over time (using random number generator
>> technology), while significantly cutting down on the number of probes
>> per request.
>>
>> second stage: cut down on the number of freelist bitarray traversals,
>> i.e. reduce the number of times the freelist entry BITARRAY is going
>> to be probed. This is where the statistics gathering pays off the most
>> at the moment: as we keep track of successful and failed probes - and
>> the related request sizes - we can state with reasonable certainty
>> which sections of the bitarray are useless to check as those will
>> contain only 0-bits (occupied) or -- for fast DAM modes -- how many
>> free ersus occupied bits we have in the midrange, so that we can skip
>> to a suitable starting point somewhere in the bitarray.
>> Some times, this gets even better as the hinter will deliver data
>> which tells us we won't be able to get a match for the requested size
>> anyhow, shortcutting the bitarray probe, so we can on to the next
>> freelist page entry -- or allocate a fresh page at the end.
>>
>> third stage: cut down on the number of probes _While_ traversing the
>> freelist bitarray. This is very useful for larger requests, as those
>> don't need to have every bit probed to still guarantee a good match;
>> this is where the Boyer-Moore-Sunday techniques come in to help us
>> find a matching free slot quickly. The BM approach can be further sped
>> up by applying binary search techniques to find a first probable hit;
>> even while the freelist bitarray is not 'sorted' in the usual sense,
>> one can still treat it as such when an 'approximate' match is good
>> enough, i.e. a match where we don't mind skipping a few opportunities
>> further towards to beginning of the freelist. (the last free chunk in
>> there is an all-1s area: if we know the outer edge, we can postulate
>> that the bitarray is sorted in ascending order and the binary search
>> will then help us to locate a first free '1' bit _quickly_ (O(log(N)
>> where N ~ size of bitarray); despite not having a sorted bitarray you
>> can still show it will _always_ deliver the first '1' bit of that last
>> free chunk -- or a '1' bit further towards the start, which is
>> perfectly fine as well.
>>
>> End result: stress test with 5M records doesn't take a multitude of
>> MINUTES anymore, but just a few.
>>
>> The number of freelist entry page probes have been cut down from >>
>> 100 in the older code to an average of ~ 10-20 now; and that's just
>> the slow version, which still carries the guarantee that free space is
>> reused as soon as possible. The FAST/UBER mode takes less rounds than
>> this, but it comes at a cost of a faster growing DB. How much, I
>> cannot say yet, as those numbers are currently still too much impacted
>> by bugs and tweaking of the code.
>>
>> Now the slowest part is the near-find part of the run. Hm.... what can
>> we do about that....
>>
>>
>> So far the results as a summary of the technology used in Hamster;
>> might be handy to add it to the tech documentation later on, because
>> the freelist management code is, ahhhh, nontrivial by now.
>>
>>
>> --
>> Met vriendelijke groeten / Best regards,
>>
>> Ger Hobbelt
>>
>> --------------------------------------------------
>> web:    http://www.hobbelt.com/
>>        http://www.hebbut.net/
>> mail:   ger@hobbelt.com
>> mobile: +31-6-11 120 978
>> --------------------------------------------------
>>
>
>
>



--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.20080904-002.tar.gz	hamsterdb-1.0.9.20080904-002.tar.gz
1401K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 5 (12 days ago)
	
	
Reply
	
	Follow up message
Hi Ger!

2009/8/5 Ger Hobbelt <ger@hobbelt.com>:
> Wouldn't that append optimization clash with the freelist optimization
> I'm working on? (In fact, it's about finding free slots in the
> freelist and the change is quite pervasive, and my understanding of
> append is just the same; the latest here auto-adjusts to using

No, no clash at all! Actually both optimizations can work perfectly together.

So far, a ham_cursor_insert basically just called ham_insert (and
later "coupled" the cursor to the inserted item). This means the
B+Tree was traversed (= many key compares, cache operations, perhaps
even I/O) and then the slot in the leaf page was searched (more key
compares).

Now, i immediately fetch the last leaf page in the tree (assuming that
the cursor is already attached to this page), immediately compare the
last key to the new key (instead of binary searching for a new slot)
and then append the new key to the page. In 99% of all cases i only do
one key compare. In the remaining 1% the page is full - then i just
proceed as before (call ham_insert, which will allocate a new page and
do the SMO (structure modification operation)).

If you want to look at it then just update the repository - but i warn
you, i have not had a single test run. I will test it today. The
function is in btree_insert.c - search for HAM_HINT_APPEND.

Good news is that hamsterdb is used here at work, and they have some
performance issues (a database of 200 MB). I can therefore invest some
time today; I'll try all new stuff and your new freelist code and see
if it has some effect!

Regarding performance problems with ham_find: i'll try to find
something with a profiler. If the test is splitted into two programs,
then the profiler can maybe give some good hints.
program 1:
- create a database with 50M keys (wow, what a number)
- close it
program 2: (only this is profiled)
- open the database
- call ham_find
- close it

I'll try to do that today.

> I'm interested to see your APPEND patch and what klocwork will be
> saying about the new code; there's one function now in freelist.c that
> takes up about 50% of the entire source file. Some folks who've known
> me for a while would say here: 'typical... Hobbelt Code...' ;-)

The klocwork server is in india and currently there are negotiations
if we get our own because it's so damn slow. it may take some more
days till i get real access to it...
About your TODO:
> + for find() check what the depth of the btree is; after a certain
> (high) number, the performance visibly drops, which may be due to
> either page cache thrashing (but I don't see it happen, no '~' on

If you want to create a deep B+Tree fast, just set the page size to a
low number (i.e. 1KB) and set the key size to something high (128
byte). Then it will grow REALLY fast.

Have a nice day!
Christoph

PS: sent you the letter yesterday. It felt good to write an "invoice" :)

Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 5 (12 days ago)
	
	
Reply
	
	Follow up message
Hi Ger,

a first report: a test application opens an Environment with 7
databases (file size is 172 MB), reads everything in memory,
transforms a lot of data and writes it to disk.

hamsterdb 1.0.1: 2:23 mins
Your latest tar.gz: 0:32 mins

Great work :)  (this was without my new insert optimization)

Best regards
Christoph

2009/8/5 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Wouldn't that append optimization clash with the freelist optimization
> I'm working on? (In fact, it's about finding free slots in the
> freelist and the change is quite pervasive, and my understanding of
> append is just the same; the latest here auto-adjusts to using
> 'sequential+fast' access when the statistics indicate that performance
> is going down; see the attached distro and check out the statistics
> gatherer and hinter code (two hinters); it's all sitting behind the
> alloc + alloc_page, but that was a _huge_ bottleneck for me; I'm still
> a bit disappointed that CPU is still the the bottleneck when running
> the nearFindStressTest on a 3.6GHz AMD64, but at least it's improved
> quite a bit anyway.
>
> Creating a 50M record DB table (yup, that 50.000.000 records) still
> takes a while, but at least it's done by today instead of Sint
> Juttemis (Dutch eqv. of the Spanish 'manana' ('tomorrow' ~ forever)).
>
> When traversing such a beast though, or find()-seeking through it, at
> a certain point there is a noticable slowdown there, which I have not
> yet been able to decypher.
>
> Also another extremely weird thing: after calling the database
> integrity check call, the page cache is thrashing for quite a while,
> and this phenomenon seems DB size related. Of course I expect a
> thrashed cache when such an integrity call terminates, but if the '~'
> characters printed for every 100 cache page discards (purge) are any
> indication, several 1000's of page purge actions happen rapidly one
> after another when the the integrity check call has been finished
> already; the cache should recover faster than that. Or I'm just
> looking at it the wrong way, that may be.
>
> Anyway, distro included as it is nearing something that's 'stable'
> (ahem) and useful; at least it doesn't b0rk all over the place any
> longer.
>
> This includes an update to the DAM code: AUTOTUNE is **GONE** as it is
> a nonsense option: the hinters take care of the 'auto-tuning' anyway
> as they switch DAM modes when performance degrades -- see comments in
> freelist.c et al -- so hamsterdb.h is now get close to final; the
> thing that still needs to be added here is the extra flags for the
> insert calls so that they can drive the hinters into switching to
> SEQUENTIAL (pick free slot at the end of the freelist first, than
> search in reverse) or SEQUENTIAL+FAST (pick slot at end of freelist,
> or allocate fresh when none there). See also previous email about the
> three stages of this alloc process: two levels of hinting and only
> when you get past those you'll be accessing database [freelist] pages,
> which cuts down on db page usage and thus optimized page cache usage
> indirectly as well.
>
>
> I'm interested to see your APPEND patch and what klocwork will be
> saying about the new code; there's one function now in freelist.c that
> takes up about 50% of the entire source file. Some folks who've known
> me for a while would say here: 'typical... Hobbelt Code...' ;-)
>
>
> (NB: I played a bit with a comment reflow patch in uncrustify as the
> comments were becoming a mess, but a couple of hours hacking on such a
> beast didn't deliver a beauty either. Expect improved improved comment
> formatting in due time.)
>
>
>
>
> Note to self:
>
> TODO:
>
> improve the hinters so that large databases too get a sequential scan
> instead of a random subsampling, at least on the first request -->
> during a sequential scan we mark off every entry as 'unusable' when it
> doesn't have enough free space; that way, we can quickly bump up the
> start index for any subsequent scans; delete ops might shift that
> first_start down again --> another sequential scan to find out where
> to start looking for free slots using random sampling --> statistics
> gathering should track what the last op was and include that knowledge
> in the hinting to speed up insert even more: first insert will be
> slow, but subsequent inserts will be faster as entry range is further
> reduced.
>
> + for find() check what the depth of the btree is; after a certain
> (high) number, the performance visibly drops, which may be due to
> either page cache thrashing (but I don't see it happen, no '~' on
> screen, so minimal page flushing) or a bug/feature that I have yet to
> discover. I haven't timed it exactly yet, but the perception is that
> find() is (slightly) slower than insert now, which shouldn't be, as
> both would traverse the Btree and insert has to do more than find()
> anyway, at least theoretically. profiler says btree_get_node is the
> biggest cpu hog right now, but that makes some sense.
>
>
>
>
> On Tue, Aug 4, 2009 at 10:38 AM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger,
>>
>> thanks for the update!
>>
>> 5M records is quite a bit - i'm really excited to test this soon.
>>
>> Some updates from me: I started with the APPEND optimization for
>> ham_cursor_insert. It compiles but so far i haven't tested it and i
>> don't know how fast it became. But the code is in and i really hope it
>> will speed up things for larger databases. Sadly it only works when
>> you use a cursor, which means that the record numbers are only fast if
>> ham_cursor_insert is used, not ham_insert. But i don't find a way
>> around this...
>>
>> Also, most likely i will get access to klocwork (a static code
>> analysis tool) soon.
>> http://www.klocwork.com/products/insightDefects.asp?_kk=static%20code%20analysis&_kt=25cecd44-ad37-4be8-bd53-249be834279f&gclid=CMjbtYHMiZwCFZF_3godozH5Yw
>>
>> This thing is expensive as hell (starting at 16000  or so) and i know
>> that it found about 20 issues in an older hamsterdb version (i think
>> 1.0.4). I'll run it through the current code and maybe it returns some
>> helpful results.
>>
>> And i will write a new sample for the approx. matching functionality.
>> I think it can have really cool use cases and people should be
>> informed about it :)
>>
>> Have a nice day!
>> Christoph
>>
>> 2009/8/3 Ger Hobbelt <ger@hobbelt.com>:
>>> Just a short update this time. (I'll try to keep it short, I promise ;-) )
>>>
>>> Still several assertion failures and unittests is just so-so, but the
>>> big one: the near-find stress test, now dialled up to representative
>>> numbers, i.e. inserting, searching and traversing 5M records, finally
>>> performs near to what I'd like to see.
>>>
>>> It turns out the whole statistics gathering business is far more
>>> powerful than I'd given it credit up to now -- now that I'm doing it
>>> to Hamster I understand much better why tuning the statistics of the
>>> big iron out there is always so effective (Oracle, SQL server, those
>>> folks). I've tuned databases to a reasonable performance level before,
>>> but I always wondered why the statistics were almost negligible in
>>> cost.
>>> Now Hamster has the same.
>>>
>>> To the tuning there's really three stages:
>>>
>>> first stage: cut down on the number of freelist page ENTRIES we're
>>> going to check out. This is the latest addition and seemed like
>>> nonsense at the time, but a 5M record DB table with 4K pages also
>>> implies there's a nice FAT number of freelist entries in the freelist
>>> entry array. And the sequential traversal in there, testing each
>>> freelist ENTRY to see if it's got sufficient space available, is just
>>> hurting bad. --> now there's a 'global hinter' and a 'per page entry'
>>> hinter, the first of which limits the number of probes into the
>>> freelist entry array in such a way that we get an (almost) flat
>>> distribution of entry probes over time (using random number generator
>>> technology), while significantly cutting down on the number of probes
>>> per request.
>>>
>>> second stage: cut down on the number of freelist bitarray traversals,
>>> i.e. reduce the number of times the freelist entry BITARRAY is going
>>> to be probed. This is where the statistics gathering pays off the most
>>> at the moment: as we keep track of successful and failed probes - and
>>> the related request sizes - we can state with reasonable certainty
>>> which sections of the bitarray are useless to check as those will
>>> contain only 0-bits (occupied) or -- for fast DAM modes -- how many
>>> free ersus occupied bits we have in the midrange, so that we can skip
>>> to a suitable starting point somewhere in the bitarray.
>>> Some times, this gets even better as the hinter will deliver data
>>> which tells us we won't be able to get a match for the requested size
>>> anyhow, shortcutting the bitarray probe, so we can on to the next
>>> freelist page entry -- or allocate a fresh page at the end.
>>>
>>> third stage: cut down on the number of probes _While_ traversing the
>>> freelist bitarray. This is very useful for larger requests, as those
>>> don't need to have every bit probed to still guarantee a good match;
>>> this is where the Boyer-Moore-Sunday techniques come in to help us
>>> find a matching free slot quickly. The BM approach can be further sped
>>> up by applying binary search techniques to find a first probable hit;
>>> even while the freelist bitarray is not 'sorted' in the usual sense,
>>> one can still treat it as such when an 'approximate' match is good
>>> enough, i.e. a match where we don't mind skipping a few opportunities
>>> further towards to beginning of the freelist. (the last free chunk in
>>> there is an all-1s area: if we know the outer edge, we can postulate
>>> that the bitarray is sorted in ascending order and the binary search
>>> will then help us to locate a first free '1' bit _quickly_ (O(log(N)
>>> where N ~ size of bitarray); despite not having a sorted bitarray you
>>> can still show it will _always_ deliver the first '1' bit of that last
>>> free chunk -- or a '1' bit further towards the start, which is
>>> perfectly fine as well.
>>>
>>> End result: stress test with 5M records doesn't take a multitude of
>>> MINUTES anymore, but just a few.
>>>
>>> The number of freelist entry page probes have been cut down from >>
>>> 100 in the older code to an average of ~ 10-20 now; and that's just
>>> the slow version, which still carries the guarantee that free space is
>>> reused as soon as possible. The FAST/UBER mode takes less rounds than
>>> this, but it comes at a cost of a faster growing DB. How much, I
>>> cannot say yet, as those numbers are currently still too much impacted
>>> by bugs and tweaking of the code.
>>>
>>> Now the slowest part is the near-find part of the run. Hm.... what can
>>> we do about that....
>>>
>>>
>>> So far the results as a summary of the technology used in Hamster;
>>> might be handy to add it to the tech documentation later on, because
>>> the freelist management code is, ahhhh, nontrivial by now.
>>>
>>>
>>> --
>>> Met vriendelijke groeten / Best regards,
>>>
>>> Ger Hobbelt
>>>
>>> --------------------------------------------------
>>> web:    http://www.hobbelt.com/
>>>        http://www.hebbut.net/
>>> mail:   ger@hobbelt.com
>>> mobile: +31-6-11 120 978
>>> --------------------------------------------------
>>>
>>
>>
>>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 5 (12 days ago)
	
	
Reply
	
	Follow up message
Hi Ger,

hamsterdb 1.0.1: 2:23 mins
Your latest tar.gz: 0:32 mins
Your last tar.gz with my HINT_APPEND flag: 0:22 mins

I think the HINT_APPEND flag will have major impact on your use case
(storing sequential timestamps).

BTW - the code in btree_insert.c was buggy. i fixed it and committed
it to the repository. Will add some unittests today.

It's not yet final - maybe it's even a bit inconsistent with your
changes. If that's the case, i modify my code (i.e. rename
HAM_HINT_APPEND to HAM_DAM_SEQUENTIAL or something like that and align
it to your ideas).

Another idea for optimization: if a sequential key is inserted, and a
page has to be split, then the pivot element for the split doesn't
need to be in the middle. This wastes page space which is never
reused. Changing this would mean less page splits (=more performance,
smaller files). I'll write it on my TODO list.

Best regards
Christoph

2009/8/5 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Wouldn't that append optimization clash with the freelist optimization
> I'm working on? (In fact, it's about finding free slots in the
> freelist and the change is quite pervasive, and my understanding of
> append is just the same; the latest here auto-adjusts to using
> 'sequential+fast' access when the statistics indicate that performance
> is going down; see the attached distro and check out the statistics
> gatherer and hinter code (two hinters); it's all sitting behind the
> alloc + alloc_page, but that was a _huge_ bottleneck for me; I'm still
> a bit disappointed that CPU is still the the bottleneck when running
> the nearFindStressTest on a 3.6GHz AMD64, but at least it's improved
> quite a bit anyway.
>
> Creating a 50M record DB table (yup, that 50.000.000 records) still
> takes a while, but at least it's done by today instead of Sint
> Juttemis (Dutch eqv. of the Spanish 'manana' ('tomorrow' ~ forever)).
>
> When traversing such a beast though, or find()-seeking through it, at
> a certain point there is a noticable slowdown there, which I have not
> yet been able to decypher.
>
> Also another extremely weird thing: after calling the database
> integrity check call, the page cache is thrashing for quite a while,
> and this phenomenon seems DB size related. Of course I expect a
> thrashed cache when such an integrity call terminates, but if the '~'
> characters printed for every 100 cache page discards (purge) are any
> indication, several 1000's of page purge actions happen rapidly one
> after another when the the integrity check call has been finished
> already; the cache should recover faster than that. Or I'm just
> looking at it the wrong way, that may be.
>
> Anyway, distro included as it is nearing something that's 'stable'
> (ahem) and useful; at least it doesn't b0rk all over the place any
> longer.
>
> This includes an update to the DAM code: AUTOTUNE is **GONE** as it is
> a nonsense option: the hinters take care of the 'auto-tuning' anyway
> as they switch DAM modes when performance degrades -- see comments in
> freelist.c et al -- so hamsterdb.h is now get close to final; the
> thing that still needs to be added here is the extra flags for the
> insert calls so that they can drive the hinters into switching to
> SEQUENTIAL (pick free slot at the end of the freelist first, than
> search in reverse) or SEQUENTIAL+FAST (pick slot at end of freelist,
> or allocate fresh when none there). See also previous email about the
> three stages of this alloc process: two levels of hinting and only
> when you get past those you'll be accessing database [freelist] pages,
> which cuts down on db page usage and thus optimized page cache usage
> indirectly as well.
>
>
> I'm interested to see your APPEND patch and what klocwork will be
> saying about the new code; there's one function now in freelist.c that
> takes up about 50% of the entire source file. Some folks who've known
> me for a while would say here: 'typical... Hobbelt Code...' ;-)
>
>
> (NB: I played a bit with a comment reflow patch in uncrustify as the
> comments were becoming a mess, but a couple of hours hacking on such a
> beast didn't deliver a beauty either. Expect improved improved comment
> formatting in due time.)
>
>
>
>
> Note to self:
>
> TODO:
>
> improve the hinters so that large databases too get a sequential scan
> instead of a random subsampling, at least on the first request -->
> during a sequential scan we mark off every entry as 'unusable' when it
> doesn't have enough free space; that way, we can quickly bump up the
> start index for any subsequent scans; delete ops might shift that
> first_start down again --> another sequential scan to find out where
> to start looking for free slots using random sampling --> statistics
> gathering should track what the last op was and include that knowledge
> in the hinting to speed up insert even more: first insert will be
> slow, but subsequent inserts will be faster as entry range is further
> reduced.
>
> + for find() check what the depth of the btree is; after a certain
> (high) number, the performance visibly drops, which may be due to
> either page cache thrashing (but I don't see it happen, no '~' on
> screen, so minimal page flushing) or a bug/feature that I have yet to
> discover. I haven't timed it exactly yet, but the perception is that
> find() is (slightly) slower than insert now, which shouldn't be, as
> both would traverse the Btree and insert has to do more than find()
> anyway, at least theoretically. profiler says btree_get_node is the
> biggest cpu hog right now, but that makes some sense.
>
>
>
>
> On Tue, Aug 4, 2009 at 10:38 AM, Christoph Rupp<chris@crupp.de> wrote:
>> Hi Ger,
>>
>> thanks for the update!
>>
>> 5M records is quite a bit - i'm really excited to test this soon.
>>
>> Some updates from me: I started with the APPEND optimization for
>> ham_cursor_insert. It compiles but so far i haven't tested it and i
>> don't know how fast it became. But the code is in and i really hope it
>> will speed up things for larger databases. Sadly it only works when
>> you use a cursor, which means that the record numbers are only fast if
>> ham_cursor_insert is used, not ham_insert. But i don't find a way
>> around this...
>>
>> Also, most likely i will get access to klocwork (a static code
>> analysis tool) soon.
>> http://www.klocwork.com/products/insightDefects.asp?_kk=static%20code%20analysis&_kt=25cecd44-ad37-4be8-bd53-249be834279f&gclid=CMjbtYHMiZwCFZF_3godozH5Yw
>>
>> This thing is expensive as hell (starting at 16000  or so) and i know
>> that it found about 20 issues in an older hamsterdb version (i think
>> 1.0.4). I'll run it through the current code and maybe it returns some
>> helpful results.
>>
>> And i will write a new sample for the approx. matching functionality.
>> I think it can have really cool use cases and people should be
>> informed about it :)
>>
>> Have a nice day!
>> Christoph
>>
>> 2009/8/3 Ger Hobbelt <ger@hobbelt.com>:
>>> Just a short update this time. (I'll try to keep it short, I promise ;-) )
>>>
>>> Still several assertion failures and unittests is just so-so, but the
>>> big one: the near-find stress test, now dialled up to representative
>>> numbers, i.e. inserting, searching and traversing 5M records, finally
>>> performs near to what I'd like to see.
>>>
>>> It turns out the whole statistics gathering business is far more
>>> powerful than I'd given it credit up to now -- now that I'm doing it
>>> to Hamster I understand much better why tuning the statistics of the
>>> big iron out there is always so effective (Oracle, SQL server, those
>>> folks). I've tuned databases to a reasonable performance level before,
>>> but I always wondered why the statistics were almost negligible in
>>> cost.
>>> Now Hamster has the same.
>>>
>>> To the tuning there's really three stages:
>>>
>>> first stage: cut down on the number of freelist page ENTRIES we're
>>> going to check out. This is the latest addition and seemed like
>>> nonsense at the time, but a 5M record DB table with 4K pages also
>>> implies there's a nice FAT number of freelist entries in the freelist
>>> entry array. And the sequential traversal in there, testing each
>>> freelist ENTRY to see if it's got sufficient space available, is just
>>> hurting bad. --> now there's a 'global hinter' and a 'per page entry'
>>> hinter, the first of which limits the number of probes into the
>>> freelist entry array in such a way that we get an (almost) flat
>>> distribution of entry probes over time (using random number generator
>>> technology), while significantly cutting down on the number of probes
>>> per request.
>>>
>>> second stage: cut down on the number of freelist bitarray traversals,
>>> i.e. reduce the number of times the freelist entry BITARRAY is going
>>> to be probed. This is where the statistics gathering pays off the most
>>> at the moment: as we keep track of successful and failed probes - and
>>> the related request sizes - we can state with reasonable certainty
>>> which sections of the bitarray are useless to check as those will
>>> contain only 0-bits (occupied) or -- for fast DAM modes -- how many
>>> free ersus occupied bits we have in the midrange, so that we can skip
>>> to a suitable starting point somewhere in the bitarray.
>>> Some times, this gets even better as the hinter will deliver data
>>> which tells us we won't be able to get a match for the requested size
>>> anyhow, shortcutting the bitarray probe, so we can on to the next
>>> freelist page entry -- or allocate a fresh page at the end.
>>>
>>> third stage: cut down on the number of probes _While_ traversing the
>>> freelist bitarray. This is very useful for larger requests, as those
>>> don't need to have every bit probed to still guarantee a good match;
>>> this is where the Boyer-Moore-Sunday techniques come in to help us
>>> find a matching free slot quickly. The BM approach can be further sped
>>> up by applying binary search techniques to find a first probable hit;
>>> even while the freelist bitarray is not 'sorted' in the usual sense,
>>> one can still treat it as such when an 'approximate' match is good
>>> enough, i.e. a match where we don't mind skipping a few opportunities
>>> further towards to beginning of the freelist. (the last free chunk in
>>> there is an all-1s area: if we know the outer edge, we can postulate
>>> that the bitarray is sorted in ascending order and the binary search
>>> will then help us to locate a first free '1' bit _quickly_ (O(log(N)
>>> where N ~ size of bitarray); despite not having a sorted bitarray you
>>> can still show it will _always_ deliver the first '1' bit of that last
>>> free chunk -- or a '1' bit further towards the start, which is
>>> perfectly fine as well.
>>>
>>> End result: stress test with 5M records doesn't take a multitude of
>>> MINUTES anymore, but just a few.
>>>
>>> The number of freelist entry page probes have been cut down from >>
>>> 100 in the older code to an average of ~ 10-20 now; and that's just
>>> the slow version, which still carries the guarantee that free space is
>>> reused as soon as possible. The FAST/UBER mode takes less rounds than
>>> this, but it comes at a cost of a faster growing DB. How much, I
>>> cannot say yet, as those numbers are currently still too much impacted
>>> by bugs and tweaking of the code.
>>>
>>> Now the slowest part is the near-find part of the run. Hm.... what can
>>> we do about that....
>>>
>>>
>>> So far the results as a summary of the technology used in Hamster;
>>> might be handy to add it to the tech documentation later on, because
>>> the freelist management code is, ahhhh, nontrivial by now.
>>>
>>>
>>> --
>>> Met vriendelijke groeten / Best regards,
>>>
>>> Ger Hobbelt
>>>
>>> --------------------------------------------------
>>> web:    http://www.hobbelt.com/
>>>        http://www.hebbut.net/
>>> mail:   ger@hobbelt.com
>>> mobile: +31-6-11 120 978
>>> --------------------------------------------------
>>>
>>
>>
>>
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 5 (12 days ago)
	
	
Reply
	
	Follow up message
> hamsterdb 1.0.1: 2:23 mins
> Your latest tar.gz: 0:32 mins
>
> Great work :)  (this was without my new insert optimization)

Good to hear others are benefiting as well. A factor 5. Now that's a
factor not to be sneezed at. :-) (Yes, I'm proud of it.)

And 50M records in a single table ain't nothing yet, baby ;-))

From where I come from, it's gets 'interesting' when you get beyond a
couple of 100M records, though I'd be called 'amateur' by some folks
that don't wake up until someone asks them to wrangle even larger DBs
- I've probably been treated to too much Oracle addicts these last few
years.


Anyway, this baby is meant to store data for day trading and
historical analysis of stock data; I have SQL server 2005 overhere
(crazy me bought a license for that back in the day when we still had
a happy economy and I still had a steady income) and it does all right
for the search queries, but there's no way I could get the
inserts/updates going fast enough. That's when I started to look
further; first built a custom 1GB mmapped hash-based file DB, which
discarded old data as new data was pushed in (check out cuckoo hashing
tables if you find the time; wicked stuff), but that turned out to
work absolutely great - except on the laptop(s) it's supposed to run
on when done. Those didn't want to accept a single file mapping of 1G
size. :-((((
Then I've looked at BerkelyDB again and I can't really say exactly
_why_ I don't like it; maybe too much concurrency fluff is the excuse
and an API that made me go WTF? despite having working with it about a
decade ago - but I recall not liking it back then either, so it's
probably an emotional thing stuck in the brain. Which led me to
HamsterDB; had a look at the code and API and liked it. Some small
tests turned out favorably, so I went with it. Until I started to feed
it some _real_ data streams, and speed dropped away after the first
few million or so - when you print a dot for every 1K inserts or so,
like I do in nearFindStressTest, you'd see it going from fast to
bloody slow after a while. I still feel it should go faster, but the
latest tests with Release builds show the CPU doesn't top out at 100%
any longer and disc I/O (+ operating system overhead?) is the NEW
bottleneck. With is good sign as that means the optimization session
has been effective; the number of I/O operations was cut down as well
during this time, so it wasn't shuffling effort around.

To give you an idea where this is going: ideally, I like stick all my
data in it, and that means about 1M+ inserts PER DAY. Yup. Nuts. I
know. I haven't checked the 'delete old stuff' side of the matter yet,
as the ideal should be able to store between 1-10 years of data
anyhow, but the trial with 50M records pushed the file size to ~ 8GB
so sizes are a little worse than SQL server (200M records @ 25GB), but
I have no qualms in having a 1TB DB file around, when it comes to
that.


(OFF TOPIC: in case you ever need ever need to log measurement data;
there's an American firm (OSI Software) I've worked with who sell the
PI database, which is a hierarchical database which has *superb*
support for time series. And here's their wicked idea to keep the data
store size limited: they 'compress' older data: old data is globbed
together and compressed by storing the time range + average, min, max,
std. dev. and such of the data within the range. This happens in
multiple (configurable) stages, where the time range increases with
every stage.
So say you've got a piece of hardware measuring flow every second; and
this sample train is fed to the PI database and stored as is.
However, as the database grows, the configured compression kicks in
and takes, say, 10 second sections (~ range) of all the old data
that's about 1 day old, calculates the min/max/avg/sd/... and stores
those data bits in a record, while deleting the original samples
(about 10 in my example here). That's stage 1 of the compression. The
next stage would be taking data from 1 week old, e.g. as ranges of 30
minutes each, and calculate the cummulative min/max/avg/sd/... and
store that as a single 'compressed' record, while deleting all those
1-minute min/max/avg/sd records within that range in the same go, thus
compressing the data even further. Of course the number of compression
stages, the ranges, etc. are all configurable. Meanwhile any find()
request would simply produce the sample data as if it was still
sitting as-is in the DB: for old data, the sample data is recalculated
from the distribution info stored instead. With each measurement you
can also obtain 'accuracy' information. Incidentally, this 'accuracy'
is a known parameter of all physical measurements anyway and the PI
folks use this (configurable) parameter to further compress data: you
can configure the DB to employ 'adaptive compression', which globs not
fixed ranges, but ranges of data which data points are all within the
given [reporting or measurement] accuracy. For the observer, nothing
is lost, while the amount of data stored is reduced quite a bit. To
me, when I first encountered this, it was a great idea and feels a bit
like JPG compression: it's lossy but from the 'perception of the
observer' you still have all the goodies available.

This PI database system is used a lot throughout the process industry
(pharma, petrochem, ...) to log process parameters. There are PI
databases out there that store over 10 years of data on a relatively
small HD, thanks to the compression - which does not harm the
reporting / efficiency calculations / etc. that are applied to the
data as the effect of the compression is that the data is only
'smoothed' over time, but otherwise remains intact.

Guess where the near match would be very useful, assuming Hamster was
sitting at the bottom of such a tool chain: hamster stores the
measurement data; I don't know yet how one would go about that, but I
think the filters you offer can be extended to enable the compression
functionality, and then near-find can be used to retrieve the nearest
time sample (or for old data, it's compressed timerange eqv. record -
where the Hamster [post-find] filters again can be used to expand the
compressed data into 'faked' measurement data a la PI.

Since you were talking about use cases for the new functionalities:
that's one I have been thinking about before.
The only pity here is that for my analysis systems I cannot tolerate
such compression/smoothing, so I'll have to stick to storing the data
as it is. (The data is required for Monte Carlo simulations of stock
trade models; by feeding them with collected (historic) data, I can
get an estimate about their effectiveness before I take such a model
(idea) into the wild and loose money with it.)

So, yes, storing stock trade data is another good example of Hamster
use + near find. :-) Now I can store trades as they come in and since
my models are sampled-time systems, I can now simply poke the database
for the sample at timestamp X without any worry whether there's a
trade available at that particular time or not. SQL doesn't like this
type of scenario, as you need to either specify a range, maybe with a
SORT and TOP thrown in, or a >= SELECT fed into a cursor, which just
picks off the first record. These can be made to execute fast, but I
think a close-to-bare-metal system like Hamster can be made to be
faster, as I can kick all the general purpose SQL engine overhead.)


And to close this blurb: I'm thinking about having a look at the find
code (and consequently the B+tree traversal) and see if I can
speed-kick the finds by having them look at the latest leaf B+tree
node before doing the official traversal; that shouldn't only work for
cursors, but also regular find as this kind of stuff can optionally be
stuck in the statistics gathering, thus 'persisting' this 'last use'
type of info for the duration of an OPEN-CLOSE session of the DB.
I say this, because your insert speedup might also be manipulated that
way: stick it in the statistics and when the trick doesn't work out,
disable it, until the statistics hint otherwise -- currently I do not
(yet) collect record FIND oriented statistics, but that's in the
pipeline if I get my way.


(And how does performance compare with your new v2 now? ;-) )
- Show quoted text -




--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 5 (12 days ago)
	
	
Reply
	
	Follow up message
> hamsterdb 1.0.1: 2:23 mins
> Your latest tar.gz: 0:32 mins
> Your last tar.gz with my HINT_APPEND flag: 0:22 mins

Wow! I'm going to check it out right now!


> It's not yet final - maybe it's even a bit inconsistent with your
> changes. If that's the case, i modify my code (i.e. rename
> HAM_HINT_APPEND to HAM_DAM_SEQUENTIAL or something like that and align
> it to your ideas).

HINT_APPEND is fine for now; my own ideas there are a little vague,
but I believe the SEQUENTIAL vs. R/A DAM hints should not switch the
DB bahaviour from one extreme to the other, so if we can have some
sort of 'gradual switchover' I'd feel more comfortable with that,
never mind me myself probably using one extreme of this universe in
99.5% of cases. Hamster shouldn't be 'tuned' to my particular need in
that sense; whenever possible other use case scenarios should benefit
from the hints as much as I do; hence the introduction of the
statistics gathering + dual layer hinter mechanism: it's rather
generic and helps everyone speed up their inserts when the
insert/erase (I keep forgetting you call 'delete' 'erase' :-( ) ratio
is relatively large.


> Another idea for optimization: if a sequential key is inserted, and a
> page has to be split, then the pivot element for the split doesn't
> need to be in the middle. This wastes page space which is never
> reused. Changing this would mean less page splits (=more performance,
> smaller files). I'll write it on my TODO list.

<scratches head> I'm not familiar enough with B+ yet (have to recheck
my Knuth and Sedgewick again there) to agree or counter this one. My
gut feel tells me that, when this would help, there's a more
fundamental issue with the node splitting; thought train here is this:
add node 1, 2, 3, 4, ..., N, where N causes node overflow; then I'd
imagine a split a la binary tree, i.e. at, in this case, N/2.
Hmmmmm.... the system cannot really compare keys X and Y and deliver
the DISTANCE, which is a thing needed for that. (Now I recall I had a
quick idea about speeding up btree_get_node as that was in the top10
in the profiler report here, but speeding up a binary search can only
be done by improving on the median, which requires one to be able to
calculate such DISTANCES between keys, so the median can be 'weighted'
based on such distances. Of course, such a thing could be hacked into
Hamster by allowing custom compare routines to deliver numbers other
than -1 and +1, i.e. distances, but that would be another change.

I'll see if I can get MSVC2008 to report the profiler results in some
legible way; haven't used it often enough to be comfortable with it;
in former days I always used a Compuware profiler product; much better
sampling profiler than M$'s...


--
- Show quoted text -



































\section HINT_APPEND
				
	Inbox		X	
				
				
	direct @ me		X	
				
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 6 (11 days ago)
	
	
Reply
	
	Follow up message
I'm refactoring (koff koff koff) the statistics code a bit; it's grown
enough to merit it's own source file. Besides, there's three different
categories of stats gathering + hinting now; the third was an idea for
FIND still simmering at the back of the brain and then you did
HINT_APPEND which showed me the way.

What's in store?

HINT_APPEND will probably be renamed (and I have to see about all
those flags; probably define a common range for hints, etc. and
document that in hamsterdb.h) as this type of hint can not only be
applied to insert but to find and erase as well -- and there's no need
to restrict it to cursor only, though with cursors, we can include
your speed optimization as-is.
The trick is to include the last btree leaf node as an info item for
statistics gathering, i.e. remeber per operation (FIND/ERASE/INSERT)
what the last used btree leaf node is. Then the btree-level hinter[*]
can hint to us to check out that last used btree leaf node first -- as
long as it's still in the cache, so available at minimal cost, AND(for
find at least) the key match lands us away from the node edge (because
when landing ON the edge, you don't know the distance to the actual
match; especially with LT/GT find ops).

[*] (that's the third category I mentioned above: 1 = ENV global for
freelist, 2= freelist page entry, 3 = btree-level insert / find /erase
operation)


I'm currently mixing this with your transaction specific speedup --
which can be induced by the hinter without a hitch: this way it's a
combo of transaction specific, current find/insert/erase flags (acting
as overrides for the DB-wide DAM preferences), etc.



Got to monitor a stock as I'm currently riding one, so little time to
work on this; maybe some sources tonight, maybe later.

Sorry if typos in here or vague. Attention elsewhere ;-)

Ciao,

Ger




--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 6 (11 days ago)
	
	
Reply
	
	Follow up message
Hi Ger,

2009/8/6 Ger Hobbelt <ger@hobbelt.com>:
> I'm refactoring (koff koff koff) the statistics code a bit; it's grown
> enough to merit it's own source file. Besides, there's three different
> categories of stats gathering + hinting now; the third was an idea for
> FIND still simmering at the back of the brain and then you did
> HINT_APPEND which showed me the way.

Feel free to change the HINT_APPEND flag or completely merge it with
DAM. We can look at the interface later on.

> The trick is to include the last btree leaf node as an info item for
> statistics gathering, i.e. remeber per operation (FIND/ERASE/INSERT)
> what the last used btree leaf node is. Then the btree-level hinter[*]
> can hint to us to check out that last used btree leaf node first -- as
> long as it's still in the cache, so available at minimal cost, AND(for
> find at least) the key match lands us away from the node edge (because
> when landing ON the edge, you don't know the distance to the actual
> match; especially with LT/GT find ops).

That's a cool idea! But you have to make sure that the page is not
unloaded (because of a cache purge) and that it's not erased (because
of a structure modification in erase). If you look at the cursor code,
then you see that a cursor has two modes: it's either "coupled" or
"uncoupled" to/from a page. In the first mode, it has a direct pointer
to the page_t *structure. In the latter mode, it only stores the
offset of the page. If the page is needed, it has to be fetched
through the cache.

This works fine, but it's a little bit complicated and took a while
till it worked well.

I'm still reading about Monte Carlo (don't remember the details from
university) and about OSI software's PI Database. It's an interesting
concept. Do they allow modifications in the archived data?

MySQL also has an archive backend which is based on zlib, but they do
not allow modifications for it.

Regarding compression the filters won't help because they either
compress records or pages. If you compress an index page, then the
size is no longer constant. it will be very tricky to get this
running.

An alternative would be to use compression mechanisms used in
column-storage DBMS's (i.e. vertica). They compress index pages with
special mechanisms (i.e. RLE or by storing only the difference between
one key and the next etc.) They save LOTS of space and therefore also
I/O bandwidth. but that's for a future far far away... :)

Good luck with your shares!

Christoph
Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 6 (11 days ago)
	
	
Reply
	
	Follow up message
Did alright with the shares; just a little gain, not as much as I'd
wished for, but alas, it was a nice game. Today looked like selling
day and indeed it was. I am liquide again, so no stress. Until the
next buy ;-)

Anyway...

> Feel free to change the HINT_APPEND flag or completely merge it with
> DAM. We can look at the interface later on.

Oki. I'll post a distro somewhere this night - I hope - which will
have the code moving around quite a bit.

One thing to note - as I was pondering a future erase/delete speedup -
delayed delete by marking keys as 'deleted' and flag the btree node as
'compressible' so subsequent insert/delete/compress (no zlib, mind you
;-) ) ops can detect this and either re-use the space or compress the
btree node after the fact. Think bulk delete: the memmove to
shift/recompress btree nodes (or should I say: 'tighten', to prevent
confusion with zlib compression?) is prohibitive - top10 rating in the
profiler when deleting series of records in that infamous 50M record
test DB.

And then -- and this is the FYI note here -- I ran into the ambiguity
of the key_get_flags/key_set_flags macros: both int_key_t and
ham_key_t have a _flags, but _int_key_t's in 8-bit while ham_key_t's
is 32-bit; meanwhile, I coded my LT?GT markers as if they were sitting
in the int_key_t as well - which is WRONG - so I changed the code to
make sure the compiler threw up, then fixed the code. Change:
int_key_t now has _flags --> _flags8 renamed, so the compiler b0rks,
then the key_get_flags/set_flags is changed alongside - as it turned
out the int_key_t related use of these macros was larger than the
ham_key_t related use, then added the new
ham_key_get_flags/set_flags() macros which are targeted at the
ham_key_t structure and fixed all relevant places in the code again (a
lot of LT/GT code).

End result: no more _flags ambiguity, IS_LT/IS_GT defines have moved a
bit, code is adapted and compiles again and now it's on with the
various flags bitranges inspection -- I want to document those as I'm
getting a bit cuckoo with a all the mixing flag series out there, so
it's time to define some bitranges in the documentation so that my
confusion will go away.
Side effect: doxygen output improved.


Thoughts:

I'm still a little hazy about how to merge your APPEND idea and my
statistics/hinting based pervasive approach, but the current idea
there is to state that HINT_APPEND is basically the FAST/UBER mode of
the INSERT operation and the hinter should therefore check that 'is
this new key the very last one' check like you do and hint about what
path to take from there. As such, HINT_APPEND is a type of SEQUENTIAL
access -- I consider SEQUENTIAL more generic than that, as I intend to
monitor the amount of times a find/insert/delete land on the same
btree leaf time and again: that tells us that find/insert/delete ops
use keys which are located very closely together, i.e. a kind of
'SEQUENTIAL' access taking place.

For find, it's easy: when you find out the previous few finds landed
on the same btree leaf, it's common sense to hint/advise to check that
leaf for the next find as well - until this hint turns out to be
incorrect, in which case the accounting starts afresh and a while
later the hint can trigger again.
For [cursor] move, it's likewise: when not in jump-to-start or
jump-to-end mode, it's really 'SEQUENTIAL' what you're asking Hamster
to do for you.
For insert, it's a little trickier - and I haven't coded this yet - as
you can also have 'SEQUENTIAL' somewhere in the middle of the key
range; just as long as the inserts fit into the same btree leaf,
you're good to go. Kinda like your APPEND, but a little more
generalized.
For delete, same story as insert, only this time it's necessary to
check if the node will be collapsed/folded up as it becomes emptier
and emptier. To further speed up delete, I was thinking about
introducing delayed delete in there, so that I save big time on the
memmove shifting; especially important with big pages and hence large
btree nodes: there memmove shifting is a rather costly operation.

So far the thoughts; only FIND is currently implemented; when I see
how I want to tackle it exactly, the HINT_APPEND will be merged into
this pattern as well.

Meanwhile, I'll try to heed your warning about the btree page caching
vs. erase restructuring and all; I didn't yet entirely grok the
'coupling' of those pages with the cursors but the understanding is
growing as I go along.


> I'm still reading about Monte Carlo (don't remember the details from
> university) and about OSI software's PI Database. It's an interesting
> concept. Do they allow modifications in the archived data?

Phew. Too long ago to recall this with any certainty. I don't remember
that they did; after all it's targeted at time series and what you do
there is LOGGING of NEW data; manipulating old data could even be
frowned upon from a security point of view: consider this stuff being
used in pharma environments and I don't know if you've ever sat with a
team who had to get FDA approval for a new pharma manufacturing
process, but it's worse than ISO900x certification and don't even dare
to change something once it's signed off, as the whole show restarts
from base 1. Included in the whole shebang is the requirement that
you, as a manufacturer, can trace your /entire/ process. Read as: any
pill in a bottle is part of a batch; each bottle has a serial and you,
the manufacturer, must be able, for the next 10-20 years at least, to
cough up EXACTLY where this pill stuff (as a pill, a powder, fluid,
any intermediate!) has been in your manufacturing processes and what
was done to it.

In such a environment, people can become extremely agitated when you
inform them that historical data can be 'edited'. Of course, the
manufacturer would like it, but they can't say that and CERTAINLY
don't want this kind of thing to, ah, 'leak' to the FDA or their
European equivalent, because right there and then, you can forget
about getting approval. Which is a multimillion dollar operation, so
the agitation is understandable, in a way ;-))


> MySQL also has an archive backend which is based on zlib, but they do
> not allow modifications for it.

zlib/RLE, etc. are all non-lossy compression /overlays/ really; this
thing OSI is doing is embedded into the database at the core level; it
is part of the original design, not an afterthought. The whole
/intent/ of the database is to compress the data to these summary
chunks. Think of it as smoothing+resampling of the old data; you lose
detail, but when this loss is within specified bounds, it is perfectly
acceptable. The best comparison is with JPEG compression, where you
set the quality level. With PI, you configure the required data
accuracy: more 'tolerance' / less 'accuracy' means more data can be
aggregated --> compressed.
When I really go nuts one day, I'll code it up as an example of
Hamster use; the filters I was talking about for this are not there
yet, but you already have post-find filters, etc. so it's simply
extending that concept - in a big way. You can either compress on the
fly or have it scheduled (PI); the same can be built using Hamster:
when an insert surpasses a certain threshold (# of keys stored) one
could fire a callback which traverses the database from oldest to
newer, grabs the data and recompress it on the run a la PI: delete N
records, insert one summary record in their stead. Then have the
already present post-find filter help out by having it convert those
summary records, when hit, to 'faked' data records instead, thus
acting as if Hamster was storing all that beautifully logged data all
the time.


And don't think Monte Carlo is complex; it's really just a test rig
where you drive the tests through a random generator: it picks the
test cases (generates the test data; in my case it generates the test
choices like stock and time and then the real trade data is fed to the
model under test) and collects the results of each test. Then rinse &
repeat a sufficiently large couple of times, say 1000+ tests. Now we
have a collection of values for each result item and for that we can
determine things like mean, min, max and std.dev. and that can be
thrown into a plot (or an Excel sheet, over here). The averaged result
tells me something about the ability of the model-under-test, while
the min/max/std.dev. numbers tell me a bit about the probable
stability (predictability of behaviour) of the MUT. The hard part is
getting the random generator set up right; standard rand() doesn't cut
it regarding randomness; any 'patterns' occurring in the randomness
can have a major impact on the final results as those semi-randomness
artifacts can drive tests towards a unnoticed direction. Few more
details to spice the soup and you're good to go. ;-)

> Regarding compression the filters won't help because they either
> compress records or pages. If you compress an index page, then the
> size is no longer constant. it will be very tricky to get this
> running.

I know. See above; PI compression is not like zlib's; it's more JPEGgy
applied to a whole set of records (and keys).

When you've got that down, the usefulness of LT/GT search becomes
clearly apparent as you can now simply track down the summary record
instead of the (now removed) old data items themselves; LEQ search
will deliver the proper summary record; then all you need to do is
take that summary record and 'fake' a data sampling record instead to
feed to the FIND() output.


> An alternative would be to use compression mechanisms used in
> column-storage DBMS's (i.e. vertica). They compress index pages with
> special mechanisms (i.e. RLE or by storing only the difference between
> one key and the next etc.) They save LOTS of space and therefore also
> I/O bandwidth. but that's for a future far far away... :)

Welllll.... you could say that PI's compression is the lossy version
of this. (Haven't looked into column-storage DBMSs myself, but from
the sound of it, the comparison should hold.)
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 7 (10 days ago)
	
	
Reply
	
	Follow up message
Hi Ger,

Glad that you're not broke yet. I had my own share of bad experience
with stocks (i was young and thought that i can get rich without
honest work :) ) and now i don't touch them with a 10 foot stick...

I'll be out the whole weekend because of family issues but  i hope i'm
back sunday evening, and then i'll definitely spend some time with the
hamster.

Regarding merging HINT_APPEND and DAM (and the next steps in general)
- i would really be happy if i could release a new hamsterdb version
(1.10) in the next few weeks - even if it's not yet complete regarding
the other optimizations. (as long as the API is stable and there are
no known bugs)

So if you have something that you consider stable (regarding Bugs
*and* API interface) then you can drop it to me and i let my tests run
through, and also let the folks at work run some tests (they found a
crash in the newest drop).

So if there's anything i can do to help (besides testing) then please
tell me or i will feel a bit useless :)

i'll write a sample and will enhance the tutorial.

Regarding compression - the lossless compression sounds good, but the
concept of lossy compression is totally new to me. We could create a
filter which is called for pages older than a specific date (i.e.
everything older than a year) and then the filter can compress the
page and merge it with the sibling pages. i think that's doable as
long as the page sizes remain constant - everything else would be
really difficult. But the more i think about it, the easier it would
be - a kind of map/reduce algorithm working on older leaf pages.

Actually, this would really be a cool use case for map/reduce...  We
could even run it multi-threaded, because it's mostly CPU-related, i
guess.

Back to work...

2 more hours - goed weekend :)
Christoph

2009/8/6 Ger Hobbelt <ger@hobbelt.com>:
- Show quoted text -
> Did alright with the shares; just a little gain, not as much as I'd
> wished for, but alas, it was a nice game. Today looked like selling
> day and indeed it was. I am liquide again, so no stress. Until the
> next buy ;-)
>
> Anyway...
>
>> Feel free to change the HINT_APPEND flag or completely merge it with
>> DAM. We can look at the interface later on.
>
> Oki. I'll post a distro somewhere this night - I hope - which will
> have the code moving around quite a bit.
>
> One thing to note - as I was pondering a future erase/delete speedup -
> delayed delete by marking keys as 'deleted' and flag the btree node as
> 'compressible' so subsequent insert/delete/compress (no zlib, mind you
> ;-) ) ops can detect this and either re-use the space or compress the
> btree node after the fact. Think bulk delete: the memmove to
> shift/recompress btree nodes (or should I say: 'tighten', to prevent
> confusion with zlib compression?) is prohibitive - top10 rating in the
> profiler when deleting series of records in that infamous 50M record
> test DB.
>
> And then -- and this is the FYI note here -- I ran into the ambiguity
> of the key_get_flags/key_set_flags macros: both int_key_t and
> ham_key_t have a _flags, but _int_key_t's in 8-bit while ham_key_t's
> is 32-bit; meanwhile, I coded my LT?GT markers as if they were sitting
> in the int_key_t as well - which is WRONG - so I changed the code to
> make sure the compiler threw up, then fixed the code. Change:
> int_key_t now has _flags --> _flags8 renamed, so the compiler b0rks,
> then the key_get_flags/set_flags is changed alongside - as it turned
> out the int_key_t related use of these macros was larger than the
> ham_key_t related use, then added the new
> ham_key_get_flags/set_flags() macros which are targeted at the
> ham_key_t structure and fixed all relevant places in the code again (a
> lot of LT/GT code).
>
> End result: no more _flags ambiguity, IS_LT/IS_GT defines have moved a
> bit, code is adapted and compiles again and now it's on with the
> various flags bitranges inspection -- I want to document those as I'm
> getting a bit cuckoo with a all the mixing flag series out there, so
> it's time to define some bitranges in the documentation so that my
> confusion will go away.
> Side effect: doxygen output improved.
>
>
> Thoughts:
>
> I'm still a little hazy about how to merge your APPEND idea and my
> statistics/hinting based pervasive approach, but the current idea
> there is to state that HINT_APPEND is basically the FAST/UBER mode of
> the INSERT operation and the hinter should therefore check that 'is
> this new key the very last one' check like you do and hint about what
> path to take from there. As such, HINT_APPEND is a type of SEQUENTIAL
> access -- I consider SEQUENTIAL more generic than that, as I intend to
> monitor the amount of times a find/insert/delete land on the same
> btree leaf time and again: that tells us that find/insert/delete ops
> use keys which are located very closely together, i.e. a kind of
> 'SEQUENTIAL' access taking place.
>
> For find, it's easy: when you find out the previous few finds landed
> on the same btree leaf, it's common sense to hint/advise to check that
> leaf for the next find as well - until this hint turns out to be
> incorrect, in which case the accounting starts afresh and a while
> later the hint can trigger again.
> For [cursor] move, it's likewise: when not in jump-to-start or
> jump-to-end mode, it's really 'SEQUENTIAL' what you're asking Hamster
> to do for you.
> For insert, it's a little trickier - and I haven't coded this yet - as
> you can also have 'SEQUENTIAL' somewhere in the middle of the key
> range; just as long as the inserts fit into the same btree leaf,
> you're good to go. Kinda like your APPEND, but a little more
> generalized.
> For delete, same story as insert, only this time it's necessary to
> check if the node will be collapsed/folded up as it becomes emptier
> and emptier. To further speed up delete, I was thinking about
> introducing delayed delete in there, so that I save big time on the
> memmove shifting; especially important with big pages and hence large
> btree nodes: there memmove shifting is a rather costly operation.
>
> So far the thoughts; only FIND is currently implemented; when I see
> how I want to tackle it exactly, the HINT_APPEND will be merged into
> this pattern as well.
>
> Meanwhile, I'll try to heed your warning about the btree page caching
> vs. erase restructuring and all; I didn't yet entirely grok the
> 'coupling' of those pages with the cursors but the understanding is
> growing as I go along.
>
>
>> I'm still reading about Monte Carlo (don't remember the details from
>> university) and about OSI software's PI Database. It's an interesting
>> concept. Do they allow modifications in the archived data?
>
> Phew. Too long ago to recall this with any certainty. I don't remember
> that they did; after all it's targeted at time series and what you do
> there is LOGGING of NEW data; manipulating old data could even be
> frowned upon from a security point of view: consider this stuff being
> used in pharma environments and I don't know if you've ever sat with a
> team who had to get FDA approval for a new pharma manufacturing
> process, but it's worse than ISO900x certification and don't even dare
> to change something once it's signed off, as the whole show restarts
> from base 1. Included in the whole shebang is the requirement that
> you, as a manufacturer, can trace your /entire/ process. Read as: any
> pill in a bottle is part of a batch; each bottle has a serial and you,
> the manufacturer, must be able, for the next 10-20 years at least, to
> cough up EXACTLY where this pill stuff (as a pill, a powder, fluid,
> any intermediate!) has been in your manufacturing processes and what
> was done to it.
>
> In such a environment, people can become extremely agitated when you
> inform them that historical data can be 'edited'. Of course, the
> manufacturer would like it, but they can't say that and CERTAINLY
> don't want this kind of thing to, ah, 'leak' to the FDA or their
> European equivalent, because right there and then, you can forget
> about getting approval. Which is a multimillion dollar operation, so
> the agitation is understandable, in a way ;-))
>
>
>> MySQL also has an archive backend which is based on zlib, but they do
>> not allow modifications for it.
>
> zlib/RLE, etc. are all non-lossy compression /overlays/ really; this
> thing OSI is doing is embedded into the database at the core level; it
> is part of the original design, not an afterthought. The whole
> /intent/ of the database is to compress the data to these summary
> chunks. Think of it as smoothing+resampling of the old data; you lose
> detail, but when this loss is within specified bounds, it is perfectly
> acceptable. The best comparison is with JPEG compression, where you
> set the quality level. With PI, you configure the required data
> accuracy: more 'tolerance' / less 'accuracy' means more data can be
> aggregated --> compressed.
> When I really go nuts one day, I'll code it up as an example of
> Hamster use; the filters I was talking about for this are not there
> yet, but you already have post-find filters, etc. so it's simply
> extending that concept - in a big way. You can either compress on the
> fly or have it scheduled (PI); the same can be built using Hamster:
> when an insert surpasses a certain threshold (# of keys stored) one
> could fire a callback which traverses the database from oldest to
> newer, grabs the data and recompress it on the run a la PI: delete N
> records, insert one summary record in their stead. Then have the
> already present post-find filter help out by having it convert those
> summary records, when hit, to 'faked' data records instead, thus
> acting as if Hamster was storing all that beautifully logged data all
> the time.
>
>
> And don't think Monte Carlo is complex; it's really just a test rig
> where you drive the tests through a random generator: it picks the
> test cases (generates the test data; in my case it generates the test
> choices like stock and time and then the real trade data is fed to the
> model under test) and collects the results of each test. Then rinse &
> repeat a sufficiently large couple of times, say 1000+ tests. Now we
> have a collection of values for each result item and for that we can
> determine things like mean, min, max and std.dev. and that can be
> thrown into a plot (or an Excel sheet, over here). The averaged result
> tells me something about the ability of the model-under-test, while
> the min/max/std.dev. numbers tell me a bit about the probable
> stability (predictability of behaviour) of the MUT. The hard part is
> getting the random generator set up right; standard rand() doesn't cut
> it regarding randomness; any 'patterns' occurring in the randomness
> can have a major impact on the final results as those semi-randomness
> artifacts can drive tests towards a unnoticed direction. Few more
> details to spice the soup and you're good to go. ;-)
>
>> Regarding compression the filters won't help because they either
>> compress records or pages. If you compress an index page, then the
>> size is no longer constant. it will be very tricky to get this
>> running.
>
> I know. See above; PI compression is not like zlib's; it's more JPEGgy
> applied to a whole set of records (and keys).
>
> When you've got that down, the usefulness of LT/GT search becomes
> clearly apparent as you can now simply track down the summary record
> instead of the (now removed) old data items themselves; LEQ search
> will deliver the proper summary record; then all you need to do is
> take that summary record and 'fake' a data sampling record instead to
> feed to the FIND() output.
>
>
>> An alternative would be to use compression mechanisms used in
>> column-storage DBMS's (i.e. vertica). They compress index pages with
>> special mechanisms (i.e. RLE or by storing only the difference between
>> one key and the next etc.) They save LOTS of space and therefore also
>> I/O bandwidth. but that's for a future far far away... :)
>
> Welllll.... you could say that PI's compression is the lossy version
> of this. (Haven't looked into column-storage DBMSs myself, but from
> the sound of it, the comparison should hold.)
>
>
>
> --
> Met vriendelijke groeten / Best regards,
>
> Ger Hobbelt
>
> --------------------------------------------------
> web:    http://www.hobbelt.com/
>        http://www.hebbut.net/
> mail:   ger@hobbelt.com
> mobile: +31-6-11 120 978
> --------------------------------------------------
>


Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 7 (10 days ago)
	
	
Reply
	
	Follow up message
On Fri, Aug 7, 2009 at 2:38 PM, Christoph Rupp<chris@crupp.de> wrote:
> Hi Ger,
>
> Glad that you're not broke yet. I had my own share of bad experience
> with stocks (i was young and thought that i can get rich without
> honest work :) ) and now i don't touch them with a 10 foot stick...

:-))) well, with me it's the reverse; I'm getting older and I think I
can get rich (read: buy my bread and butter) without 'honest work'
this way; at least as long my brain functions well and they haven't
pried my cold dead fingers off my keyboard. And stock trading is hard
work too; just different. Anyway, we'll see if I live to enjoy working
in this casino.


> I'll be out the whole weekend because of family issues but  i hope i'm
> back sunday evening, and then i'll definitely spend some time with the
> hamster.

Only happy issues, I hope.

> Regarding merging HINT_APPEND and DAM (and the next steps in general)
> - i would really be happy if i could release a new hamsterdb version
> (1.10) in the next few weeks - even if it's not yet complete regarding
> the other optimizations. (as long as the API is stable and there are
> no known bugs)

It's moving there; I'm working on getting a stable API now; what's
still an unsolved is my (perceived) need to export the statistics so I
can drive Hamster the way I want from an outer layer, which means the
HAM_PARAM which says 'get me my statistics' needs to be fleshed out -
probably along with a public hamster_stats.h a la hamster_int.h,
because I feel the statistics are a bit of a world of their own and
not to be touched by folks who don't feel comfortable about
white-boxing their libraries like I do ;-)

Anyway, I've just grabbed a latest still-in-flux distrob from my
hamster source tree so you can have a look; the Doxygen documentation
sure has seen some improvement and I'm working on something there too
that I wanted all the time - had a look at doxygen again and looked
for examples of what I want, then thought: what the heck, Dimitri says
he's done his website using doxy, so let's pray it's in the distro and
BANG, it's in there - exactly the kind of sample I was looking for --
I want to write up a few of the technical bits, maybe copy&paste a
couple of my emails to you, so the 'knowledge' about the statistics
and other parts in Hamster are persisted in some kind of 'architecture
& implementation' documentation tech geeks like me can OD on, if you
get my drift ;-)

Anyway, I'm holding off that bit until the next transmission as then
you'll at least have something that compiles NOW and diff-see what
I've done to the documentation next. The refactoring and idling
through the code in search for some intangibles already had made quite
a mess, so be prepared to see a lot of edits all over; sorry about
that.


The good stuff for the current distro:

- Statistics have moved off to their own source file. Their complexity
is not yet at a 'production ready' level as find/insert/erase stats
are known buggy - see the BIG FAT WARNING in there, which is the
result of you warning me; thanks for that, by the way!

- nearFindStressTest now prints a few performance numbers at the end;
the dev box here says: 6-9K inserts/sec, 29-40K cursor moves/sec,
8-13K finds/sec (that's an even mix of all eq/neq/lt/gt/leq/geq/near
search modes)



> So if you have something that you consider stable (regarding Bugs
> *and* API interface) then you can drop it to me and i let my tests run
> through, and also let the folks at work run some tests (they found a
> crash in the newest drop).

I am VERY INTERESTED in that crash! At least it would be very useful
for me if it is reproducible with the latest now.


> So if there's anything i can do to help (besides testing) then please
> tell me or i will feel a bit useless :)

heh. Don't worry, check the diffs and you'll there's loads of work
waiting for you; after all, who is the maintainer here? You're biggest
(and to me MOST important) job is having a very good look at
everything I've done and check my moves; this may sound stupid but be
assured it isn't: I know my mistakes are always Hiroshima-sized, so
the biggest value for me is someone understanding the system inside &
out who is reviewing my messing about and commenting on it. After a
while, like I said before, this optimization frenzy of mine will cease
and it would really hurt me if you didn't get up to speed with the new
structures and changes introduced by then - and decided on their merit
- by then. Right now, it may not look like it, but I'm a
wham-bam-thank-you-mam guy: I get in, do what needs done from my point
of view, then I move on. It will hurt us both if the end result of
such a session on Hamster leaves me (and you) a system which is in
just that kind of 'ehhhhhh' state where further progress is halted by
a sufficient degree of 'WTF?'.
I'm repeating myself, but (code) review is the very highest support I
crave. I would value it greatly, as I've wanted that level of peer
exchange, but almost never get it - not in my professional life nor
outside - mostly what happens is a while of 'yes, cool!' after which
it enters a state of 'he knows, ask him' as the others are not willing
to keep up. Did I mention I'm probably bipolar? Anyway, when you find
the time, have a look-see and run a few tests - doublecheck me,
please.


> i'll write a sample and will enhance the tutorial.
>
> Regarding compression - the lossless compression sounds good, but the
> concept of lossy compression is totally new to me. We could create a
> filter which is called for pages older than a specific date (i.e.
> everything older than a year) and then the filter can compress the
> page and merge it with the sibling pages. i think that's doable as
> long as the page sizes remain constant - everything else would be
> really difficult. But the more i think about it, the easier it would
> be - a kind of map/reduce algorithm working on older leaf pages.

Hm, you got me there - I'm still struggling with the btree in there
(but then balanced trees never were my technology forte), but I've
been thinking about copy/moving the nearFindStressTest off to a env4
example source, where I can flesh it out, feed it some trade data
maybe and code that PI-compression way of things in a crude way.

By the way: despite that you allow a user-depined pointer to be stored
in the ham_db_t, it's not easy to code visitor patterns (and functors)
around Hamster in a re-entrant way; which is the way I currently code
my stock processing, which is - in a way - similar to the PI lossy
compression approach. Anyway, a sample implementation may clarify this
blurb quite a bit.

How 'stable' is the C++ wrapper API? Cast in stone? Or okay to
augment? I'm planning on having an exception-less version, which is
nicer for both deep embedded systems and use cases where one would
expect quite a few failure returns (key not found, etc.). Ah well,
blabbing again. .tar.gz it is and then off to other pastures - got
other obligations myself in a few secs.



> Actually, this would really be a cool use case for map/reduce...  We
> could even run it multi-threaded, because it's mostly CPU-related, i
> guess.
>
> Back to work...
>
> 2 more hours - goed weekend :)

You too!





--
- Show quoted text -
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
hamsterdb-1.0.9.20090807-001.tar.gz	hamsterdb-1.0.9.20090807-001.tar.gz
1416K   Download  
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 10 (7 days ago)
	
	
Reply
	
	Follow up message
Hi Ger!

i'm going to answer one email after the other :)

2009/8/7 Ger Hobbelt <ger@hobbelt.com>:
>> I'll be out the whole weekend because of family issues but  i hope i'm
>> back sunday evening, and then i'll definitely spend some time with the
>> hamster.
>
> Only happy issues, I hope.

Yes - but stressy. Family in law, my wife's aunt/uncle, my parents all
want to have their share of time with Felix (the kid, not the cat) and
since they don't life right around the corner but about an hour away
it takes some time. Understandable, because he's really cute, and also
good for my wife and we because my parents can take care of him for a
night and we can enjoy some nightlife. But it shows that the biggest
thread for open source is having a family, and not Microsoft... :)

here's the master of desaster (the picture is already 6 months old but
i just don't have the time to upload the newest ones... it's a shame)
http://crupp.de/pics/index.php?spgmGal=felix&spgmPic=21&spgmFilters=#pic

Regarding hamster_stats.h: if you think that some users would like to
have access to the statistics then i don't mind giving them a public
visibility. i'd also say that they are very specific and internal, but
i don't mind.

> Anyway, I've just grabbed a latest still-in-flux distrob from my
> hamster source tree so you can have a look; the Doxygen documentation
> sure has seen some improvement and I'm working on something there too
> that I wanted all the time - had a look at doxygen again and looked
> for examples of what I want, then thought: what the heck, Dimitri says
> he's done his website using doxy, so let's pray it's in the distro and
> BANG, it's in there - exactly the kind of sample I was looking for --
> I want to write up a few of the technical bits, maybe copy&paste a
> couple of my emails to you, so the 'knowledge' about the statistics
> and other parts in Hamster are persisted in some kind of 'architecture
> & implementation' documentation tech geeks like me can OD on, if you
> get my drift ;-)

that's a great idea. I always invested time in writing documentation
for users, but never for internals. i think i just didn't expect that
anyone would come and actually work on the internals! Although by now
i would already benefit from that documentation because i tend to
forget things fast anyway...

> I am VERY INTERESTED in that crash! At least it would be very useful
> for me if it is reproducible with the latest now.

Well - for sure i'm not allowed to give away the sources because
they're doing stuff which is relevant for the business. in case you're
curious: they read object formats (coff, borland, gcc) and transfer
them to a virtual object format. For this they use hamsterdb. And then
they cut out sections, merge sections and do loads of other stuff
based on the binary code. Then they export coff, borland, gcc etc.

I suggest that as soon as you have a promising version i'll approach
them and we run their tests. If it still fails, and if i fail to find
the bug, then i dump the database and make a reproducable version
without their sources.

> - by then. Right now, it may not look like it, but I'm a
> wham-bam-thank-you-mam guy: I get in, do what needs done from my point
> of view, then I move on. It will hurt us both if the end result of
> such a session on Hamster leaves me (and you) a system which is in
> just that kind of 'ehhhhhh' state where further progress is halted by
> a sufficient degree of 'WTF?'.

Well - i can tell you that i enjoy conversations and working with you,
and FINALLY there's someone who understands what i'm doing here! (my
wife tries to, but i think she just pretends :)

anyway, i know that hamster doesn't pay your bills, and i understand
that you move on. Nevertheless i'll keep you a bit updated and maybe
ask you for opinions about new features and implementations, because
your input is valuable for me.

anyway - it's not the time to shed tears, i'll download the tar.gz and
will do what i can (= code review!)

goedendag (i have to improve my netherland skillz - we now have van
Bommel, Braafheid, van Gaal...)
Christoph

Reply
		
Forward
		
		
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 14 (3 days ago)
	
	
Reply
	
	Follow up message
Well, these were a few days.. jeez. Got a mailbomb thanks to spammers
using my domains and enough dumbo mail sysadmins who still send bounce
messages cf. the mail RFCs for recipients unknown: 105K mail messages
received. And gmail doesn't like that kind of barrage at the input,
fetching only 200 mails per hours. (If the number of Barracuda mail
server/filter hardware owners who just bought the device and jacked it
in without ever checking what it does, can be assumed representative
for the intellect of J. Doe, then I'm afraid the amoebas overtook us
and we're eating their dust IQ wise. :-(  Sorry, I'm a bit
'opinionated' this week.)


> night and we can enjoy some nightlife. But it shows that the biggest
> thread for open source is having a family, and not Microsoft... :)
>
> here's the master of desaster (the picture is already 6 months old but
> i just don't have the time to upload the newest ones... it's a shame)
> http://crupp.de/pics/index.php?spgmGal=felix&spgmPic=21&spgmFilters=#pic

:-) Cute guy. He likes the photographer.


> Regarding hamster_stats.h: if you think that some users would like to
> have access to the statistics then i don't mind giving them a public
> visibility. i'd also say that they are very specific and internal, but
> i don't mind.

Yup. Very specific and internal. It's rather meant for 'advanced uses'
- whatever anyone else comes up with. I still need to set up the
'public' structures for this; it's simmering on the stove so to speak;
most probably it will be a container struct carrying the internal
env/db/freelist-page items as-they-are to the outside for further
inspection.

> that's a great idea. I always invested time in writing documentation
> for users, but never for internals. i think i just didn't expect that
> anyone would come and actually work on the internals! Although by now
> i would already benefit from that documentation because i tend to
> forget things fast anyway...

Of course I still believe I have little 'brain leakage' like that, but
reality has kicked me in the head a few times too often to forego the
tech documentation - even when most of the time it's a series of
cryptic notes to myself.

>> I am VERY INTERESTED in that crash! At least it would be very useful
>> for me if it is reproducible with the latest now.
>
> Well - for sure i'm not allowed to give away the sources because
> they're doing stuff which is relevant for the business. in case you're

ah. hm, I can think of a few uses for that, but I don't yet see where
the money is in that. No matter; I already assumed the source was
subject to an NDA; I was interested in if they had been able to
somehow isolate the crash a little (or not).

Anyway, today I'm working on hamster again; there's a few bits in
there that need mentioning:

from the looks of it, the key/record USER_ALLOC flag handling doesn't
cope with key/record structs where 'size' is too small to contain the
actual key/record - a risk for variable-sized keys and records.
Stumbled over this while looking for spots where I should inject the
insert & erase hinting stuff; the unittests turned up another area
where find/erase can be greatly sped up: out-of-range keys: right now
it takes a tree traversal to find out that it's key-not-found, while
the statistics gathering can help us out there up front, when we take
the minimalist approach to database histograms, i.e. storing just the
lower and upper bound keys of the database in the stats -- cost is
worst case 2 extra key comparisons in the hinter, 1 key comparison +
copy in the stats gatherer. It'll be in the next archive coming your
way.

And coincidentally, such bounds tracking also helps insert hinting:
one key comparison extra in the hinter and you have an automatic
HINT_APPEND (is my new key > upper bound?).
Another key comparison would do the same for the lower bound, which
could lead to some sort of auto-HINT_PREPEND.

I also have a little check for those USER_ALLOC key/record structs,
which should help reduce the risk of out-of-bounds memory accesses by
hamster, in case a user-alloced key or record struct is passed in.


> I suggest that as soon as you have a promising version i'll approach
> them and we run their tests. If it still fails, and if i fail to find
> the bug, then i dump the database and make a reproducable version
> without their sources.

The stuff is getting closer to 'beta/stable' state; statistics
organization is stable now; the API is 'stable', apart from the
statistics export bit, which is not yet fleshed out.

All in all, I'd say we're getting close.

I have a wicked transaction log unittest failure, which has led to a
few BFC changes, and I'd like to drop that one in your lap for a
precise diagnose as I didn't want to take on the transaction logging
internals as well, at least not yet ;-)

To get a quick snapshot, here's the screendump of the latest unittest
run on Linux/64 - note 'scenario #4' i.e. item [4] in the transaction
log dump that accompanies that error here: 3K instead of 1K, and it
looks the one-write, commit, 3-write sequence is re-ordered into
3-write, commit, 1-write. Why? I throw up my hands in the air (and,
no, I haven't looked really deep at this; it was more an 'oh crap;
when did this start to happen?' thing and I haven't been able to track
down the offending change(s) in the hour I spent on it.

-----------------------
----- error #1 in EnvTest::createCloseEmptyOpenCloseWithDatabasesTest
env.cpp:274 assertion failed in expr ps[0].value == 64*1024
----- error #2 in LogHighLevelTest::eraseMergeTest
log.cpp:981 failure in compareLogs(): assertion failed in expr
log_entry_get_offset(&(*itl).m_entry) ==
log_entry_get_offset(&(*itr).m_entry) for scenario #4
[1]     txn:0, type:8(LOG_ENTRY_TYPE_FLUSH_PAGE), offset:0, datasize:0
   vs. txn:0, type:8(LOG_ENTRY_TYPE_FLUSH_PAGE), offset:0, datasize:0
[2]     txn:0, type:8(LOG_ENTRY_TYPE_FLUSH_PAGE), offset:1024, datasize:0
   vs. txn:0, type:8(LOG_ENTRY_TYPE_FLUSH_PAGE), offset:1024, datasize:0
[3]     txn:8, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:8, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[4]     txn:8, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:8, type:5(LOG_ENTRY_TYPE_WRITE), offset:3072, datasize:1024
[5]     txn:8, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:8, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
[6]     txn:7, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:8, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[7]     txn:7, type:5(LOG_ENTRY_TYPE_WRITE), offset:3072, datasize:1024
   vs. txn:8, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[8]     txn:7, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
   vs. txn:7, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[9]     txn:7, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:7, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
[10]    txn:7, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:7, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[11]    txn:6, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:6, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[12]    txn:6, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
   vs. txn:6, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
[13]    txn:6, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:6, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[14]    txn:5, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:5, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[15]    txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[16]    txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
   vs. txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:2048, datasize:1024
[17]    txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:3072, datasize:1024
   vs. txn:5, type:5(LOG_ENTRY_TYPE_WRITE), offset:3072, datasize:1024
[18]    txn:5, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:3072, datasize:1024
   vs. txn:5, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:3072, datasize:1024
[19]    txn:5, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:2048, datasize:1024
   vs. txn:5, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:2048, datasize:1024
[20]    txn:5, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:5, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[21]    txn:4, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:4, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[22]    txn:4, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:4, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[23]    txn:4, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:4, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[24]    txn:3, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:3, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[25]    txn:3, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:3, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[26]    txn:3, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:3, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[27]    txn:2, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:2, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[28]    txn:2, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:2, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[29]    txn:2, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:2, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[30]    txn:1, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
   vs. txn:1, type:3(LOG_ENTRY_TYPE_TXN_COMMIT), offset:0, datasize:0
[31]    txn:1, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
   vs. txn:1, type:5(LOG_ENTRY_TYPE_WRITE), offset:1024, datasize:1024
[32]    txn:1, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
   vs. txn:1, type:1(LOG_ENTRY_TYPE_TXN_BEGIN), offset:0, datasize:0
[33]    txn:0, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:1024, datasize:1024
   vs. txn:0, type:4(LOG_ENTRY_TYPE_PREWRITE), offset:1024, datasize:1024
-----------------------------------------
total: 2 errors, 2 tests
---------------------------------


> Well - i can tell you that i enjoy conversations and working with you,
> and FINALLY there's someone who understands what i'm doing here! (my
> wife tries to, but i think she just pretends :)

Well, at least she cares enough to nod when you bounce up and down ;-)

And don't think I grok it all yet; I haven't looked at the transaction
logging; reading through the btree rebalancing act had my head go numb
-- which means I'll probably need to revisit that section if there's
more need for speed / better hinting from my part.


> anyway, i know that hamster doesn't pay your bills, and i understand
> that you move on. Nevertheless i'll keep you a bit updated and maybe
> ask you for opinions about new features and implementations, because
> your input is valuable for me.

Sure, no problem. Can't promise I'll be able to utter something sane
each time, but at least it would be interesting mental puzzles.







Then something which merits its own topic really, and should end up in
the tech documentation: cryptography and its use by hamster.

Because I was putting down some structure/design for a fleshed out
example use of hamster where millions of records get accessed by a
(faked) stock model simulation rig, I had another look through
hamsterdb while checking out the external hooks (callbacks) -- which
incidentally are a little hard on visitor patterns and their ilk as
you store one user-specced pointer in the db, but do not register/pass
user specced pointers per callback - alas, an augmented C++ wrapper
will take care of that for me, back to the topic at hand:

I had a look at your AES128 filter implementation and use and saw that
you implemented this as 128-bit ECB (Electronic Code Book) AES128. (If
you are interested in this arena, there's Bruce Schneier's book and a
few others that will get you started in the pretty darn mess called
cryptography; be warned, it's heady stuff) Anyway, the point here is:

You have a couple of security risks in there, that can be reduced,
some with little, some with more effort.

Risks:

- due to 1-block sized ECB encryption, you're wide open to attacks
which employ this (electronic codebook, known plaintext). You can use
a reduced (or full) form of CBC mode encryption to reduce this risk.
The 'full' version would be one where you use a cryptographically
strong random IV with each CBC encryption, e.g. one per db page. [CBC]

- as you do not use any IV, multiple databases using the same AES key
can be combined to produce one codebook: this means the risk increases
with the _total_ number of blocks written while using a single AES
key. This can be avoided by generating per-database AES keys by mixing
the AES key with a cryptographically strong IV per database. (A
cryptographically strong random number next to the 'serial' comes to
mind.)

- hamsterdb offers data obfuscation through ECB mode AES128
encryption, but does not offer data integrity validation. In other
words: you are 100% vulnerable to injection & substitution attacks as
the decrypted data is not validated (apart from a quick check by
decrypting the db header, but this does not protect against any of the
injection/substitution attacks). This can be fixed by adding a
(cryptographically strong) signature to the plaintext before
encryption and validating the signature upon decryption. This can be
done on a per-page basis; unfortunately this implies that plaintext
length < encrypted output length, as the signature is appended to the
plaintext and included in the encrypted output.

Note that these risks do not require access to the hamster binaries or
RAM: all that is needed is access to the encrypted hamsterdb files.


This is not to scare you, but it can serve as the preliminary security
risk analysis of hamsterdb; something that may be valuable to some
folks out there looking for secure database storage.


There are a couple of ways to improve on this (as mentioned in the
risks list above), which might be useful for mission critical
applications.

ECB encryption is, by design, more sensitive to certain types of
attacks: if you consider each encrypted item as one 'word', ECB allows
the (would-be) attacker to collect a 'dictionary' of encrypted words;
as hamsterdb contains data pages where the data is pretty predictable
(e.g. the freelists: lots of sequential 0 or 1 bits) and this can be
used to his advantage when you also can infer the related unencrypted
'word' with sufficient certainty; I do not know of any attacks on AES
based on 'known plaintext', but using ECB for encryption keeps the
gates wide open to such a risk (= such 'known plaintext' attacks
affecting your security).

Injection and substitution attacks can be viewed as a derivative of
that: once you've collected a 'codebook', you can inject or replace
encrypted blocks with ones of your own, thus altering data in the
database. Imagine what this will do to the blobs stored in the DB:
hamster will not crash, but the data is corrupted/altered in an
unchecked way. With the same ease can this be applied to btree pages;
with a little luck you keep a seemingly valid btree structure while an
attacker can illegally erase keys from the database and no-one would
be the wiser then.


For hamster databases, the number of 'ciphertext ECB entries' is about
N/16 where N is the size of the database in bytes. Eek!

To counter dictionary attacks, one can employ the CBC (Cipher Block
Chaining) encryption mode (there are other modes too) - hamster can do
this too, as data is always read or written on a per-page basis
anyway, so this is another way of saying: why not make it harder for
the attacker by reducing the number of ECB entries he can collect from
a database, by increasing the size of each plaintext 'word': instead
of using 128-bit 'words', we can use 1-page-sized 'words' here.

And this just considers using CBC with a fixed IV (Initialization Vector).
CBC is very simple really: you take the encrypted output of the
previous block (128 bits) and XOR it with the next plaintext block,
before you encrypt that one.

Thus, CBC has one benefit ECB has not: when we take the encrypted data
for any 128-bit block in the database, ECB by design means: same
encrypted data == same plaintext. With CBC, as it 'mixes' in previous
blocks within the given CBC chain length (1 'page' in our suggested
case), only the very first (series of) identical encrypted data blocks
== same plaintext. Thus CBC helps to, ah, obfuscate, the plaintext, as
the same plaintext will produce different encrypted output if only one
(or more) preceding plaintext blocks in the 'chain' differ.

With a fixed IV (I'll come to that in a second), this means CBC helps
cutting down the number of ECB codebook entries usable by the attacker
from N/16 to N/1024 or better (N/4096 on Linux, N/65536 for Win32).
Don't think you'll get the full CBC 'benefits' as as you do not use a
'cryptographically strong' IV, though.

What is an IV? The IV is simply a 128-bit block of bits which you put
at the start of the CBC chain:

b = IV
while (chainlength-- > 0)
{
 b = b ^ next_plaintext_block()
 b = encrypt(b)
 output(b)
}

As you see, the output length == input length for this CBC
implementation, just like it was for ECB as you have done in hamster,
yet the cryptographic 'strength' is improved.

Normally, cryptographers will advise to use a per-run RANDOM IV, which
means each CBC run (in hamster terms: each page) should also store
it's random IV alongside, as the decryption routine will need that IV
to properly decrypt:

t = IV
while (chainlength-- > 0)
{
 b = next_encrypted_block()
 b = decrypt(b)
 t = b ^ t
 output(t)
}

With a fixed / global IV, you don't have to remember the IV per page,
so inlen == outlen, but most of the ECB risks remain then, especially
when the first series of blocks per page are easily guessable (page
header structures are very well defined and predictable in plaintext
bit content).

That's why a random per-run IV is advised: this renders the ECB
codebook useless, as each block in the database, even when they all
have the same plaintext content, will produce different encrypted
output, thus making it much harder for the attacker to deduce the AES
key.

I've mentioned those words 'cryptographically strong' a lot of times
by now, and that means, for one, that using system 'rand()' for random
number generation simply doesn't cut it, even while we neglect the
fact that every compiler/platform has it's own, output-incompatible,
rand(). You'll need a cryptographically strong SRNG for stuff like
this, and that's a task for stronger men than me; I'm not a
professional cryptographer.

Given that, the listed risks can still be reduced by several powers of
2, while we will assume there's no additional cryptographical tools
being introduced to hamster (I'd suggest using the crypto library that
comes with OpenSSL otherwise; no use reproducing something that has a
LOT of risk of doing it wrong in all sorts of subtle ways):

- using CBC with a semi-random IV per page

- as we assumed just above that we don't introduce a cryptographically
strong random number generator in hamster, it's no use to have a
per-page-different IV as the IV will be predictable. Unless we can
'obfuscate' the IV in some way, which won't make it as tough as a
truely random IV, but still... when there's room in the database
header page to store a cryptographically strong random block (to be
provided by the user together with the AES key?) then we can
'propagate' that randomness by hashing that randomness block with the
page address (ham_offset_t) to create a per-page strong IV and feed
that to the per-page encryption/decryption process.

When we cannot guarantee cryptographically strong random IV input,
it's very probably no use creating per-page IVs; it might even be
dangerous as the page headers are predictable, so are the IVs then,
which only leads to an increased number of known plaintext codebook
entries available to the attacker, which is a bad thing as it helps
attackers, especially when attacks become available which are
dependent on bit distribution issues in the cipher used.


The injection/substitution attacks can only be caught by validating
the decrypted output, which means it needs to have a cryptographically
strong signature attached - CRC32 and their ilk won't cut it. SHA256
is advised, these days, as both MD5 and SHA-1 have known
vulnerabilities. Besides, there's the rule of thumb that the number of
bits in the signatures (hashes) should be about twice that of the
symmetric encryption cipher used (AES128), so that would fit nicely
with the suggestion of SHA256. Unfortunately, it adds 256 bits to each
page, which means either a format change or a suboptimal hack where
this (and other) page info is stored in a 0xF00? 'user-hidden'
database alongside.


(Side thought: too bad the filters cannot influence the 'usable page
size' value; when they could, one would be able to create filters
which do all the stuff suggested above, included signatures and all,
for arbitrary cipher suits (= varying key/IV/signature bit sizes).)


In closing, using crypto is almost as hard as designing it.

Currently, hamsterdb offers a degree of data /obfuscation/ through
encryption, but data /integrity/ will only be feasible once a data
validation signature is appended to the encrypted pages.





> goedendag (i have to improve my netherland skillz - we now have van
> Bommel, Braafheid, van Gaal...)

Heh. They get paid enough to be required to learn some decent,
accent-less German, IMO. 'goedendag' is the old fashioned form of the
word (the 'n' does it ;-) ), is very correct Dutch, but, of course, we
water it down like our tomatoes into 'goejedag' which is not written
down, ever, as the written greeting is something akin to Tschuess:
'tot ziens' (or on the phone: 'tot horens', which is a literal
equivalent of 'Auf Wiederhoeren' - though only 'conservatives / old
fashioned people' like me use that last one).
So it's:

veel plezier en alvast een goed weekend gewenst,

;-)

Ger
- Show quoted text -



--
Met vriendelijke groeten / Best regards,

Ger Hobbelt

--------------------------------------------------
web:    http://www.hobbelt.com/
       http://www.hebbut.net/
mail:   ger@hobbelt.com
mobile: +31-6-11 120 978
--------------------------------------------------
Reply
		
Forward
		
		
 
Reply
 
|
Christoph Rupp
 to Ger
	
show details Aug 16 (1 day ago)
	
	
Reply
	
	Follow up message
Hi Ger,

glad to hear that you're ok (more or less) - the mail bomb sounds
really really nasty. why did they use your domain for 105k mails? did
you somehow piss them off? I heard that they sometimes do that as
"retaliation" against anti-spam-activists.

i basically gave up all my efforts regarding spam-filtering. till some
months ago i had a root-server with sendmail and all the stuff running
to manage the mails for crupp.de and 6 or 7 other domains. but the
administration effort was so immense and nevertheless the spam level
was so high that i gave up and moved everything to google apps (which
is awesome, but i never had to deal with 105k incoming mails...)

>> Well - for sure i'm not allowed to give away the sources because
>> they're doing stuff which is relevant for the business. in case you're
>
> ah. hm, I can think of a few uses for that, but I don't yet see where
> the money is in that. No matter; I already assumed the source was
> subject to an NDA; I was interested in if they had been able to
> somehow isolate the crash a little (or not).

well - basically they patch DLL's/so's  and replace objects/sections
in the file, adding obfuscation and other stuff. that way each
customer gets its custom library and we don't have to regenerate it
from scratch. Each library looks different and if one is hacked, all
other customers are still safe. and since we're a security company
with a software DRM/copy protection product, security is the most
important thing that we offer.


> from the looks of it, the key/record USER_ALLOC flag handling doesn't
> cope with key/record structs where 'size' is too small to contain the
> actual key/record - a risk for variable-sized keys and records.

Yes, that's true. The caller has to make sure that the pointer is big
enough. I don't have a good idea how to solve this without breaking
backwards compatibility.

> Stumbled over this while looking for spots where I should inject the
> insert & erase hinting stuff; the unittests turned up another area
> where find/erase can be greatly sped up: out-of-range keys: right now
> it takes a tree traversal to find out that it's key-not-found, while
> the statistics gathering can help us out there up front, when we take
> the minimalist approach to database histograms, i.e. storing just the
> lower and upper bound keys of the database in the stats -- cost is
> worst case 2 extra key comparisons in the hinter, 1 key comparison +
> copy in the stats gatherer. It'll be in the next archive coming your
> way.

cool :)

> I have a wicked transaction log unittest failure, which has led to a
> few BFC changes, and I'd like to drop that one in your lap for a
> precise diagnose as I didn't want to take on the transaction logging
> internals as well, at least not yet ;-)

don't worry, i'll have a look and fix either sources or unittests.

regarding encryption: yes, you're right. actually, i should have known
better when implementing it. i told you above that i write code for a
security company, so i'm roughly familiar with AES and CBC. While i'm
not designing the crypto stuff, i have to implement it and i used
both, AES and CBC. But i can tell you that noone before explained it
as good as you.

I always considered obfuscation and integrity as two different things,
because for some users it might be useful to have an integrity check
without obfuscation, i.e. by adding a CRC or rather a MAC in the page
header.

i added the AES obfuscation when the file format was already fixed,
and then i discovered that i forgot to reserve 16 bytes for a MAC. And
then i forgot the whole topic anyway.

Regarding CBC and finding a safe IV: i'll ask at work, i have some
crypto-experts sitting in the office next to me :) maybe one of them
has an idea. At work i know that we have some random byte generators
which uses a big variety of sources. but as you said, i don't want to
reinvent the wheel. i'm too lazy for that.

Regarding visitor pattern: how would you like to have it? do you want
to visit only the leaf pages or ALL pages? or do you just want to
enhance the callback mechanism when reading/writing a page and have an
additional pointer as parameter for the callback?

> veel plezier en alvast een goed weekend gewenst,

dank u wel!

next week we're on vacation (South Tyrol) from tuesday till sunday and
tomorrow i have to do babysitting. i expect to have very limited email
access.

have a good spam-free week!

Christoph


































\section The Sunday Funnies #1: huge BLOBs -- or is it EXTREMELY huge BLOBs (ELB)?
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 17 (1 day ago)
	
	
Reply
	
	Follow up message
No surprise here: hugeBlobTest (for any fixture) was slow as a turd
traveling uphill.

Yes, it did surprise me (in a somewhat neglected way) a few 'releases'
ago (releases ~ tar.gz transmissions from me to you).

Alas, today was blob bashing day. And here's a funny one I ran into:

Assume you're using hamster with pagesize P and blobsize B. Then it
follows that a freelist page (of size P bytes) will contain a little
less than P*8 bits, which represents a space a little less than P*8*32
(DB_CHUNKSIZE) bytes.
Assuming the UNIX default for P @ 4K, that means one freelist 'entry'
(page) can represent close to 4K*8*32 = 1Mbyte.

Now for the Sunday Funnies.

Take a use case similar to the hugeBlobTest unittest, where 4MByte
blobs are stored in the database. Adding? Works. Finding? Works.
Erasing? Works. Overwrite=Replace? Works.
Insert new blob into free space obtained by erasing one or more
consecutive, previously inserted blobs? NO WAY.

The freelist search & alloc routine is always looking at ONE freelist
'entry' only, so ANY alloc request that's larger than fits in one
freelist page will get a lot of soulsearching in there and a very
deterministic answer: no way.

The result? Working with such large blobs will cause the hamster to
grow infinitely, no matter the use pattern (= mix of insert/erase
operations).

This has been fixed in the very latest distro only (the one
transmitted about 15 minutes ago) by

1) detecting such extremely large freelist alloc requests in the
freelist hinter.

2) serving those requests by a special (new) code section, which looks
at the entire freelist as a whole instead of just one freelist
'entry'.

Speed?

Good, I'd say. Because I made one lazy execution time optimization in
there: when we look for alloc space for extremely large blobs, i.e.
ones that are going to span more than one freelist entry/page,
guaranteed, we just round the request up to the next number of
_complete_ freelist entry pages required to fit such an allocation and
then search for so many _completely_ _empty_ freelist entries in a
single sequence. Of course, since we're looking for a sequence again,
it's just like in the older bit-level freelist alloc routine:
sequences are not scanned in the naive/classic way, but through
employing Boyer-Moore as that's another speed gain we can use,
especially when the number of freelist entries (pages) grows large:
those multi-MByte databases I like so much.

The drawback of this lazy scheme?

Any 'extremely large blob' will always be positioned at a freelist
entry boundary. Take heed: that's NOT a 'page boundary' (say 4K
alignment for UNIX), but FREELIST ENTRY/PAGE BOUNDARY (that's
something close to 1MB alignment for UNIX: P*8*32 minus a few). In
other words: we only put extremely large blobs at bit position 0 in
any already existing freelist entry. That's bad? Nah. The worst that
can happen is that you end up with a disc space utilization factor of
little over 50% as the worst case scenario is an extremely large blob
(ELB for short) which occupies one entire freelist entry (P*8*32-HDR
bytes) PLUS 1 byte. As we'll be looking for _completely_ _free_
freelist entries only, we'll need 2 of those for that one, while the
second one won't be eligible as a starting point for the next ELB
coming around, resulting in a wee bit above 50% fill ratio.

Do I mind? Nope. It was game over for freelist space re-use for ELBs
before this time, so this is a huge improvement when regarded from
that point of view.

Fries On The Side?

Yes, please. The freelist hinter has been augmented to support these
ELBs now; the alloc code has been augmented accordingly and then
there's the 'fries on the side': within the blob handling code
(blob.c) there was another optimization waiting to be found: most of
the slow performance was due to transaction logging kicking in in the
hugeBlobTest unittests, resulting in very effective cache threashing
by the blob write routines as each page of the blob is fetched, the
old (and previously non-existent) content logged to the recovery/txn
log, the page written, added to the transaction and pushed into the
cache, meanwhile forcing a perfectly good cached page to be dropped
out of there, thanks to the 'age' of the blob page being more recent
(younger) than that other page, which had nothing to do with the blob
itself.

This has now been taken care of: the page is not logged any more when
it was created fresh, i.e. has not existed before the blob alloc+write
action, as there won't be anything useful to rollback on txn abort
anyway: it was (virtual) free space and it will become free space on
txn abort, which doesn't mind about the actual byte content of such
pages, as 'free' only means the freelist bits need to be set on txn
abort. Meanwhile, the write is rather immediate (the cache was
thrashed, so it was flushed to disc anyhow) so a power outage and
subsequent recovery action should NOT need those non-existent previous
page data bytes anyway, but rewrite the blob as it was written to disc
right now. Both ways: no need to log any FULL PAGES written to by the
blob. And partial pages, of course, are logged and cached: after all,
there might be a wee tiny key finding a place in there and then having
these cached will speed that up.


The usefulness?

Well, 1MB+ records are rare, but they certainly exist. And this should
take care of the issue of ever-growing hamster for such record sizes.
(Hmmm... I /could/ try hamster as a database/filesystem for my p0rn
collection now. Talk about major performance test. :-) (Dirty mind is
joy forever...)



Caveats:

Haven't VERIFIED this with the official 1.0.9 release, but my guess is
it's there as well as it was in my current completely pimped revision.



























\section The Sunday Funnies #2: security, encryption, ...
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details Aug 17 (1 day ago)
	
	
Reply
	
	Follow up message
I had another look around -- been too long since I really worked on
crypto and that's taking it's toll as my advise is a little flaky.

First of all: the CBC-is-only-ECB-with-less-blocks mention is correct,
given the context of the use in hamster of ECB and the assumption that
it's replaced with CBC without an IV (or a predictable IV).

A way to improve on this further might be to generate a IV which is
similar to a nonce, BUT WATCH THE CAVEATS HERE:

one can take the page address as a kind of unique counter, mix it with
a hash of the encryption key (not the key itself, mind you) and
encrypt that in order to produce a reasonable IV for that CBC mode,
without the need to store a (random) IV with each page: the decryption
side can reconstruct the IV thus generated, while it's expected to be
harder for an attacker to (a) predict the IV, (b) manipulate the IV
(as it's based on a deterministic counter and a secret, derived from
the secret encryption key).

For good measure, one should really use a different key for the
hash(counter || key) operation there to prevent another subtle screwup
in this scheme, as getting the IV generation RIGHT for CBC mode is a
tough task.

Then there's other chaining modes to consider (CTR, OFB, ...) but I
like CBC for some inexplicable reason; though the other modes have
some possible advantages over CBC in terms of speed (hardware) or
otherwise.

Anyhow, it's still only obfuscation that we got then. (The reason I
don't like CTR and stream-for-block cipher modes is that most of them
xor the plaintext with an encrypted block to create the ciphertext and
that means I can simply toggle bits in the crypted output in order to
edit my database the way I want - success guaranteed, even while I
don't bother with the encryption or its key then. CBC at least crypts
the plaintext block before writing it, so bit changes avalanche to the
output, making palintext editing from across the crypted fence a tough
task.
Given that the pages often contain headers, which are quite
predictable, while we can assume that the data stored in the pages is
a little more variable, one might consider reversing the block order,
i.e. start crypting at the LAST block of the page instead of the
first, but note that this is a measly rotten stopgap hack any which
way and not very adding much, if any, quality to the obfuscation.

Then there's the validation side of the matter: ECB, CBC or any other
mode don't save us from the bit whackers screwing up our database.

I recalled OCB as a fast way of doing it, but that's patented and,
alas, it still requires a larger output than input, so there's no way
out there, no matter if we MAC the content (Encrypt-than-MAC is
preferred; a nonce/IV constructed using a E(hash(page
address-as-counter || key2)) is probably a good thing to do then too
to ensure no MAC collisions for identical page contents (important:
different nonces for the MAC and the CBC/XXX encrypt modes), either
plain or crypted.

I don;t know if it's feasible to generate those extra keys for the CBC
IV and MAC init nonce by using some sort of keygen (looked the right
term up: key derivation function) so the user can't screw up by
passing a weak keyset into hamster, but that's all pending a change
that hasn't happened yet:


For all the extra goodies up there, you need space to store things
like MACs, IVs, etc. per page. Read:

What is needed here is that the filters can tell hamster what the
USABLE PAGESIZE is: hamster already has this notion, though in a
limited way, for those pages which store internal structures (btree
pages, freelist pages, header page), so this could be done by telling
hamster, maybe at create/open time (oh boy, another HAM_PARAM_XXXXX
coming up!) what the 'slack' is that filter X needs.
Hm, trouble with the HAM_PARAM_ approach there is that the filters are
stackable and such a param is a one-shot; augmenting the filters to
have them report their desired 'slack' per page is nicer, in a way,
but it would break the filter interface.
Heck, that interface is broken anyway if we like to use the page
address (ham_offset_t) as a unique counter, or some such.




And then the topping: CAVEATS

Using the page address (offset, really) as a unique counter for CTR
mode, as part of a IV, or anything else is MAJORLY FLAWED. Unless we
expand the db header and store some unique per database randomness in
there.

See this scenario: N database files. Each of those N files will have
the same page address sequence: each has page 0, page 4K, page 8K,
etc. (assuming UNIX 4K page boundary). Im short: the page address is
NOT unique when multiple databases are created using the same AES key.
This either requires one AES key per database (and require users to
make sure), or revert to an additional source for uniqueness in the
form of a cryptographically strong random generator and have that one
produce a sufficient chunk of randomness per database file, so the
page address 'counter' can be mixed with that randomness to guarantee
a unique derived 'counter' for the use case of multiple databases with
one shared key (set).

This however requires the expansion of the database header; of course,
such could be fitted into the new idea where the filters tell hamster
how much slack is needed per page: the CS random block can be stoed in
the slack space for the header page and none would suffer from it.



























\section The Sunday Funnies #3: visitor patterns and the hamster filter callbacks 'n all
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details 3:20 AM (22 hours ago)
	
	
Reply
	
	Follow up message
(By the way: I write these to remind myself -- these should maybe end
up in the tech docs after a while, but until then...
and it was about high time for email subject lines that again came
near to covering the actual content of the messages. My chain view of
HAM_APPEND is impressive but it doesn't help googlability in my
Thunderbird, so to speak. Alas, on with the show.)

What would I like for hamster to do for visitor patterns (or functors
or whatchamacallit)?

Oooooh. I wish... Nah, seriously, it would have been nice to have had
a filter callback system where one can pass a user defined reference
to hamster on each invocation and have that show up in the filter
interface immediately.

But that was before I had another good look.

First, this naive suggestion assumes that one likes to see the same
reference (pointer) with all filters. What if one would like to see a
per-filter user reference?

Second, it completely foregoes the fact that hamster does not (and
should not) tell you, the user, when which filter will be invoked:
after all, We The Hamster decide when to read or write a page of data,
etc., and there should not be any (artificial) limitation to our
hamster freedom there. Given #1 above, that means we need to maybe
keep a user reference with hamster at all times, available to the
filter(s) at the time of need. Which is what hamster currently has on
offer.

Third, passing a user reference in with each invocation is, given the
unpredictability as mentioned in #2, only complication the API
interface without adding any real benefits.


So how about those visitor patterns and functors that I crave?

Well, the C++ interface of hamster can cope with that. By registering
visitor instances beforehand, then invoking hamster APIs and allowing
hamster to call a C++ wrapper callback, which in turn churns through
the registered visitor instances, we can have our cake and eat it too.
Sure it's a little different from what I regularly do, but it should
work out well. The same goes for functors, which are, when not looking
too closely, just another way of saying 'visitor', i.e. 'OO callback'.

So all that means there's probably going to be a ham_ex C++ wrapper
some day. With support for registering visitor class objects, which
will be invoked in a largely asynchronous manner (page I/O) or at
predictable monents (key & record filters).

Now, before I get dragged off in chains to have a friendly chat with
the local Design Pattern Puritan Police Constable (and friends), let's
make this WTF a little more concrete by looking at two nice examples
where this stuff comes into play:

a) support for the 'lossy compression' we talked about before, and

b) support for associated indexes a la BerkeleyDB (yes, I had a look
at their API and other docs to get an impression of what I was missing
out on while having a marathon party with the hamster in his cozy
treadmill)


Case A: an aged-based lossy key+record data compression scheme...

... done up in C++. I wouldn't like to do that through inheritance,
deriving from the C++ hamster wrapper (besides, it's lacking some
'virtual' members to make that a good kneejerk solution for the such
inclined). Instead, something akin to a visitor would be nice: the
visitor knows about the hamster and gets to visit every time a record
is sought: find() with such a visitor sitting in the key+record filter
slot (oh, how I'd love to have the key available in that record
filter! ;-) ) would be able to deliver to me a nice 'data record' for
any time 'T', even while that time 'T' might lay in the aged,
compressed zone of the database: the visitor just picks up the
key+record, which is a time t1 and a record saying
'range(t1,t2)+start/end/average/samplecount/tolerance', so that the
visitor can derive a nice value V = (start +
(end-start)*(T1-t1)/(range:t2-t1) and maybe some nifty attribs such as
the related value tolerance (or 'accuracy', if you rather like to have
that one, which is, with a lot of hand waving and ignoring all the
good stuff about value normalization and all, its reciprocal).

So Case A is a case where visitor comes along for the find() ride,
does the voodoo work of data 'decompression' as described above, and
delivers neat (data sample value + sample time) record+key pairs to us
as if nothing had been done to that data in there anyway.

Why a 'visitor' and not a simple callback? Because the same 'visitor'
(pardon the abuse of the term, Mr. Gamma) class instance can help us
out with the data compression as well: this has the advantage that the
(lossy) compression and later decompression is housed in a single
entity, not part of the hamster kernel (a fact which should ride
favorably with the UNIX-inclined kernel philosophers who crave micro
kernels) and thus adaptable, ajustable, you-name-it-tweakable to the
user's heart's content.

Hence case A would like a hamster I/F where

1) keys travel together with records into filters (and out of them -
transfer references, not copies)

2) the hamster API is re-entrant (Yes, that's a biggie! Because a
callback, no matter the language it's written in, would benefit from
the ability to invoke the hamsterDB API, recursively if necessary, so
things like 'see if there are more 'old' records around, collect and
compress/merge them in larger time-slot ranges' can be done from any
filter.

3) for OO filters, acting as parts of such alleged 'visitors', it
should suffice to have one user specifiable reference (pointer) per
filter available; it's not a real visitor, but it at least allows for
the implementability of the same general idea: offloaded functionality
through third parties.


I have to check again, my mind is wandering too much now, but I don't
believe we currently have (1), I have never checked into the
re-entrancy of the hamsterdb API for real, but one thing that's a red
flag already in my brain are those 'local_txn' faked local
transactions in hamsterdb.c: if those are not exported to all the
filters, we're toast, as we'll have nested transactions then, which is
an unsupported (and tough to support from now on) feature of v1 - so
this last bit means the transactions have to travel to the filters as
well as the key and record references. And then there's the user
reference per filter, which I seem to recall IS currently available.
But believing we already have (3) may be a pipedream of mine at this
late hour and not sitting at the developer machine.

So far, so good.


Case B: the Berkeley Look Alike: the Associated Index

Here, one can find() using a secondary key (the key stored with the
record being the primary key) and get the proper record in return.

The hamster can be like BDB in this regard: secondary indexes are
nothing but additional databases (tables) within the same environment,
where the secondary key is stored as key and the related primary key
as record, meaning secondary-key based searches take two find()s to
complete: one find-the-primary-key find() in the 'secondary' table,
after which a second find() in the primary table using this retrieved
key, produces the record we were looking for.

Of course, there's also the situation to consider when the secondary
key allows duplicates, i.e. one secondary key maps to multiple primary
keys: do we return the lot of them, or do we act all hamstery and just
return the first of the dupes, unless there's a cursor-like thingy
involved, in which case we can get all the duplicates in order of
appearance. Note those words: 'order of appearance'. Hamster currently
does not really support sorted insertion of duplicate keys per se,
though it can be manhandled in doing so through INSERT_AFTER/BEFORE
and a bit of cursor travel through the dupe list. Too bad the cursor
can't jump through the duplicates in true binary search style: we lack
a way to say 'step forward N duplicates', 'jump to duplicate M' or
something like that.

Now, having such secondary indexes can be generally accomplished in two ways:

1) you extend the wrapper, the derivative knows about these 'secondary
index' things and can act accordingly, while simply calling the
hamster API methods a lot, just like the next kid in town.

2) you can offer this functionality in a way that is transparent to
the hamster API user: the same (number of) hamsterdb API calls, no
need to 'know' about this is a secondary index or not, no derived
wrappers, nothing. And still you have secondary index support in
there, plugin style. That's what the filters are good for: any
secondary database (table) gets a proper filter installed which
accepts the key+record on find() and then executes another
(re-entrant!) find() under the hood, using the secondary record as
primary key to retrieve the actual record we were looking for.

Of course, (2) has a few flaws when written just like this: though
there's obvious support for duplicates that way, there's no easy way
to provide sorted order output of those duplicates. Besides this minor
issue (after all, the rest of hamster doesn't provide sorted order
output of regular duplicates either, so we might be prepared to live
with this - there's the real flaw in the 'transparancy' whcih occurs
as soon as we INSERT or ERASE a primary or secondary key+record: these
actions have to answer one of these questions at least: (a) how do we
construct a secondary key from a primary key+record pair? or (b) how
do we construct a primary key from a secondary key+primary record
pair?  And that is not answerable in an easily transparent fashion: we
need a key construction/derivation definition somewhere. And isn't
that looking like the humble beginnings of an SQL database's data
dictionary definition?

Ah well. This kind of trickery again requires

1) fully re-entrant hamsterdb API, which allows recursive calls
(calling find() from within find(), for one)

2) it is desirable to allow filters to signal the calling hamster
function that an error occurred, or at least a way to say 'please stop
the train, I want to get off here'.

IIRC, the filters already return a mandatory ham_status_t, which
should take care of (2), while the re-entrant bit about (1) has
already been mentioned above: this has not yet been reviewed, though
the local_txn looks like a probable risk there, unless it is exported
to the filters.




Case C: data aggregation on the fly

Just a short mention of this one: aggregating data in a filter, which
traverses a section of the database, which is used to augment the
record data returned. Closely related to case B with a bit of case A
(only in reverse) thrown in, this can be done with the hamster as is.


Yet the ability to do this in the filters makes the filters stronger
and allows me to take the hamster API wrapper (or it's replacement)
and plug it in a standard-shaped slot in various instances in an
application. By having such callback mechanisms (done as registerable
plugins, which is not entirely like 'visitor' but similar in some ways
in my mind) one can have a single class interface wrapping the hamster
and yet offer different, augmented reality to different user instances
within an application. It goes nicely with the reduced need for
wrapper class inheritance that way, though some people may consider
the way these case solutions work as 'non intuitive'.

There's different ways to shave the same monkey, but I happen to like
the filter concept for it allows me to do things at the 'back end'
which would otherwise have required quite a bit of inheritance on the
'front end' -- or re-doing the whole filter/callback thing again in
the C++ wrapper, adding yet another round of pre- and postprocessing
queue invocations around each hamster API call.


I can live with the one user reference per DB, though it's not my
kneejerk answer to the problems it can help solve. No matter. I can
see it work for me, which is all that is needed.

Then I need to check up on that hamster API re-entrancy bit; the
local_txn is a burning red flag at the back of the brain here, so
there's a probably issue with that; now I only need to find out which
issue that is, exactly. :-) Possibly maybe not getting exported to the
filters, I guess.

Which brings me to my wish, pipedream, or 'already there!' reality
(take your pick): one user reference per registered callback and keys
traveling with records anywhere. At least that will be a big help in
getting easier functor code going, which in turn is easier on all
callback-oriented patterns all around, such as visitor. I'll have a
look at it when the bug dust has settled around the latest hamster
surgery. Execution speed is now closing in on my targets for that one,
so it's slowly moving away from hamster internals towards more-or-less
OO-based hamster externals. All of which should result in a bloody
fast stock model evaluator and real-time trade support tool.




Mix that with a per-filter definable page 'slack' consumption (a.k.a.
custom header chunk) for things like authenticated obfuscation through
data encryption and we have the BFG9000 of database backends. Sans the
multithreading and the locks, alas, but that's fine with me. (Which,
incidentally, brings me to the last Sunday Funnies of today: #4 ...)













\section The Sunday Funnies #4: Two Writers Considered Harmful
Reply to all
Forward
Reply by chat
Filter messages like this
Print
Add to Contacts list
Delete this message
Report phishing
Report not phishing
Show original
Show in fixed width font
Show in variable width font
Message text garbled?
Why is this spam/nonspam?
 
Reply
 
|
Ger Hobbelt
 to Christoph
	
show details 3:38 AM (22 hours ago)
	
	
Reply
	
	Follow up message
A while ago, I had messed a bit with the sharing flags for hamsterdb
open/create ops and then your comment was something along the lines
of: two hamster writers on the same database is allowed and okay. Or
that's what got stuck stuck in my brain, never mind the actual words.
So correct me if I'm wrong, but here it goes:



Two Writers Considered Harmful

Consider a hamster database and two processes opening the database,
each in read/write mode. This is not good. It is Harmful(tm).


Why? We've got copy-on-write memory mapping, we've got transactions?
Ain't that good enough?

Nay. Consider the case where writer A accesses a table 1 in the
database, adds a record, which is stored (key) in page P. Page P gets
copy-on-write memory mapped to RAM, so writer B doesn't see this
change yet. It's sitting inside a transaction so that's perfectly
okay, wouldn't we agree?
Now consider writer B accessing another table in there, adding a
record there, and because it just so happens that there's ample free
space in page P, writer B gets to write the key to page P too. Another
copy-on-write, etc.etc.

Nothing bad so far.

Until the transactions are committed, that is.

As A and B act asynchronously (and are here assumed to overlap in
activity) we arrive at a merge-on-write problem, in other words: a
major fault: writer A commits, writing the new content to page P and
the related freelist slots. Then writer B commits as well, overwriting
page P with partially OLD DATA, _blowing_ _away_ the key data written
to that page by writer A (the first merge-on-write issue). If this
wasn't bad enough, there's a big change writer B allocated slots in
the freelist which were also allocated by writer A in that
semi-parallel action back there, as the freelist scanner running in
each process is not, and cannot be, aware of the activity in the other
process(es) accessing the same database. Hence the 'best' thing that
can happen to this scenario is when A and B are writing the same key
but different record content, resulting in either A or B 'winning';
anything else, such as different keys added, will result in database
corruption with high probability.

This is irrespective of platform support for sharing memory mapped
pages between multiple processes; given the copy-on-write tactic for
transactions, it doesn't matter: two writers are going to fight each
other any way.


Conclusion:

two writers on the same database is Harmful.

The create/open sharing flags should be configured so this is
disallowed by default. Only very particular circumstance, some related
to buggy file locking by the OS (Windows platform with remote file
system, e.g. samba, and 'stuck' file locks, come to mind), are a
'good' reason to allow another writer to access a database. And even
then it's 'goodness' is quite questionable as it's very probably
plugging another fatal leak with a hole of its own.


If I'm all wrong about the two writers being allowed by intention,
please correct me.

(This was pending in my head today, after I had a look this morning,
but may have got things garbled over the time it took before I started
to write this.)




*/
